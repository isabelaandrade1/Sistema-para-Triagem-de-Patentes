[
  {
    "id1": "140-416-112-830-70X",
    "id2": "156-217-120-172-056",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train an ensemble with set of training data items wherein the ensemble comprises multiple machine learning ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to train the ensemble in first iteration of the training by selectively directing with preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 2 the machine learning computer system of claim 1 wherein the preliminary classifier is not trained with the ensemble claim_text 3 the machine learning computer system of claim 2 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 4 the machine learning computer system of claim 2 wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to further train the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 5 the machine learning computer system of claim 1 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 6 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 7 the machine learning computer system of claim 6 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 8 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 9 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores machine learning ml system with set of training data items wherein the ml system comprises an ensemble that comprises multiple ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and preliminary classifier that is trained through machine learning and iteratively training the ml system comprises in first iteration of the training selectively directing by the preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members and training the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 10 the computerimplemented machine learning method of claim 9 wherein the preliminary classifier is not trained with the ensemble claim_text 11 the computerimplemented machine learning method of claim 10 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 12 the computerimplemented machine learning method of claim 10 further comprising training by the one or more programmed processor cores the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 13 the computerimplemented machine learning method of claim 9 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 14 the computerimplemented machine learning method of claim 9 wherein training the preliminary classifier comprises training the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 15 the computerimplemented machine learning method of claim 9 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 16 the computerimplemented machine learning method of claim 14 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item lang en",
    "claims2": "claims claim_text 1 method comprising storing at machine learning server computer one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the machine learning server computer particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 2 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 3 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 4 the method of claim 3 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 5 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 6 the method of claim 5 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 7 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 8 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 9 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 10 the method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 11 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 12 computer system comprising one or more processors memory storing instructions which when executed by the one or more processors cause performance of storing one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 13 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 14 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 15 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 16 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 17 the computer system of claim 16 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 18 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 19 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 20 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 21 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 22 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "label": 1
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "013-300-859-082-341",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 network slice configuration method comprising sending by first network element network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 2 the network slice configuration method of claim 1 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 3 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 4 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is sent by the first network element to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 5 the network slice configuration method of claim 1 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 6 network slice configuration method comprising receiving by second network element network slice configuration information of first network element and storing by the second network element network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 7 the network slice configuration method of claim 6 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 8 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 9 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is received by the second network element from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 10 the network slice configuration method of claim 6 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 11 first network element comprising sending module which is configured to send network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 12 the first network element of claim 11 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 13 the first network element of claim 11 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 14 the first network element of claim 11 wherein the sending module is further configured to send information of network slice supported by ta to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 15 the first network element of claim 11 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 16 second network element comprising receiving module which is configured to receive slice configuration information of first network element and storing module which is configured to storing network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 17 the second network element of claim 16 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 18 the second network element of claim 16 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 19 the second network element of claim 16 wherein the receiving module is further configured to receive information of network slice supported by ta from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 20 the second network element of claim 16 wherein the network slice information comprises single network slice selection assistance information snssai lang en",
    "label": 0
  },
  {
    "id1": "020-698-584-160-318",
    "id2": "022-129-282-380-908",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 system building learning machine comprising reference learning machine target learning machine being built component analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of components in the reference learning machine and the target learning machine and return set of output values each component on the list of components and component tuner module configured to modify different components in the target learning machine based on the set of output values and component mapping thereby resulting in tuned learning machine claim_text 2 the system of claim 1 further comprising feedback loop feeding back the tuned learning machine new target learning machine in an iterative manner claim_text 3 the system of claim 1 wherein the target learning machine is new component to be inserted into the reference learning machine claim_text 4 the system of claim 1 wherein the target learning machine replaces an existing set of components in the reference learning machine claim_text 5 the system of claim 1 wherein the reference learning machine and the target learning machine are graphbased learning machines claim_text 6 the system of claim 5 wherein the component analyzer module is node analyzer module claim_text 7 the system of claim 5 wherein the component tuner module is an interconnect tuner module claim_text 8 the system of claim 5 wherein the component mapping is mapping between nodes from the reference learning machine and nodes from the target learning machine claim_text 9 the system of claim 7 wherein the interconnect tuner module updates interconnect weights in the target learning machine claim_text 10 the system of claim 1 wherein the tuned learning machine includes components updated by the component tuner module claim_text 11 system building learning machine comprising an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 12 the system of claim 11 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture claim_text 13 the system of claim 12 further comprising feedback loop feeding back the new graphbased learning machine new initial graphbased learning machine in an iterative manner claim_text 14 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to each machine component in the initial graphbased learning machine compute an average of set of computed output values of each machine component each data point in the set of data points to produce combined output value of each machine component corresponding to each data point in the set of data points and compute machine component importance score each machine component by averaging final combined output values of each machine component corresponding to all data points in the set of data points from the set of data and dividing the average by normalization value claim_text 15 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations randomly select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the initial graphbased learning machine average set of computed output values each data point in the set of data points to produce final combined output value corresponding to each data point and compute full machine score each component in the initial graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data claim_text 16 the system of claim 15 wherein the machine analyzer is further configured to feed each data point in the set of data points from the set of data into reduced graphbased learning machine with at least some machine components excluded predetermined set of iterations randomly select groups of nodes and interconnects in the reduced graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the reduced graphbased learning machine compute an average of set of computed output values each data point to produce final combined output value corresponding to each data point and compute reduced machine score each component in the reduced graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data points in the set of data claim_text 17 the system of claim 11 wherein the machine architecture builder is further configured to control the size of the new graphbased learning machine architecture based on the set of machine component importance scores and the set of machine factors claim_text 18 the system of claim 17 wherein the machine architecture builder controls the size of the new graphbased learning machine architecture by determining whether each node will exist in the new graphbased learning machine architecture claim_text 19 system building learning machine comprising reference learning machine target learning machine being built node analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of nodes in the reference learning machine and the target learning machine and return set of output values each component on the list of nodes an interconnect tuner module configured to modify different components in the target learning machine based on the set of output values and node mapping thereby resulting in tuned learning machine an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 20 the system of claim 19 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture lang en",
    "label": 1
  },
  {
    "id1": "161-152-626-124-189",
    "id2": "091-570-367-989-78X",
    "claims1": "claims claim_text claims what is claimed is 1 machine learning computer system comprising first student machine learning system that using machine learning automatically learns from and make predictions on input source data and first learning coach machine learning system that is in communication with the first student machine learning system wherein input to the first learning coach machine learning system comprises data about an internal state of the first student machine learning system and the learning coach machine learning system using machine learning automatically learns and implements an enhancement to the first student machine learning system based on the data about the internal state of the first student machine learning system to improve operation of the first student machine learning system 2 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes different patterns than the first student machine learning system 3 the machine learning computer system of claim 1 wherein the first student machine learning system has different objective than the first student machine learning system 4 the machine learning computer system of claim 1 wherein the first student machine learning system comprises deep neural network 5 the machine learning computer system of claim 4 wherein the first learning coach machine learning system comprises machine learning architecture that is not deep neural network 6 the machine learning computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning system that improve learning by the first student machine learning system 7 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise minibatch size the first student machine learning system 8 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise learning rate the first student machine learning system 9 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise regularization parameter the first student machine learning system 10 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise momentum parameter the first student machine learning system 11 the machine learning computer system of claim 1 wherein the enhancement comprises structural change the first student machine learning system 12 the machine learning computer system of claim 11 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning system 13 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the first student machine learning system 14 the machine learning computer system of claim 13 wherein existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the network of the first student machine learning system below the selected layer and activations of the virtual nodes are controlled by the first learning coach machine learning system 15 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and first learning coach machine learning system controls regularization to the second set of nodes so that activations the second set of nodes tend to agree with the first set of nodes to control dropout rate of the nodes in the first and second sets 16 the machine learning computer system of claim 15 wherein performance objective of the first learning coach machine learning system in controlling the regularization is different from performance objective of the first student machine learning system 17 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing connection weights the additional nodes of the network of the first student machine learning system 18 the machine learning computer system of claim 11 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning system 19 the machine learning computer system of claim 1 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control the learning of the first student machine learning system 20 the machine learning computer system of claim 1 wherein the machine learning system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activations the first student machine learning system are stored in the high speed memory so that the first student machine learning system can be run when the first student machine learning system is active and the connection weights and activations the first student machine learning system are stored in the secondary storage when the first student machine learning system is not active 21 the machine learning system of claim 1 wherein the first student machine learning system comprises graphics processing unit that comprises multiple processing cores on single integrated circuit 22 the machine learning system of claim 21 wherein the first learning coach machine learning system comprises graphic processing unit 23 the machine learning system of claim 1 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises controlling data flow to members of the ensemble 24 the machine learning system of claim 1 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises adding new member to the ensemble 25 computer system comprising first set of one or more processing cores first set of one or more computer readable media first student machine learning module maintained on the first set of one or more computer readable media that when executed by the first set of one or more processing cores causes the first set of one or more processing cores to using machine learning automatically learn from and make predictions on input source data second set of one or more processing cores second set of one or more computer readable media and first learning coach machine learning module maintained on the second set of computer readable media that when executed by the second set of one or more processing cores causes the second set of one or more processing cores to receive input data about an internal state of the first student machine learning module and using machine learning automatically learn and implement change to the first student machine learning module based on the data about the internal state of the first student machine learning module to improve operation of the first student machine learning module 26 the machine learning computer system of claim 25 wherein the first student machine learning module comprises deep neural network 27 the machine learning computer system of claim 26 wherein the first learning coach machine learning module utilizes machine learning architecture that is not deep neural network 28 the machine learning computer system of claim 25 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning module that improve the performance of the first student machine learning module 29 the machine learning computer system of claim 25 wherein the enhancement comprises structural change the first student machine learning module 30 the machine learning computer system of claim 25 wherein the enhancement comprises structural change the first student machine learning module 31 the machine learning computer system of claim 30 wherein the first student machine learning module comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning module 32 the machine learning computer system of claim 30 wherein the first student machine learning module comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning module 33 the machine learning computer system of claim 25 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning module to control the learning of the first student machine learning module 34 the machine learning computer system of claim 25 wherein the first set of one or more processing cores comprises first graphics processing unit that comprises multiple processing cores on first single integrated circuit 35 the machine learning computer system of claim 34 wherein the second set of one or more processing cores comprises second graphics processing unit that comprises multiple processing cores on second single integrated circuit 36 method of improving operation of first student machine learning system that using machine learning automatically learns from and makes predictions on input source data the method comprising receiving by first learning coach machine learning system from the first student machine learning system data about an internal state of the first student machine learning system using machine learning automatically learning and implementing by the first learning coach machine learning system an enhancement to the first student machine learning system based on the data about the internal state of the first student machine learning system to improve operation of the first student machine learning system 37 the method of claim 36 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning system that improve learning by the first student machine learning system 38 the method of claim 36 wherein the enhancement comprises structural change the first student machine learning system 39 the method of claim 38 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning system 40 the method of claim 38 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning system 41 the method of claim 36 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control learning of the first student machine learning system 42 the method of claim 36 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises controlling data flow to members of the ensemble 43 the method of claim 36 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises adding new member to the ensemble lang en",
    "claims2": "claims claim_text 1 method of aiding in diagnosing whether subject has depression comprising analyzing blood sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid alphahydroxyisobutyric acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the biological sample is blood plasma claim_text 8 the method of claim 1 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 9 the method of claim 8 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 10 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and the first sample is obtained from the subject at first time point and wherein the analysis method the blood sample is mass spectrometry analyzing second biological sample that was removed from subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 11 the method of claim 10 wherein the method further comprises comparing the levels of the biomarkers in the first sample the levels of the biomarkers in the second sample andor the results of the comparison of the levels of the biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the biomarkers claim_text 12 the method of claim 10 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 13 the method of claim 12 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 14 method of assessing the efficacy of composition treating depression comprising analyzing blood sample removed from subject having depression and currently or previously being treated with composition to determine the levels of biomarkers depression wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to levels of the biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the biomarkers c depressionnegative reference levels of the biomarkers d depressionprogressionpositive reference levels of the biomarkers andor depressionregressionpositive reference levels of the biomarkers claim_text 15 the method of claim 14 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 16 the method of claim 15 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 17 method assessing the efficacy of composition in treating depression comprising analyzing first blood sample obtained from subject to determine the levels of biomarkers depression the first sample obtained from the subject at first time point wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry administering the composition to the subject analyzing second biological sample obtained from the subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point after administration of the composition and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 18 the method of claim 17 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids lang en",
    "label": 0
  },
  {
    "id1": "032-496-986-475-726",
    "id2": "189-406-053-095-096",
    "claims1": "claims claim_text computerimplemented method improving locality of machine learning models the method performed by data processing apparatus the method comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the method of claim 1 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the method of claim 3 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the method of one of claims 1 to 5 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text machine learning computations system comprisingn data processing apparatus and n memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the system of claim 7 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the system of claim 9 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the system of one of claims 7 to 11 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the nontransitory computer storage medium of claim 13 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time of the at least one compound in the chromatography bed such that equation i is satisfied equation i t res t diff equation i calculating concentration czt of the at least one compound in the mobile phase at predetermined location of the chromatography device and at predetermined time t based on the adsorption isotherm claim_text 2 the method according to claim 1 wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 3 the method according to claim 1 wherein the minimum residence time of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 4 the method according to claim 1 wherein in the calculation step czt is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 5 the method according to claim 4 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 6 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 7 the method according claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 8 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 9 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 10 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 11 the method according to claim 10 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 12 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 13 the chromatography method according to claim 12 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising iii carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 15 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 11 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "031-656-825-993-606",
    "id2": "062-542-216-489-868",
    "claims1": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "claims2": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "label": 1
  },
  {
    "id1": "030-076-623-736-233",
    "id2": "114-059-893-173-320",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "claims2": "claims claim_text 1 method implemented by federated learning server in first management domain the method comprising obtaining first machine learning model from machine learning model management center performing federated learning in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model and sending the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text 2 the method of claim 1 further comprising sending machine learning model requirement information to the machine learning model management center wherein obtaining the first machine learning model comprises receiving the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 3 the method of claim 2 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 4 the method of claim 3 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 5 the method of claim 1 further comprising sending access permission information of the second machine learning model to the machine learning model management center claim_text 6 the method of claim 1 further comprising sending the second machine learning model to federated learning clients claim_text 7 the method of claim 1 further comprising determining that an application effect of the second machine learning model meets preset condition claim_text 8 the method of claim 1 wherein performing the federated learning comprises sending the first machine learning model to federated learning clients to enable the federated learning clients to perform the federated learning based on the first machine learning model and network service data and to obtain intermediate machine learning models of the federated learning client obtaining the intermediate machine learning models from the federated learning clients and aggregating the intermediate machine learning models to obtain the second machine learning model claim_text 9 method implemented by machine learning model management center and comprising sending first machine learning model to first federated learning server in first management domain receiving second machine learning model from the first federated learning server wherein the second machine learning model is based on first federated learning in the first management domain using the first machine learning model and first local network service data in the first management domain and replacing the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text 10 the method of claim 9 wherein before sending the first machine learning model the method further comprises receiving machine learning model requirement information from the first federated learning server and determining the first machine learning model based on the machine learning model requirement information claim_text 11 the method of claim 10 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 12 the method of claim 11 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 13 the method of claim 9 wherein the second machine learning model is based on first training framework wherein the method further comprises converting the second machine learning model into third machine learning model based on second training framework and wherein the third machine learning model and the second machine learning model correspond to same model service information claim_text 14 the method of claim 9 further comprising receiving access permission information of the second machine learning model from the first federated learning server claim_text 15 the method of claim 9 further comprising sending the second machine learning model to second federated learning server in the second management domain receiving fourth machine learning model from the second federated learning server wherein the fourth machine learning model is based on second federated learning in the second management domain using the second machine learning model and second local network service data in the second management domain and replacing the second machine learning model with the fourth machine learning model claim_text 16 federated learning system comprising federated learning server in first management domain and configured to obtain first machine learning model from machine learning model management center send the first machine learning model obtain intermediate machine learning models aggregate the intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain and federated learning clients in the first management domain and configured to receive the first machine learning model from the federated learning server and perform first federated learning based on the first machine learning model and local network service data in the first management domain to obtain the intermediate machine learning models claim_text 17 the federated learning system of claim 16 wherein the federated learning server is further configured to send the second machine learning model to the federated learning clients and wherein the federated learning clients are further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text 18 the federated learning system of claim 16 wherein the federated learning server is further configured to send machine learning model requirement information to the machine learning model management center and receive the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 19 the federated learning system of claim 16 wherein the federated learning server is further configured to send access permission information of the second machine learning model to the machine learning model management center claim_text 20 the federated learning system of claim 16 wherein the federated learning server is further configured to determine that an application effect of the second machine learning model meets preset condition lang en",
    "label": 1
  },
  {
    "id1": "023-604-939-311-912",
    "id2": "024-958-324-997-756",
    "claims1": "claims claim_text claims 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computer readable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text machine learning model management method executed by federated learning server wherein the federated learning server is in first management domain and is connected to machine learning model management center and the method comprisesn obtaining first machine learning model from the machine learning model management center n performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model and n sending the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the method according to claim 1 wherein the obtaining first machine learning model from the machine learning model management center comprisesn sending machine learning model requirement information to the machine learning model management center and n receiving the first machine learning model determined by the machine learning model management center based on the machine learning model requirement information claim_text the method according to claim 2 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model andor machine learning model training requirement claim_text the method according to claim 3 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the method according to any one of claims 1 to 4 wherein the method further comprises nsending access permission information of the second machine learning model to the machine learning model management center claim_text the method according to any one of claims 1 to 5 wherein the method further comprises nsending the second machine learning model to the plurality of federated learning clients claim_text the method according to any one of claims 1 to 6 wherein the sending the second machine learning model to the machine learning model management center comprises nsending the second machine learning model to the machine learning model management center if an application effect of the second machine learning model meets preset condition claim_text the method according to any one of claims 1 to 7 wherein the performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model comprisesn sending the first machine learning model to the plurality of federated learning clients in the first management domain to enable each of the plurality of federated learning clients to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n obtaining plurality of intermediate machine learning models obtained by the plurality of federated learning clients and aggregating the plurality of intermediate machine learning models to obtain the second machine learning model claim_text machine learning model management method executed by machine learning model management center wherein the machine learning model management center is connected to first federated learning server and the first federated learning server is in first management domain and the method comprisesn sending first machine learning model to the first federated learning server n receiving second machine learning model from the first federated learning server wherein the second machine learning model is obtained by the first federated learning server by performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain and n replacing the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text the method according to claim 9 wherein before the sending first machine learning model to the first federated learning server the method further comprisesn receiving machine learning model requirement information sent by the first federated learning server and n determining the first machine learning model based on the machine learning model requirement information claim_text the method according to claim 10 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the method according to claim 11 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the method according to any one of claims 9 to 12 wherein the second machine learning model is machine learning model based on first training framework and the method further comprises nconverting the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the method according to any one of claims 9 to 13 wherein the method further comprises nreceiving access permission information that is of the second machine learning model and that is sent by the first federated learning server claim_text the method according to any one of claims 9 to 14 wherein the method further comprisesn sending the second machine learning model to second federated learning server wherein the second federated learning server is in the second management domain n receiving fourth machine learning model from the second federated learning server wherein the fourth machine learning model is obtained by the second federated learning server by performing federated learning with plurality of federated learning clients in the second management domain based on the second machine learning model and local network service data in the second management domain and n replacing the second machine learning model with the fourth machine learning model claim_text federated learning system comprising federated learning server and plurality of federated learning clients wherein the federated learning server and the plurality of federated learning clients are in first management domain and the federated learning server is connected to machine learning model management center whereinn the federated learning server is configured to obtain first machine learning model from the machine learning model management center and send the first machine learning model to the plurality of federated learning clients n each of the plurality of federated learning clients is configured to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n the federated learning server is further configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients aggregate the plurality of intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the federated learning system according to claim 16 whereinn the federated learning server is further configured to send the second machine learning model to the plurality of federated learning clients and n each of the plurality of federated learning clients is further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text network system comprising machine learning model management center federated learning server and plurality of federated learning clients wherein the federated learning server and the plurality of federated learning clients are in first management domain and the federated learning server is connected to the machine learning model management center whereinn the machine learning model management center is configured to send first machine learning model to the federated learning server n the federated learning server is configured to send the first machine learning model to the plurality of federated learning clients n each of the plurality of federated learning clients is configured to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client n the federated learning server is further configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients aggregate the plurality of intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain and n the machine learning model management center is further configured to replace the first machine learning model with the second machine learning model claim_text the network system according to claim 18 whereinn the federated learning server is further configured to send machine learning model requirement information to the machine learning model management center and n the machine learning model management center is further configured to send the first machine learning model to the federated learning server based on the machine learning model requirement information claim_text the network system according to claim 19 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the network system according to claim 20 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the network system according to any one of claims 18 to 21 wherein the second machine learning model is machine learning model based on first training framework and nthe machine learning model management center is further configured to convert the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the network system according to any one of claims 18 to 22 whereinn the federated learning server is further configured to send the second machine learning model to the plurality of federated learning clients and n each of the plurality of federated learning clients is further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text federated learning server wherein the federated learning server is in first management domain and is connected to machine learning model management center and the federated learning server comprisesn transceiver unit configured to obtain first machine learning model from the machine learning model management center and n processing unit configured to perform federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model wherein n the transceiver unit is further configured to send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the federated learning server according to claim 24 wherein the transceiver unit is specifically configured ton send machine learning model requirement information to the machine learning model management center and n receive the first machine learning model determined by the machine learning model management center based on the machine learning model requirement information claim_text the federated learning server according to claim 25 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the federated learning server according to claim 26 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the federated learning server according to any one of claims 24 to 27 wherein nthe transceiver unit is further configured to send access permission information of the second machine learning model to the machine learning model management center claim_text the federated learning server according to any one of claims 24 to 28 wherein nthe transceiver unit is further configured to send the second machine learning model to the plurality of federated learning clients claim_text the federated learning server according to any one of claims 24 to 29 wherein nthe transceiver unit is specifically configured to send the second machine learning model to the machine learning model management center if an application effect of the second machine learning model meets preset condition claim_text the federated learning server according to any one of claims 24 to 30 whereinn the transceiver unit is further configured to send the first machine learning model to the plurality of federated learning clients to enable each of the plurality of federated learning clients to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n the processing unit is specifically configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients and aggregate the plurality of intermediate machine learning models to obtain the second machine learning model claim_text machine learning model management center wherein the machine learning model management center is connected to first federated learning server and the first federated learning server is in first management domain and the machine learning model management center comprisesn sending unit configured to send first machine learning model to the first federated learning server n receiving unit configured to receive second machine learning model from the first federated learning server wherein the second machine learning model is obtained by the first federated learning server by performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain and n processing unit configured to replace the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text the machine learning model management center according to claim 32 whereinn the receiving unit is further configured to receive machine learning model requirement information sent by the first federated learning server and n the processing unit is further configured to determine the first machine learning model based on the machine learning model requirement information claim_text the machine learning model management center according to claim 33 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the machine learning model management center according to claim 34 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the machine learning model management center according to any one of claims 32 to 35 wherein the second machine learning model is machine learning model based on first training framework and the processing unit is further configured to nconvert the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the machine learning model management center according to any one of claims 32 to 36 wherein nthe receiving unit is further configured to receive access permission information that is of the second machine learning model and that is sent by the first federated learning server claim_text the machine learning model management center according to any one of claims 32 to 37 whereinn the sending unit is further configured to send the second machine learning model to second federated learning server wherein the second federated learning server is in the second management domain n the receiving unit is further configured to receive fourth machine learning model from the second federated learning server wherein the fourth machine learning model is obtained by the second federated learning server by performing federated learning with plurality of federated learning clients in the second management domain based on the second machine learning model and local network service data in the second management domain and n the processing unit is further configured to replace the second machine learning model with the fourth machine learning model claim_text machine learning model management apparatus comprising memory and processor wherein the memory is configured to store computer instructions and the processor is configured to invoke the computer instructions to perform the method according to any one of claims 1 to 15 claim_text computerreadable storage medium wherein the computerreadable storage medium stores computer program and when the computer program is run on computer the computer is enabled to perform the method according to any one of claims 1 to 15 lang en",
    "label": 1
  },
  {
    "id1": "160-348-204-331-805",
    "id2": "057-360-526-753-072",
    "claims1": "claims claim_text 1 34 canceled claim_text 35 computerimplemented method executing machinelearning model performed in first network node the method comprising developing first machinelearning model based on first set of data and using machinelearning algorithm communicating to second network node the first machinelearning model receiving from the second network node information about difference between the first machinelearning model and second machinelearning model receiving request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtaining information indicative of an execution policy and depending on the obtained information indicative of the execution policy either executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result claim_text 36 the method according to claim 35 wherein partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model at the second network node to obtain result comprises partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model based on the information about difference between the first machinelearning model and the second machinelearning model claim_text 37 the method according to claim 35 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises any one or more of information that the second machinelearning model is different from the first machinelearning model information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model information identifying difference between the first machinelearning model and the second machinelearning model or the second machinelearning model claim_text 38 the method according to claim 35 wherein the information indicative of an execution policy is obtained from policy node or is obtained from memory in the first network node claim_text 39 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating execution of the machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model said information further comprises information indicating that at least part of said machinelearning model should be executed in an enclaved mode claim_text 40 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating partial execution of the first machinelearning model and partial execution of the second machinelearning model said information further comprising information indicating that at least one component of said first machinelearning model or of said second machinelearning model should be executed in an enclaved mode claim_text 41 the method according to claim 35 wherein the first machinelearning model and the second machinelearning model are representable computational graphs claim_text 42 the method according to claim 41 wherein the computational graphs are directed acyclic graphs claim_text 43 the method according to claim 41 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 44 the method according to claim 35 wherein the step of executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result comprises at least partially executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model in an enclaved memory segment claim_text 45 the method according to claim 35 wherein the step of partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result comprises one or more of executing at least one component of the first machinelearning model in an enclaved memory segment causing the second network node to partially execute the second machinelearning model in an enclaved memory segment partially executing the first machinelearning model to form first partial result communicating to the second network node the first partial result causing the partial execution of the second machinelearning model using the first partial result at the second network node such that second partial result is formed at the second network node receiving from the second network node the second partial result or partially executing the first machinelearning model using the second partial result to form final result claim_text 46 first network node executing machinelearning model the first network node comprising an interface configured allowing communication with other network nodes and processing circuitry operatively associated with the interface and configured to develop first machinelearning model based on first set of data and using machinelearning algorithm communicate the first machinelearning model to second network node receive from the second network node information about difference between the first machinelearning model and second machinelearning model receive request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtain information indicative of an execution policy and depending on the obtained information indicative of an execution policy either execute machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially execute the first machinelearning model and cause the second network node to partially execute the second machinelearning model to obtain result claim_text 47 method executing machinelearning model performed in second network node the method comprising receiving from first network node first machinelearning model developing second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicating to the first network node information about difference between the first machinelearning model and the second machinelearning model receiving from the first network node partial result partially executing the second machinelearning model using the first partial result to form second partial result and communicating to the first network node the second partial result claim_text 48 the method according to claim 47 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises information that the second machinelearning model is different from the first machinelearning model andor information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model andor information identifying difference between the first machinelearning model and the second machinelearning model andor the second machinelearning model claim_text 49 the method according to claim 47 wherein the machinelearning model and the second machinelearning model are representable computational graphs claim_text 50 the method according to claim 49 wherein the computational graphs are directed acyclic graphs claim_text 51 the method according to claim 49 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 52 the method according to claim 47 wherein the step of partially executing the second machinelearning model using the first partial result to form second partial result comprises executing at least one component of the second machinelearning model at the second node in an enclaved memory segment claim_text 53 network node executing machinelearning model the network node referred to second network node and comprising an interface configured allowing communication with other network nodes processing circuitry operatively associated with the interface and configured to receive from first network node first machinelearning model develop second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicate to the first network node information about difference between the first machinelearning model and the second machinelearning model receive from the first network node partial result partially execute the second machinelearning model using the first partial result to form second partial result and communicate to the first network node the second partial result lang en",
    "claims2": "claims claim_text 1 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer setting detected number of active connections in counters provided each output port and controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections wherein said controlling step further includes outputting cell addressed to an output port corresponding to counter with said count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to said output port and subtracting value of said counter corresponding to an output port that has output cells at every output of cell claim_text 2 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of cell addressed to said output port from an input multiplex buffer claim_text 3 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of setting detected number of active connections in counters provided each output port and sequentially outputting cell addressed to an output port corresponding to counter with count value not set to 0 among said counters and subtracting value of counter corresponding to an output port that has output cell at every output of said cell when outputting cells accumulated in an output separation buffer to an output port claim_text 4 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output from said output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of the cells addressed to said output from an input multiplex buffer claim_text 5 an asynchronous transfer mode hereinafter referred to atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively input multiplex buffer control means controlling said input multiplex buffer so that output of cell addressed to certain output port indicated by first back pressure signal is inhibited and output separation buffer control means monitoring an accumulation amount of cells in said output separation buffer at every output port to which said cell is addressed and outputting said first back pressure signal to said input multiplex buffer control means to inhibit output of cell addressed to an output port having said accumulation amount equal to or more than predetermined threshold value detection means detecting number of active connections existing between each output port and each input multiplex buffer and said output separation buffer control means further comprises output control means controlling number of cells addressed to said output port output from an output separation buffer so that said number of cells are in proportional to number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said output control means further comprises counter each provided an output port means setting each number of active connections detected by said detection by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting counter value corresponding to an output port which has output cells at every output of cell claim_text 6 the atm switch of claim 5 wherein said output control means comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to said output separation buffer when said cells output from said output separation buffer are equal to or larger than cells input thereto and means outputting said first back pressure signal inhibiting output of cells addressed to said detected output port to said input multiplex buffer control means claim_text 7 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises counter provided each output port means setting each number of active connections detected by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting value of said counter corresponding to an output port that has output cell at every output of cell claim_text 8 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to an output separation buffer when cells output separation buffer when cells output from said output separation buffer is equal to or more than cells input thereto and means outputting said first back pressure signal inhibiting output of cell addressed to said detected output port to said input multiplex buffer control means lang en",
    "label": 0
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "152-806-746-371-786",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 pipeline type processor asynchronous transfer mode atm cells comprisingninput buffers storing n pieces of asynchronous transmission mode atm cellsnn pairs of processor and buffer pipeline processing said each cell the processors provided in first pipeline arrangement and the buffers provided in second pipeline arrangementnan address determining section determining pipeline processing condition said address determining section determining processing instruction code starting address of said each cell based upon the information of said cell stored in said input buffer andnan n1 address transferring sections respectively provided between adjacent ones of said processors in the first pipeline arrangement each of said address transferring sections transferring said processing instruction code starting address between one of the adjacent ones of said processors on an incoming side of the first pipeline arrangement to an other of the adjacent ones of said processors on an outgoing side of the pipeline arrangementnwherein said processors sequentially execute pipeline processing based upon said processing instruction code starting address andnwherein the first pipeline arrangement is separate from the second pipeline arrangement and wherein said each cell is passed along the second pipeline arrangement of said buffers execution by said respective processors at different one of plurality of states claim_text 2 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein each of said address determining sections includesnan information storing memory storing information required retrievalnan address storing memory storing an address at which processing instruction code executed by said processor is storedna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining section claim_text 3 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 4 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci claim_text 5 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said address determining section determines storing address an instruction code according to data format claim_text 6 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said pipeline processing condition is not determined by any of said processors but instead is provided respective processing instruction code starting address each said processor claim_text 7 pipeline type processor asynchronous transfer mode atm cells provided with plurality of processors each executing partial process dedicated to each processor an atm cell being stored into buffer corresponding to each processor said pipeline type processor comprisingnan address storing memory which stores plurality of sets of predetermined process starting addresses each of processors constituting said pipeline type processor each set of predetermined process starting addresses corresponding to conditions of the atm cell to be processed in different respective statenan address retrieving section which retrieves set of predetermined process starting addresses from said address storing memory in accordance with conditions extracted from an input atm cell and transfers each predetermined processing starting address to the corresponding processor andnan execution control section which detects atm cell arrival at each buffer corresponding to each processor and instructs the execution of process to the processor when the atm cell arrives at the corresponding buffernwherein said processors are disposed in first pipeline arrangement separate from second pipeline arrangement in which said buffers are disposed claim_text 8 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein at least one of said processors includes state transition control sections which read instruction code from memory and decode and execute pipeline processing based upon said starting address claim_text 9 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprising an address transferring section provided between said processors said address transferring section transferring said processing instruction code starting address claim_text 10 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprisingnan input buffernan information storing memory storing information required retrievalna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining sectionnwherein the processing instruction code starting address is respectively different each of said processors based on plurality of states the retrieval condition that are obtained from the information storing memory in correspondence with particular condition that matches the retrieval condition claim_text 11 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 12 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci lang en",
    "label": 0
  },
  {
    "id1": "151-548-458-500-410",
    "id2": "130-909-330-439-20X",
    "claims1": "claims claim_text 1 47 canceled claim_text 48 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by broker component maintaining provider register containing information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the method comprising receiving request desired machine learning model from machine learning model consumer determining based on the information contained in the provider register machine learning model among the machine learning models provided by the machine learning model providers that matches the desired machine learning model and sending response to the machine learning model consumer providing information associated with the determined machine learning model claim_text 49 the method of claim 48 wherein the request includes information characterizing the desired machine learning model and wherein determining the machine learning model that matches the desired machine learning model includes matching the information characterizing the desired machine learning model with the information contained in the provider register claim_text 50 the method of claim 49 wherein the information characterizing the desired machine learning model includes at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 51 the method of claim 48 wherein the information contained in the provider register includes each machine learning model provided by one of the plurality of machine learning model providers at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 52 the method of claim 48 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes notification on the availability of the determined machine learning model and wherein the response including the notification is sent to the machine learning model consumer conditionally when the determined machine learning model matches the desired machine learning model better than machine learning model previously sent to the machine learning model consumer matching the desired machine learning model claim_text 53 the method of claim 52 further comprising receiving upon receiving the request the desired machine learning model registration message from machine learning model provider to register its machine learning models with the provider register wherein determining the machine learning model that matches the desired machine learning model includes checking the machine learning models registered by the registration message on match with the desired machine learning model if match with the desired machine learning model is determined sending request the determined machine learning model to the machine learning model provider providing the determined machine learning model and receiving the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the request claim_text 54 the method of claim 48 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes an output value output by the determined machine learning model in response to the one or more input values claim_text 55 the method of claim 54 further comprising sending the one or more input values to the machine learning model provider providing the determined machine learning model input to the desired machine learning model and receiving an output value output by the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the one or more input values claim_text 56 the method of claim 48 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes access information to the machine learning model provider providing the determined machine learning model claim_text 57 the method of claim 48 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 58 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model consumer and comprising sending request desired machine learning model to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers and receiving response from the broker component providing information associated with machine learning model determined by the broker component among the machine learning models provided by the machine learning model providers matching the desired machine learning model claim_text 59 the method of claim 58 wherein the request includes information characterizing the desired machine learning model the information characterizing the desired machine learning model including at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 60 the method of claim 58 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response from the broker component includes notification on the availability of the determined machine learning model and optionally the determined machine learning model claim_text 61 the method of claim 58 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model claim_text 62 the method of claim 58 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response from the broker component includes access information to the machine learning model provider providing the determined machine learning model claim_text 63 the method of claim 62 further comprising sending to the machine learning model provider providing the determined machine learning model using the access information request to use the desired machine learning model wherein the request includes one or more input values to be passed input to the desired machine learning model and receiving from the machine learning model provider providing the determined machine learning model an output value output by the determined machine learning model in response to the one or more input values claim_text 64 the method of claim 58 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 65 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model provider of the plurality of machine learning model providers and comprising sending to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the provider register enabling the broker component to determine machine learning model among the machine learning models provided by the plurality of machine learning model providers that matches desired machine learning model requested by machine learning model consumer registration message to register machine learning models provided by the machine learning model provider with the provider register of the broker component claim_text 66 the method of claim 65 wherein the registration message includes each machine learning model provided by the machine learning model provider at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 67 the method of claim 65 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time t res of the at least one compound in the chromatography bed such that equation i is satisfied t res t diff equation i calculating concentration cz t of the at least one compound in the mobile phase at predetermined location z of the chromatography bed and at predetermined time t based on the adsorption isotherm wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 2 the method according to claim 1 wherein the minimum residence time t res of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 3 the method according to claim 1 wherein in the calculation step cz t is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 4 the method according to claim 3 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 5 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 6 the method according to claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 7 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 8 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 9 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 10 the method according to claim 9 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 11 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 12 the chromatography method according to claim 11 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 13 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 9 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "160-348-204-331-805",
    "id2": "040-970-435-595-428",
    "claims1": "claims claim_text 1 34 canceled claim_text 35 computerimplemented method executing machinelearning model performed in first network node the method comprising developing first machinelearning model based on first set of data and using machinelearning algorithm communicating to second network node the first machinelearning model receiving from the second network node information about difference between the first machinelearning model and second machinelearning model receiving request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtaining information indicative of an execution policy and depending on the obtained information indicative of the execution policy either executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result claim_text 36 the method according to claim 35 wherein partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model at the second network node to obtain result comprises partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model based on the information about difference between the first machinelearning model and the second machinelearning model claim_text 37 the method according to claim 35 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises any one or more of information that the second machinelearning model is different from the first machinelearning model information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model information identifying difference between the first machinelearning model and the second machinelearning model or the second machinelearning model claim_text 38 the method according to claim 35 wherein the information indicative of an execution policy is obtained from policy node or is obtained from memory in the first network node claim_text 39 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating execution of the machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model said information further comprises information indicating that at least part of said machinelearning model should be executed in an enclaved mode claim_text 40 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating partial execution of the first machinelearning model and partial execution of the second machinelearning model said information further comprising information indicating that at least one component of said first machinelearning model or of said second machinelearning model should be executed in an enclaved mode claim_text 41 the method according to claim 35 wherein the first machinelearning model and the second machinelearning model are representable computational graphs claim_text 42 the method according to claim 41 wherein the computational graphs are directed acyclic graphs claim_text 43 the method according to claim 41 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 44 the method according to claim 35 wherein the step of executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result comprises at least partially executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model in an enclaved memory segment claim_text 45 the method according to claim 35 wherein the step of partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result comprises one or more of executing at least one component of the first machinelearning model in an enclaved memory segment causing the second network node to partially execute the second machinelearning model in an enclaved memory segment partially executing the first machinelearning model to form first partial result communicating to the second network node the first partial result causing the partial execution of the second machinelearning model using the first partial result at the second network node such that second partial result is formed at the second network node receiving from the second network node the second partial result or partially executing the first machinelearning model using the second partial result to form final result claim_text 46 first network node executing machinelearning model the first network node comprising an interface configured allowing communication with other network nodes and processing circuitry operatively associated with the interface and configured to develop first machinelearning model based on first set of data and using machinelearning algorithm communicate the first machinelearning model to second network node receive from the second network node information about difference between the first machinelearning model and second machinelearning model receive request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtain information indicative of an execution policy and depending on the obtained information indicative of an execution policy either execute machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially execute the first machinelearning model and cause the second network node to partially execute the second machinelearning model to obtain result claim_text 47 method executing machinelearning model performed in second network node the method comprising receiving from first network node first machinelearning model developing second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicating to the first network node information about difference between the first machinelearning model and the second machinelearning model receiving from the first network node partial result partially executing the second machinelearning model using the first partial result to form second partial result and communicating to the first network node the second partial result claim_text 48 the method according to claim 47 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises information that the second machinelearning model is different from the first machinelearning model andor information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model andor information identifying difference between the first machinelearning model and the second machinelearning model andor the second machinelearning model claim_text 49 the method according to claim 47 wherein the machinelearning model and the second machinelearning model are representable computational graphs claim_text 50 the method according to claim 49 wherein the computational graphs are directed acyclic graphs claim_text 51 the method according to claim 49 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 52 the method according to claim 47 wherein the step of partially executing the second machinelearning model using the first partial result to form second partial result comprises executing at least one component of the second machinelearning model at the second node in an enclaved memory segment claim_text 53 network node executing machinelearning model the network node referred to second network node and comprising an interface configured allowing communication with other network nodes processing circuitry operatively associated with the interface and configured to receive from first network node first machinelearning model develop second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicate to the first network node information about difference between the first machinelearning model and the second machinelearning model receive from the first network node partial result partially execute the second machinelearning model using the first partial result to form second partial result and communicate to the first network node the second partial result lang en",
    "claims2": "claims claim_text what is claimed is 1 method of processing data comprising receiving at an processing device set of global parameters each machine learning model of plurality of machine learning models each respective machine learning model of the plurality of machine learning models processing at the processing device data stored locally on the processing device with respective machine learning model according to the set of global parameters to generate machine learning model output receiving at the processing device user feedback regarding the machine learning model output performing at the processing device an optimization of the respective machine learning model based on the machine learning model output and the user feedback associated with machine learning model output to generate locally updated machine learning model parameters and sending the locally updated machine learning model parameters to remote processing device and receiving from the remote processing device set of globally updated machine learning model parameters each machine learning model of the plurality of machine learning models wherein the set of globally updated machine learning model parameters each respective machine learning model are based at least in part on the locally updated machine learning model parameters 2 the method of claim 1 further comprising performing at the processing device number of optimizations before sending the locally updated machine learning model parameters to the remote processing device 3 the method of claim 1 wherein the set of globally updated machine learning model parameters each respective machine learning model of the plurality of machine learning models are based at least in part on locally updated machine learning model parameters of second processing device 4 the method of claim 1 wherein the user feedback comprises an indication of correctness of the machine learning model output 5 the method of claim 1 wherein the data stored locally on the processing device is one of image data audio data or video data 6 the method of claim 1 wherein the processing device is one of smartphone or an internet of things device 7 the method of claim 1 wherein processing at the processing device the data stored locally on the processing device with the machine learning model is performed at least in part by one or more neural processing units 8 the method of claim 1 wherein performing at the processing device the optimization of the machine learning model is performed at least in part by one or more neural processing units 9 processing device comprising memory comprising computerexecutable instructions one or more processors configured to execute the computerexecutable instructions and cause the processing device to receive set of global parameters each machine learning model of plurality of machine learning models each respective machine learning model of the plurality of machine learning models process data stored locally on processing device with respective machine learning model according to the set of global parameters to generate machine learning model output receive user feedback regarding machine learning model output perform an optimization of the respective machine learning model based on the machine learning model output and the user feedback associated with machine learning model output to generate locally updated machine learning model parameters and send the locally updated machine learning model parameters to remote processing device and receive from the remote processing device set of globally updated machine learning model parameters each machine learning model of the plurality of machine learning models wherein the set of globally updated machine learning model parameters each respective machine learning model are based at least in part on the locally updated machine learning model parameters 10 the processing device of claim 9 wherein the one or more processors are further configured to cause the processing device to perform number of optimizations before sending the locally updated machine learning model parameters to the remote processing device 11 the processing device of claim 9 wherein the set of globally updated machine learning model parameters each respective machine learning model of the plurality of machine learning models are based at least in part on locally updated machine learning model parameters of second processing device 12 the processing device of claim 9 wherein the user feedback comprises an indication of correctness of the machine learning model output 13 the processing device of claim 9 wherein the processing device is one of smartphone or an internet of things device 14 the processing device of claim 9 wherein one of the one or more processors is neural processing unit configured to process the data stored locally on the processing device with the machine learning model 15 the processing device of claim 9 wherein one of the one or more processors is neural processing unit configured to perform the optimization of the machine learning model 16 method of processing data comprising each respective machine learning model of plurality of machine learning models each respective remote processing device of plurality of remote processing devices sending from server to the respective remote processing device an initial set of global model parameters the respective machine learning model and receiving at the server from the respective remote processing device an updated set of model parameters the respective machine learning model and performing at the server an optimization of the respective machine learning model based on the updated set of model parameters received from each remote processing device of the plurality of remote processing devices to generate an updated set of global model parameters and sending from the server to each remote processing device of the plurality of remote processing devices the updated set of global model parameters each machine learning model of the plurality of machine learning models 17 the method of claim 16 wherein performing at the server an optimization of the respective machine learning model comprises computing an effective gradient each model parameter of the initial set of global model parameters the respective machine learning model 18 the method of claim 16 further comprising each respective machine learning model of the plurality of machine learning models determining corresponding density estimator parameterized by weighting parameters the respective machine learning model 19 the method of claim 18 further comprising determining prior mixture weights the respective machine learning model 20 the method of claim 16 wherein the plurality of remote processing devices comprises smartphone 21 the method of claim 16 wherein the plurality of remote processing devices comprise an internet of things device 22 the method of claim 16 wherein each respective machine learning model of the plurality of machine learning models is neural network model 23 the method of claim 22 wherein each respective machine learning model of the plurality of machine learning models comprises same network structure 24 processing device comprising memory comprising computerexecutable instructions one or more processors configured to execute the computerexecutable instructions and cause the processing device to each respective machine learning model of plurality of machine learning models each respective remote processing device of plurality of remote processing devices send to the respective remote processing device an initial set of global model parameters the respective machine learning model and receive from the respective remote processing device an updated set of model parameters the respective machine learning model and perform an optimization of the respective machine learning model based on the updated set of model parameters received from each remote processing device of the plurality of remote processing devices to generate an updated set of global model parameters and send to each remote processing device of the plurality of remote processing devices the updated set of global model parameters each machine learning model of the plurality of machine learning models 25 the processing device of claim 24 wherein in order to perform the optimization of the respective machine learning model the one or more processors are further configured to cause the processing device to compute an effective gradient each model parameter of the initial set of global model parameters the respective machine learning model 26 the processing device of claim 24 wherein the one or more processors are further configured to cause the processing device to each respective machine learning model of the plurality of machine learning models determine corresponding density estimator parameterized by weighting parameters the respective machine learning model 27 the processing device of claim 26 wherein the one or more processors are further configured to cause the processing device to each respective machine learning model of the plurality of machine learning models determine prior mixture weights the respective machine learning model 28 the processing device of claim 24 wherein the plurality of remote processing devices comprises smartphone 29 the processing device of claim 24 wherein each respective machine learning model of the plurality of machine learning models is neural network model 30 the processing device of claim 29 wherein each respective machine learning model of the plurality of machine learning models comprises same network structure lang en",
    "label": 1
  },
  {
    "id1": "116-391-006-009-952",
    "id2": "160-348-204-331-805",
    "claims1": "claims claim_text claims 1 method of training machine leaming model having plurality of parameters wherein the machine learning model has been trained on first machine leaming task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine leaming task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine leaming task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine leaming model achieving acceptable performance on the first machine learning task 2 the method of claim 1 wherein the first machine leaming task and the second machine leaming task are different supervised learning tasks 3 the method of claim 1 wherein the first machine leaming task and the second machine leaming tasks are different reinforcement learning tasks 4 the method of any one of claims 1 3 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine leaming model in accordance with current values of parameters of the machine leaming model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the obj ective function 6 the method of any one of claims 4 or 5 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter 7 the method of any one of claims 1 6 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task 8 the method of any one of claims 1 7 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining fisher information matrix fim of the plurality of parameters of the machine leaming model with respect to the first machine leaming task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim 9 the method of any one of claims 1 8 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter 13 the method of any one of claims 48 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models 15 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 16 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 lang en",
    "claims2": "claims claim_text 1 34 canceled claim_text 35 computerimplemented method executing machinelearning model performed in first network node the method comprising developing first machinelearning model based on first set of data and using machinelearning algorithm communicating to second network node the first machinelearning model receiving from the second network node information about difference between the first machinelearning model and second machinelearning model receiving request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtaining information indicative of an execution policy and depending on the obtained information indicative of the execution policy either executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result claim_text 36 the method according to claim 35 wherein partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model at the second network node to obtain result comprises partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model based on the information about difference between the first machinelearning model and the second machinelearning model claim_text 37 the method according to claim 35 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises any one or more of information that the second machinelearning model is different from the first machinelearning model information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model information identifying difference between the first machinelearning model and the second machinelearning model or the second machinelearning model claim_text 38 the method according to claim 35 wherein the information indicative of an execution policy is obtained from policy node or is obtained from memory in the first network node claim_text 39 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating execution of the machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model said information further comprises information indicating that at least part of said machinelearning model should be executed in an enclaved mode claim_text 40 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating partial execution of the first machinelearning model and partial execution of the second machinelearning model said information further comprising information indicating that at least one component of said first machinelearning model or of said second machinelearning model should be executed in an enclaved mode claim_text 41 the method according to claim 35 wherein the first machinelearning model and the second machinelearning model are representable computational graphs claim_text 42 the method according to claim 41 wherein the computational graphs are directed acyclic graphs claim_text 43 the method according to claim 41 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 44 the method according to claim 35 wherein the step of executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result comprises at least partially executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model in an enclaved memory segment claim_text 45 the method according to claim 35 wherein the step of partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result comprises one or more of executing at least one component of the first machinelearning model in an enclaved memory segment causing the second network node to partially execute the second machinelearning model in an enclaved memory segment partially executing the first machinelearning model to form first partial result communicating to the second network node the first partial result causing the partial execution of the second machinelearning model using the first partial result at the second network node such that second partial result is formed at the second network node receiving from the second network node the second partial result or partially executing the first machinelearning model using the second partial result to form final result claim_text 46 first network node executing machinelearning model the first network node comprising an interface configured allowing communication with other network nodes and processing circuitry operatively associated with the interface and configured to develop first machinelearning model based on first set of data and using machinelearning algorithm communicate the first machinelearning model to second network node receive from the second network node information about difference between the first machinelearning model and second machinelearning model receive request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtain information indicative of an execution policy and depending on the obtained information indicative of an execution policy either execute machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially execute the first machinelearning model and cause the second network node to partially execute the second machinelearning model to obtain result claim_text 47 method executing machinelearning model performed in second network node the method comprising receiving from first network node first machinelearning model developing second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicating to the first network node information about difference between the first machinelearning model and the second machinelearning model receiving from the first network node partial result partially executing the second machinelearning model using the first partial result to form second partial result and communicating to the first network node the second partial result claim_text 48 the method according to claim 47 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises information that the second machinelearning model is different from the first machinelearning model andor information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model andor information identifying difference between the first machinelearning model and the second machinelearning model andor the second machinelearning model claim_text 49 the method according to claim 47 wherein the machinelearning model and the second machinelearning model are representable computational graphs claim_text 50 the method according to claim 49 wherein the computational graphs are directed acyclic graphs claim_text 51 the method according to claim 49 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 52 the method according to claim 47 wherein the step of partially executing the second machinelearning model using the first partial result to form second partial result comprises executing at least one component of the second machinelearning model at the second node in an enclaved memory segment claim_text 53 network node executing machinelearning model the network node referred to second network node and comprising an interface configured allowing communication with other network nodes processing circuitry operatively associated with the interface and configured to receive from first network node first machinelearning model develop second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicate to the first network node information about difference between the first machinelearning model and the second machinelearning model receive from the first network node partial result partially execute the second machinelearning model using the first partial result to form second partial result and communicate to the first network node the second partial result lang en",
    "label": 1
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "086-031-183-779-370",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 an information processing device optimizing information presented by an information presenter to an information recipient comprising an attribute determination unit that determines knowledge level and an understanding level of the information recipient with respect to the information based on biological activity information of the information recipient acquired by sensor presentation information feature determination unit that determines feature of the information presented by the information presenter to the information recipient and feedback optimization unit that converts presentation format of the information into presentation format according to the knowledge level and the understanding level of the information recipient based on the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the feature of the information determined by the presentation information feature determination unit claim_text 2 the information processing device according to claim 1 wherein the presentation information feature determination unit determines the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation in case where the information is auditory information claim_text 3 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the visual recognition level based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 4 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the voice speed level based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 5 the information processing device according to claim 1 wherein the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient by the feedback optimization unit are presented to the information presenter claim_text 6 an information processing method optimizing information presented by an information presenter to an information recipient using an information processing device comprising first step in which knowledge level and an understanding level of the information recipient with respect to the information are determined based on biological activity of the information recipient acquired by sensor and feature of the information presented by the information presenter to the information recipient is determined and second step in which presentation format of the information is converted into presentation format according to the knowledge level and the understanding level of the information recipient based on the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the feature of the information claim_text 7 the information processing method according to claim 6 wherein in the first step the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation are determined in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation are determined in case where the information is auditory information claim_text 8 the information processing method according to claim 6 wherein in the first step the visual recognition level is determined based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level is determined based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 9 the information processing method according to claim 6 wherein in the first step the voice speed level is determined based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level is determined based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 10 the information processing method according to claim 6 further comprising third step in which the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient are presented to the information presenter lang en",
    "label": 0
  },
  {
    "id1": "126-559-373-005-266",
    "id2": "038-794-758-325-674",
    "claims1": "claims claim_text 1 method of training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 2 the method of claim 1 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 3 the method of claim 2 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 4 the method of claim 3 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 5 the method of claim 1 wherein the subject node comprises feature node claim_text 6 the method of claim 5 further comprising inserting by the computer system the feature node into second neural network claim_text 7 method of training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system an activation value of subject node of the first neural network to an activation value of second node claim_text 8 the method of claim 7 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node and the second node common data training example claim_text 9 the method of claim 8 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 10 the method of claim 9 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 11 the method of claim 10 further comprising determining by learning coach system the relaxation strength parameter claim_text 12 the method of claim 11 wherein determining the relaxation strength parameter comprises changing by the learning coach the relaxation strength parameter during the training claim_text 13 the method of claim 8 wherein the second node is part of the first neural network claim_text 14 the method of claim 8 wherein the second node is part of second neural network that is different from the first neural network claim_text 15 the method of claim 8 wherein softtying comprises softtying by the first computer system the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 16 the method of claim 15 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 17 the method of claim 16 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 18 the method of claim 17 wherein the second network comprises classifier and the third network comprises feature detector claim_text 19 computer system training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 20 the computer system of claim 19 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 21 the computer system of claim 20 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 22 the computer system of claim 21 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 23 the computer system of claim 19 wherein the subject node comprises feature node claim_text 24 the computer system of claim 23 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to insert the feature node into second neural network claim_text 25 computer system training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying an activation value of subject node of the first neural network to an activation value of second node claim_text 26 the computer system of claim 25 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node and the second node common data training example claim_text 27 the computer system of claim 26 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 28 the computer system of claim 27 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 29 the computer system of claim 28 further comprising learning coach system determining the relaxation strength parameter claim_text 30 the computer system of claim 29 wherein the learning coach is changing the relaxation strength parameter during the training claim_text 31 the computer system of claim 26 wherein the second node is part of the first neural network claim_text 32 the computer system of claim 26 wherein the second node is part of second neural network that is different from the first neural network claim_text 33 the computer system of claim 26 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie by softtying the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 34 the computer system of claim 33 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 35 the computer system of claim 34 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 36 the computer system of claim 35 wherein the second network comprises classifier and the third network comprises feature detector lang en",
    "claims2": "claims claim_text 1 system training probabilistic predictive model recommending experiment designs synthetic biology comprising nontransitory memory configured to store executable instructions and hardware processor in communication with the nontransitory memory the hardware processor programmed by the executable instructions to receive synthetic biology experimental data generate training data from the synthetic biology experimental data wherein the training data comprise plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective train using the training data plurality of level0 learners of probabilistic predictive model recommending experiment designs synthetic biology wherein an input of each of the plurality of level0 learners comprises input values of the input variables and wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable and train using i predicted values of the at least one response variable determined using the plurality of level0 learners the training inputs of the plurality of training inputs and ii the reference outputs of the plurality of reference outputs correspondence to the training inputs of the plurality of training inputs level1 learner of the probabilistic predictive model recommending experiment designs synthetic biology comprising probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable claim_text 2 10 canceled claim_text 11 the system of claim 1 wherein the synthetic biology experimental data is sparse claim_text 12 the system of claim 1 wherein number of the plurality of training inputs in the synthetic biology experiment data is number of experimental conditions number of strains number of replicates of strain of the strains or combination thereof claim_text 13 15 canceled claim_text 16 the system of claim 1 wherein one or each of the plurality of input variables andor the at least one response variable comprises promoter sequence an induction time an induction strength ribosome binding sequence copy number of gene transcription level of gene an epigenetics state of gene level of protein post translation modification state of protein level of molecule an identity of molecule level of microbe state of microbe state of microbiome titer rate yield or combination thereof optionally wherein the molecule comprises an inorganic molecule an organic molecule protein polypeptide carbohydrate sugar fatty acid lipid an alcohol fuel metabolite drug an anticancer drug biofuel flavoring molecule fertilizer molecule or combination thereof claim_text 17 21 canceled claim_text 22 the system of claim 1 wherein the predetermined response variable objective comprises maximization objective minimization objective or specification objective andor wherein the predetermined response variable objective comprises maximizing the at least one response variable minimizing the at least one response variable or adjusting the at least one response variable to predetermined value of the at least one response variable claim_text 23 the system of claim 1 wherein to train the plurality of level1 learner the hardware processor is programmed by the executable instructions to determine using the plurality of level0 learners the predicted values of the at least one response variable training inputs of the plurality of training inputs claim_text 24 the system of claim 1 wherein the level1 learner comprises bayesian ensemble of the plurality of level0 learners claim_text 25 the system of claim 1 wherein parameters of the ensemble of the plurality of level0 learners comprises i plurality of ensemble weights and ii an error variable distribution of the ensemble or standard deviation of the error variable distribution of the ensemble claim_text 26 32 canceled claim_text 33 the system of claim 1 wherein to train the level1 learner the hardware processor is programmed by the executable instructions to determine posterior distribution of the ensemble parameters given the training data or the second subset of the training data wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to determine i probability distribution of the training data or the second subset of the training data given the ensemble parameters or likelihood function of the ensemble parameters given the training data of the second subset of the training data and ii prior distribution of the ensemble parameters and wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to sample space of the ensemble parameters with frequency proportional to desired posterior distribution claim_text 34 canceled claim_text 35 canceled claim_text 36 the system of claim 1 wherein to train the plurality of level0 learners the hardware processor is programmed by the executable instructions to generate first subset of the training data and train using the first subset of the training data the plurality of level0 learners claim_text 37 50 canceled claim_text 51 the system of claim 1 wherein the hardware processor is programmed by the executable instructions to determine surrogate function with an input experiment design an input the surrogate function comprising an expected value of the at least one response variable determined using the input experiment design variance of the value of the at least one response variable determined using the input experiment design and an exploitationexploration tradeoff parameter and determine using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of synthetic biology experiment obtaining predetermined response variable objective associated with the at least one response variable claim_text 52 57 canceled claim_text 58 the system of claim 51 wherein to determine the plurality of recommended experiment designs the hardware processor is programmed by the executable instructions to determine plurality of possible recommended experiment designs each comprising possible recommended values of the input variables with surrogate function values determined using the surrogate function with predetermined characteristic and select the plurality of recommended experiment designs from the plurality of possible recommended experiment designs using an input variable difference factor based on the surrogate function values of the plurality of possible recommended experiment designs claim_text 59 64 canceled claim_text 65 the system of claim 51 wherein number of the plurality of recommended experiment designs is number of experimental conditions or number of strains the next cycle of the synthetic biology experiment claim_text 66 canceled claim_text 67 canceled claim_text 68 the system of claim 51 wherein to determine the plurality of possible recommended experiment designs the hardware processor is programmed by the executable instructions to sample space of the input variables with frequency proportional to the surrogate function or an exponential function of the surrogate function and prior distribution of the input variables claim_text 69 canceled claim_text 70 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine an upper bound andor lower bound one or each of the plurality of input variables based on training values of the corresponding input variable wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 71 canceled claim_text 72 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to receive an upper bound andor lower bound one or each of the plurality of input variables wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 73 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability distribution of the at least one response variable one or each of the plurality of recommended experiment designs claim_text 74 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of one or each of the plurality of recommended experiment designs being predetermined percentage closer to achieving the objective relative to the training data claim_text 75 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective being predetermined percentage closer to achieving the objective relative to the training data claim_text 76 canceled claim_text 77 canceled claim_text 78 method recommending experiment designs synthetic biology comprising under control of hardware processor receiving probabilistic predictive model recommending experiment designs synthetic biology comprising plurality of level0 learners and level1 learner wherein an input of each of the plurality of level0 learners comprises input values of the input variables wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable wherein the level1 learner comprises probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable wherein the plurality of level0 learners and the level1 learner are trained using training data obtained from one or more cycles of synthetic biology experiment comprising plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective determining surrogate function comprising an expected value of the level1 learner variance of the level1 learner and an exploitationexploration tradeoff parameter and determining using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of the synthetic biology experiment achieving predetermined response variable objective associated with the at least one response variable claim_text 79 85 canceled lang en",
    "label": 0
  },
  {
    "id1": "062-542-216-489-868",
    "id2": "035-579-537-740-857",
    "claims1": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "020-796-323-879-358",
    "id2": "030-076-623-736-233",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyzing the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposing and displaying on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui according to one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyze the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decompose and display on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui according to one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof an executable portion that analyzes the extracted metadata wherein the extracted metadata includes provenance metadata and an executable portion that in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposes and displays on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui according to one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "label": 1
  },
  {
    "id1": "020-796-323-879-358",
    "id2": "023-224-291-896-852",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyzing the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposing and displaying on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui according to one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyze the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decompose and display on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui according to one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof an executable portion that analyzes the extracted metadata wherein the extracted metadata includes provenance metadata and an executable portion that in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposes and displays on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui according to one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text computerimplemented method of training machine learning model having plurality of parametersn wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and n wherein the method comprisesn determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task n obtaining training data training the machine learning model on second different machine learning task and n training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task n wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task n wherein the machine learning model comprises deep neural network model that includes an input layer and one or more hidden layers that each apply nonlinear transformation to received input to generate an output wherein the machine learning model generates the output based on the received input and on the values of the parameters of the model and n wherein the first and second machine learning tasks each comprise reinforcement learning task in which robotic agent interacts with realworld environment to perform the task claim_text the method of claim 1 wherein the robotic agent comprises static or moving machine or vehicle claim_text the method of any one of claims 12 wherein training the machine learning model on the training data comprises nadjusting the first values of the parameters to optimize an objective function that includesn i first term that measures performance of the machine learning model on the second machine learning task and n ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text the method of claim 1 2 or 3 training the machine learning model on the training data includes each training example in the training datan processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output n determining gradient of an objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and n adjusting the current values of the parameters using the gradient to optimize the objective function and n wherein the objective function includes discounted reward term dependent upon an expected reward from the robotic agent taking an action in state claim_text the method of claim 3 or 4 wherein training the machine learning model on the training data comprises each training example in the training datan processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output n determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and n adjusting the current values of the parameters using the gradient to optimize the objective function claim_text the method of any one of claims 35 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text the method of any one of claims 16 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises ndetermining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text the method of any one of claims 17 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises ndetermining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text the method of any one of claims 18 further comprising nafter training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning modeln obtaining third training data training the machine learning model on third different machine learning task and n training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task n wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text the method of claim 9 further comprisingn determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and n wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includesn i first term that measures performance of the machine learning model on the third machine learning task and n ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task n iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text the method of any one of claims 38 when dependent upon claim 3 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response in particular wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any one of claims 113 claim_text computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 113 lang en",
    "label": 1
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "051-909-779-474-634",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "030-076-623-736-233",
    "id2": "051-909-779-474-634",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "030-076-623-736-233",
    "id2": "036-552-384-938-884",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "claims2": "claims claim_text we claim 1 method 400 determining positioning method user equipment ue in network the method comprising loading by loading unit 302 at location server 300 plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods receiving by transceiver unit 304 at the location server 300 location request associated with service from the ue wherein the location request relates to determining location of the ue selecting by an execution unit 306 at the location server 300 first positioning method from the plurality of positioning methods based on the location request selecting by the execution unit 306 at the location server 300 fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeating by the execution unit 306 one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with nonsupported status selecting by the execution unit 306 at the location server 300 cell identifier cellid based positioning method in an event each of the selected fallback positioning methods from the set of fallback positioning methods is associated with nonsupported status 2 the method 400 claimed in claim 1 wherein each of the plurality of positioning methods is based on one of service identifier service id associated with the service and quality of service qos information associated with the location request 3 the method 400 claimed in claim 2 wherein post the receiving of the location request associated with the service from the ue the method comprises checking by processing unit 308 at the location server 300 an availability of at least one of the service id and the qos information in the received location request 4 the method 400 claimed in claim 2 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 5 the method 400 claimed in claim 1 wherein the location server 300 is location management function lmf 6 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 the first positioning method from the plurality of positioning methods the method comprises determining by the execution unit 306 one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 7 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 at the location server 300 the fallback positioning method from the set of fallback positioning methods the method comprises determining one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 8 the method 400 claimed in claim 6 wherein the cellid identifies base station associated with the ue 9 system 300 determining positioning method user equipment ue in network the system comprising location server 300 wherein the location server 300 comprises loading unit 302 configured to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 connected to at least the loading unit 302 the transceiver unit 304 configured to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 connected to at least the transceiver unit 304 the execution unit 306 configured to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method 10 the system 300 claimed in claim 9 wherein each of the plurality of positioning methods is based on at least one of service identifier service id associated with the service and quality of service qos information associated with the location request 11 the system 300 claimed in claim 10 wherein post receiving the location request associated with the service from the ue the system comprises processing unit 308 configured to check an availability of at least one of the service id and the qos information in the received location request 12 the system 300 claimed in claim 10 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 13 the system 300 claimed in claim 9 wherein the location server 300 is location management function lmf 14 the system 300 claimed in claim 9 wherein prior to the selecting the first positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 15 the system 300 claimed in claim 9 wherein prior to the selecting the fallback positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 16 the system 300 claimed in claim 9 wherein the cellid identifies base station associated with the ue 17 nontransitory computerreadable storage medium storing instructions determining positioning method of user equipment ue in network the storage medium comprising executable code which when executed by one or more units of system comprising location server 300 causes loading unit 302 to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status and select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method lang en",
    "label": 0
  },
  {
    "id1": "110-881-173-371-629",
    "id2": "127-177-660-785-528",
    "claims1": "claims claim_text 1 method comprising storing at first machine learning server computer one or more machine learning configuration files first machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters storing at the first machine learning server computer plurality of machine learning training datasets displaying at the first machine learning server computer through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the first machine learning server computer particular input dataset receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable training options corresponding to particular machine learning training dataset the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options comprising list of one or more second selectable parameter and training options each of the one or more second selectable parameter and training options being displayed in the graphical user interface via radio buttons or checkboxes the selection of the one or more second selectable parameter and training options comprising selection of corresponding graphical user interface radio button or checkbox replacing in the first machine learning configuration file at the first machine learning server computer the one or more first machine learning parameters with the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options using at the first machine learning server computer the first machine learning configuration file configuring particular machine learning system and in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset using at the first machine learning server computer the particular machine learning system and the particular input dataset computing particular output dataset the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing at the first machine learning server computer confidence score threshold value determining that subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving at the first machine learning server computer through the graphical user interface second selection of the one or more second selectable parameter and training options corresponding to second machine learning training dataset of the plurality of machine learning training datasets from second machine learning server computer updating at the first machine learning server computer the first machine learning configuration file with the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset sending to separate server computer the first machine learning configuration file with instructions to configure second machine learning system with the second selection of the one or more second selectable parameter and training options using at the separate server computer the first machine learning configuration file configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing by the first machine learning server computer one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 2 the method of claim 1 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the separate server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the second input dataset computing the second output dataset claim_text 3 the method of claim 1 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the one or more selectable training options claim_text 4 the method of claim 1 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value displaying through the graphical user interface subset of plurality of selectable training options wherein each of the subset of the plurality of selectable training options corresponds to the particular data category receiving the first selection of the one or more selectable training options from the subset of the plurality of selectable training options claim_text 5 the method of claim 1 further comprising storing at the first machine learning server computer the confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets third machine learning training dataset using the first machine learning configuration file configuring third machine learning system training the third machine learning system using the third machine learning training dataset of the plurality of machine learning training datasets using the third machine learning system and the subset of the particular input dataset computing third output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the third output dataset claim_text 6 the method of claim 1 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the particular input dataset computing the second output dataset claim_text 7 the method of claim 1 further comprising storing at the second machine learning server computer second plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the second plurality of machine learning training datasets receiving through the graphical user interface selection of both the first selection of one or more selectable training options corresponding to the particular machine learning training dataset and the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset using second machine learning configuration file configuring the second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing the second output dataset claim_text 8 the method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems has higher confidence score than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system has higher confidence score than each other test machine learning system storing second machine learning configuration file with the one or more first machine learning parameters claim_text 9 the method of claim 1 further comprising displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of the first selection of one or more selectable training options corresponding to the particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 10 computer system comprising one or more processors first machine learning server computer memory storing instructions which when executed by the one or more processors cause performance of storing one or more machine learning configuration files at the first machine learning server computer first machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters storing at the first machine learning server computer plurality of machine learning training datasets displaying at the first machine learning server computer through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the first machine learning server computer particular input dataset receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable training options corresponding to particular machine learning training dataset the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options comprising list of one or more second selectable parameter and training options each of the one or more second selectable parameter and training options being displayed in the graphical user interface via radio buttons or checkboxes the selection of the one or more second selectable parameter and training options comprising selection of corresponding graphical user interface radio button or checkbox replacing in the first machine learning configuration file at the first machine learning server computer the one or more first machine learning parameters with the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options using at the first machine learning server computer the first machine learning configuration file configuring particular machine learning system and in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset using at the first machine learning server computer the particular machine learning system and the particular input dataset computing particular output dataset the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing at the first machine learning server computer confidence score threshold value determining that subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving at the first machine learning server computer through the graphical user interface second selection of the one or more second selectable parameter and training options corresponding to second machine learning training dataset of the plurality of machine learning training datasets from second machine learning server computer updating at the first machine learning server computer the first machine learning configuration file with the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset sending to separate server computer the first machine learning configuration file with instructions to configure second machine learning system with the second selection of the one or more second selectable parameter and training options using at the separate server computer the first machine learning configuration file configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing by the first machine learning server computer one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 11 the computer system of claim 10 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the second input dataset computing the second output dataset claim_text 12 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the one or more selectable training options claim_text 13 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value displaying through the graphical user interface subset of plurality of selectable training options wherein each of the subset of the plurality of selectable training options corresponds to the particular data category receiving the first selection of the one or more selectable training options from the subset of the plurality of selectable training options claim_text 14 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing the confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets third machine learning training dataset using the first machine learning configuration file configuring third machine learning system training the third machine learning system using the third machine learning training dataset of the plurality of machine learning training datasets using the third machine learning system and the subset of the particular input dataset computing third output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the third output dataset claim_text 15 the computer system of claim 10 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the particular input dataset computing the second output dataset claim_text 16 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing second plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the second plurality of machine learning training datasets receiving through the graphical user interface selection of both the first selection of one or more selectable training options corresponding to the particular machine learning training dataset and the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset using second machine learning configuration file configuring the second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing the second output dataset claim_text 17 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems has higher confidence score than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system has higher confidence score than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 18 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of the first selection of one or more selectable training options corresponding to the particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "claims2": "claims claim_text claims what is claimed is 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning mode requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 8 the method of claim 1 wherein generating the updated machine learning mode comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 1 1 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "label": 1
  },
  {
    "id1": "126-559-373-005-266",
    "id2": "091-570-367-989-78X",
    "claims1": "claims claim_text 1 method of training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 2 the method of claim 1 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 3 the method of claim 2 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 4 the method of claim 3 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 5 the method of claim 1 wherein the subject node comprises feature node claim_text 6 the method of claim 5 further comprising inserting by the computer system the feature node into second neural network claim_text 7 method of training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system an activation value of subject node of the first neural network to an activation value of second node claim_text 8 the method of claim 7 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node and the second node common data training example claim_text 9 the method of claim 8 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 10 the method of claim 9 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 11 the method of claim 10 further comprising determining by learning coach system the relaxation strength parameter claim_text 12 the method of claim 11 wherein determining the relaxation strength parameter comprises changing by the learning coach the relaxation strength parameter during the training claim_text 13 the method of claim 8 wherein the second node is part of the first neural network claim_text 14 the method of claim 8 wherein the second node is part of second neural network that is different from the first neural network claim_text 15 the method of claim 8 wherein softtying comprises softtying by the first computer system the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 16 the method of claim 15 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 17 the method of claim 16 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 18 the method of claim 17 wherein the second network comprises classifier and the third network comprises feature detector claim_text 19 computer system training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 20 the computer system of claim 19 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 21 the computer system of claim 20 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 22 the computer system of claim 21 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 23 the computer system of claim 19 wherein the subject node comprises feature node claim_text 24 the computer system of claim 23 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to insert the feature node into second neural network claim_text 25 computer system training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying an activation value of subject node of the first neural network to an activation value of second node claim_text 26 the computer system of claim 25 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node and the second node common data training example claim_text 27 the computer system of claim 26 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 28 the computer system of claim 27 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 29 the computer system of claim 28 further comprising learning coach system determining the relaxation strength parameter claim_text 30 the computer system of claim 29 wherein the learning coach is changing the relaxation strength parameter during the training claim_text 31 the computer system of claim 26 wherein the second node is part of the first neural network claim_text 32 the computer system of claim 26 wherein the second node is part of second neural network that is different from the first neural network claim_text 33 the computer system of claim 26 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie by softtying the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 34 the computer system of claim 33 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 35 the computer system of claim 34 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 36 the computer system of claim 35 wherein the second network comprises classifier and the third network comprises feature detector lang en",
    "claims2": "claims claim_text 1 method of aiding in diagnosing whether subject has depression comprising analyzing blood sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid alphahydroxyisobutyric acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the biological sample is blood plasma claim_text 8 the method of claim 1 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 9 the method of claim 8 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 10 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and the first sample is obtained from the subject at first time point and wherein the analysis method the blood sample is mass spectrometry analyzing second biological sample that was removed from subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 11 the method of claim 10 wherein the method further comprises comparing the levels of the biomarkers in the first sample the levels of the biomarkers in the second sample andor the results of the comparison of the levels of the biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the biomarkers claim_text 12 the method of claim 10 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 13 the method of claim 12 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 14 method of assessing the efficacy of composition treating depression comprising analyzing blood sample removed from subject having depression and currently or previously being treated with composition to determine the levels of biomarkers depression wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to levels of the biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the biomarkers c depressionnegative reference levels of the biomarkers d depressionprogressionpositive reference levels of the biomarkers andor depressionregressionpositive reference levels of the biomarkers claim_text 15 the method of claim 14 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 16 the method of claim 15 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 17 method assessing the efficacy of composition in treating depression comprising analyzing first blood sample obtained from subject to determine the levels of biomarkers depression the first sample obtained from the subject at first time point wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry administering the composition to the subject analyzing second biological sample obtained from the subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point after administration of the composition and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 18 the method of claim 17 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids lang en",
    "label": 0
  },
  {
    "id1": "020-698-584-160-318",
    "id2": "033-247-762-034-912",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix claim_text 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold claim_text 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold claim_text 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states claim_text 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals claim_text 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound claim_text 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs claim_text 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states claim_text 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals claim_text 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals claim_text 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result claim_text 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result claim_text 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result claim_text 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result claim_text 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module claim_text 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "label": 1
  },
  {
    "id1": "119-274-578-817-759",
    "id2": "156-217-120-172-056",
    "claims1": "claims claim_text claims 1 method comprising receiving from producer node producer node capa bilities comprising machine learning based assistance capa bility the machine learning based assistance capability prising machine learning based functionalities each machine learning based functionality comprising machine learning en tity and at least one machine learning mode associated with the machine learning entity 2 the method according to claim 1 wherein the at least one machine learning mode associated with the machine learning entity comprises at least one fallback operating mode 3 the method according to claim 1 or 2 further prising selecting at least one machine learning based function ality among the machine learning based functionalities and causing transmission of an activation request to acti vate the selected at least one machine learning based function ality to the producer node 4 the method according to claim 3 further comprising receiving from the producer node an activation re quest response the activation request response comprising an indication whether the at least one machine learning based func tionality is activated 5 the method according to claim 1 or 2 further prising receiving from the producer node an activation re quest to activate at least one machine learning based function ality among the machine learning based functionalities 6 the method according to claim 5 further comprising causing transmission of machine learning based func tionality activation response to the producer node 7 the method according to any of claims 1 6 further comprising receiving from the producer node change request to change at least one machine learning based functionality among the machine learning based functionalities 8 the method according to claim 7 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learn ing based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality 9 the method according to claim 8 further comprising when the change request comprises the deactivation in dication maintaining state of deactivated inference reports associated with the deactivated machine learning based function ality predetermined time period 10 the method according to claim 8 further compris ing causing transmission of request to enable deactivated inference reports associated with the deactivated machine learn ing to the producer node 11 the method according to any of claims 7 10 fur ther comprising causing transmission of response indicating confir mation of the changed machine learning based functionality to the producer node 12 the method according to any of claims 1 6 further comprising causing transmission of change request to change at least one machine learning based functionality among the machine learning based functionalities to the producer node 13 the method according to claim 12 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learn ing based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality 14 the method according to claim 13 further compris ing causing transmission of an inference reporting recon figuration message to the producer node when the change request comprises the deactivation indication or the switch indication 15 the method according to any of claims 13 14 further comprising when the change request comprises the deactivation in dication maintaining state of deactivated inference reports associated with the deactivated machine learning based function ality predetermined time period 16 the method according to any of claims 13 14 further comprising causing transmission of request to enable deactivated inference reports associated with the deactivated machine learn ing to the producer node 17 the method according to any of claims 12 16 further comprising receiving from the producer node reply to the change request 18 method comprising identifying producer node capabilities comprising ma chine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity and causing transmission of the producer node capabilities to consumer node 19 the method according to claim 18 wherein the at least one machine learning mode associated with the machine learning entity comprises at least one fallback operating mode 20 the method according to claim 18 or 19 further comprising receiving from the consumer node an activation re quest to activate at least one machine learning based function ality among the machine learning based functionalities 21 the method according to claim 20 further compris rng causing transmission of an activation request response to the consumer node the activation request response comprising an indication whether the at least one machine learning based functionality is activated 22 the method according to claim 18 or 19 further comprising causing transmission of an activation request to acti vate at least one machine learning based functionality among the machine learning based functionalities to the consumer node 23 the method according to claim 22 further compris ing receiving machine learning based functionality acti vation response from the consumer node 24 the method according to any of claims 18 23 further comprising causing transmission of change request to change at least one machine learning based functionality among the machine learning based functionalities to the consumer node 25 the method according to claim 24 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learn ing based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality 26 the method according to claim 25 further compris rng when the change request comprises the deactivation in dication maintaining state of deactivated inference reports associated with the deactivated machine learning based function ality predetermined time period 27 the method according to claim 25 further compris ing receiving from the consumer node request to enable deactivated inference reports associated with the deactivated machine learning 28 the method according to any of claims 24 27 further comprising receiving from the consumer node response indicat ing confirmation of the changed machine learning based function ality 29 the method according to any of claims 18 23 further comprising receiving from the consumer node change request to change at least one machine learning based functionality among the machine learning based functionalities to the consumer node and changing the at least one machine learning based func tionality based on the change request 30 the method according to claim 29 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learn ing based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality 31 the method according to claim 30 further compris ing receiving from the consumer node an inference report ing reconfiguration message when the change request comprises the deactivation indication or the switch indication 32 the method according to any of claims 30 31 further comprising when the change request comprises the deactivation in dication maintaining state of deactivated inference reports associated with the deactivated machine learning based function ality predetermined time period 33 the method according to any of claims 30 31 further comprising receiving from the consumer node request to enable deactivated inference reports associated with the deactivated machine learning 34 the method according to any of claims 29 33 further comprising causing transmission of reply to the change request to the consumer node 35 consumer node 100 comprising at least one processor 102 and at least one memory 104 including computer program code the at least one memory 104 and the computer program code configured to with the at least one processor 102 cause the consumer node 100 to at least perform receive from producer node producer node capabili ties comprising machine learning based assistance capability the machine learning based assistance capability comprising ma chine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity 36 consumer node 100 according to claim 35 wherein the at least one memory 104 and the computer program code configured to with the at least one processor 102 cause the consumer node 100 to at least perform the method of any of claims 2 17 37 producer node 200 comprising at least one processor 202 and at least one memory 204 including computer program code the at least one memory 204 and the computer program code configured to with the at least one processor 202 cause the producer node 200 to at least perform identify producer node capabilities comprising ma chine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality prising machine learning entity and at least one machine learn ing mode associated with the machine learning entity and cause transmission of the producer node capabilities to consumer node 38 producer node 200 according to claim 37 wherein the at least one memory 104 and the computer program code configured to with the at least one processor 102 cause the consumer node 100 to at least perform the method of any of claims 19 34 39 consumer node 100 comprising means performing receiving from producer node producer node capa bilities comprising machine learning based assistance capa bility the machine learning based assistance capability prising machine learning based functionalities each machine learning based functionality comprising machine learning en tity and at least one machine learning mode associated with the machine learning entity 40 consumer node 100 according to claim 39 wherein the means are configured to perform the method of any of claims 2 17 41 consumer node 100 according to any of claims 39 40 wherein the means comprises at least one processor and at least one memory including computer program code the at least one memory and computer program code configured to with the at least one processor cause the performance of the consumer node 100 42 producer node 200 comprising means perform rng identifying producer node capabilities comprising ma chine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality prising machine learning entity and at least one machine learn ing mode associated with the machine learning entity and causing transmission of the producer node capabilities to consumer node 43 producer node 200 according to claim 41 wherein the means are configured to perform the method of any of claims 19 34 44 producer node 200 according to any of claims 42 43 wherein the means comprises at least one processor and at least one memory including computer program code the at least one memory and computer program code configured to with the at least one processor cause the performance of the producer node 200 45 computer program comprising instructions causing an apparatus to perform the method of any of claims 1 17 46 computer program comprising instructions causing an apparatus to perform the method of any of claims 18 34 47 computerreadable medium comprising computer program comprising instructions causing an apparatus to per form the method of any of claims 1 17 48 computerreadable medium comprising computer program comprising instructions causing an apparatus to per form the method of any of claims 18 34 lang en",
    "claims2": "claims claim_text 1 method comprising storing at machine learning server computer one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the machine learning server computer particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 2 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 3 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 4 the method of claim 3 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 5 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 6 the method of claim 5 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 7 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 8 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 9 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 10 the method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 11 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 12 computer system comprising one or more processors memory storing instructions which when executed by the one or more processors cause performance of storing one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 13 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 14 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 15 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 16 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 17 the computer system of claim 16 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 18 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 19 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 20 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 21 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 22 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "label": 1
  },
  {
    "id1": "026-671-265-300-529",
    "id2": "116-391-006-009-952",
    "claims1": "claims claim_text 1 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 3 the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 6 the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 8 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 9 the system of claim 8 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 10 the system of claim 9 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 13 the system of claim 9 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 14 the system of claim 8 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 15 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 16 the nontransitory computer storage medium of claim 15 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 17 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 20 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text claims 1 method of training machine leaming model having plurality of parameters wherein the machine learning model has been trained on first machine leaming task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine leaming task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine leaming task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine leaming model achieving acceptable performance on the first machine learning task 2 the method of claim 1 wherein the first machine leaming task and the second machine leaming task are different supervised learning tasks 3 the method of claim 1 wherein the first machine leaming task and the second machine leaming tasks are different reinforcement learning tasks 4 the method of any one of claims 1 3 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine leaming model in accordance with current values of parameters of the machine leaming model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the obj ective function 6 the method of any one of claims 4 or 5 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter 7 the method of any one of claims 1 6 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task 8 the method of any one of claims 1 7 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining fisher information matrix fim of the plurality of parameters of the machine leaming model with respect to the first machine leaming task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim 9 the method of any one of claims 1 8 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter 13 the method of any one of claims 48 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models 15 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 16 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 lang en",
    "label": 1
  },
  {
    "id1": "156-217-120-172-056",
    "id2": "034-722-477-434-593",
    "claims1": "claims claim_text 1 method comprising storing at machine learning server computer one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the machine learning server computer particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 2 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 3 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 4 the method of claim 3 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 5 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 6 the method of claim 5 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 7 the method of claim 3 further comprising storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 8 the method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 9 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 10 the method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 11 the method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 12 computer system comprising one or more processors memory storing instructions which when executed by the one or more processors cause performance of storing one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving particular input dataset receiving through the graphical user interface selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset claim_text 13 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the second input dataset computing second output dataset claim_text 14 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset claim_text 15 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets correspond to the particular data category and in response display the plurality of selectable training options claim_text 16 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 17 the computer system of claim 16 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subset of the plurality of selectable training options correspond to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 18 the computer system of claim 14 wherein the instructions when executed by the one or more processors further cause performance of storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 19 the computer system of claim 12 wherein second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more first third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system using the second machine learning system and the particular input dataset computing second output dataset claim_text 20 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 21 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 22 the computer system of claim 12 wherein the instructions when executed by the one or more processors further cause performance of storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "claims2": "claims claim_text 1 computer system configuring plurality of machine learning pipelines into machine learning pipeline ensemble the system comprising one or more computer processors one or more computer readable storage media computer program instructions the computer program instructions being stored on the one or more computer readable storage media execution by the one or more computer processors and the computer program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 2 the computer system of claim 1 wherein the plurality of machine learning pipelines are heterogeneous claim_text 3 the computer system of claim 1 wherein the instructions the reinforcement learning agent coupled to the machine learning pipeline to adjust the configuration values of the coupled machine learning pipeline further include instructions to receive performance information associated with the uncoupled machine learning pipeline determine an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjust the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 4 the computer system of claim 3 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 5 the computer system of claim 4 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 6 the computer system of claim 1 further comprising instructions to determine similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merge the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 7 the computer system of claim 3 further comprising instructions to generate new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 8 the computer system of claim 1 wherein the configuration parameter values of the coupled and uncoupled machine learning pipelines include at least one value selected from the group consisting of training dataset learning environment machine learning pipeline structure an objective function and hyperparameter set claim_text 9 the computer system of claim 1 wherein performance information of the uncoupled machine learning pipeline includes at least one performance metric selected from the group consisting of prediction accuracy value prediction accuracy value drift diversity of predictions running time and an entropy value claim_text 10 the computer system of claim 1 further comprising program instructions to generate machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 11 the computer system of claim 1 further comprising instructions to generate the plurality of machine learning pipelines from plurality of input datasets and couple each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent claim_text 12 computer program product configuring plurality of machine learning pipelines into machine learning pipeline ensemble the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media the program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 13 computerimplemented method configuring plurality of machine learning pipelines into machine learning pipeline ensemble the method comprising determining by reinforcement learning agent coupled to machine learning pipeline performance information of the machine learning pipeline receiving by the reinforcement learning agent configuration parameter values of uncoupled machine learning pipelines of the plurality of machine learning pipelines and adjusting by the reinforcement learning agent configuration parameter values of the machine learning pipeline based on the performance information of the machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipelines claim_text 14 the computerimplemented method of claim 13 further comprising receiving performance information associated with the uncoupled machine learning pipeline determining an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjusting the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 15 the computerimplemented method of claim 14 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 16 the computerimplemented method of claim 15 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 17 the computerimplemented method of claim 13 further comprising determining similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merging the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 18 the computerimplemented method of claim 14 further comprising generating new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 19 the computerimplemented method of claim 13 further comprising generating machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 20 the computerimplemented method of claim 13 further comprising generating the plurality of machine learning pipelines from plurality of input datasets and coupling each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent lang en",
    "label": 1
  },
  {
    "id1": "140-416-112-830-70X",
    "id2": "033-247-762-034-912",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train an ensemble with set of training data items wherein the ensemble comprises multiple machine learning ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to train the ensemble in first iteration of the training by selectively directing with preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 2 the machine learning computer system of claim 1 wherein the preliminary classifier is not trained with the ensemble claim_text 3 the machine learning computer system of claim 2 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 4 the machine learning computer system of claim 2 wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to further train the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 5 the machine learning computer system of claim 1 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 6 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 7 the machine learning computer system of claim 6 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 8 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 9 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores machine learning ml system with set of training data items wherein the ml system comprises an ensemble that comprises multiple ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and preliminary classifier that is trained through machine learning and iteratively training the ml system comprises in first iteration of the training selectively directing by the preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members and training the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 10 the computerimplemented machine learning method of claim 9 wherein the preliminary classifier is not trained with the ensemble claim_text 11 the computerimplemented machine learning method of claim 10 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 12 the computerimplemented machine learning method of claim 10 further comprising training by the one or more programmed processor cores the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 13 the computerimplemented machine learning method of claim 9 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 14 the computerimplemented machine learning method of claim 9 wherein training the preliminary classifier comprises training the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 15 the computerimplemented machine learning method of claim 9 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 16 the computerimplemented machine learning method of claim 14 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item lang en",
    "claims2": "claims claim_text 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix claim_text 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold claim_text 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold claim_text 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states claim_text 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals claim_text 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound claim_text 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs claim_text 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states claim_text 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals claim_text 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals claim_text 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result claim_text 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result claim_text 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result claim_text 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result claim_text 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module claim_text 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "label": 1
  },
  {
    "id1": "152-848-867-961-18X",
    "id2": "189-406-053-095-096",
    "claims1": "claims claim_text claims we claim 1 method comprising training via at least one processor of computer system using public data inputs public data machine learning model training via the at least one processor using private data inputs private data machine learning model training via the at least one processor public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing via the at least one processor the public data machine learning model using current public data input resulting in public data machine learning prediction executing via the at least one processor the private data machine learning model using current pnvate data input resulting in private data machine learning prediction and executing via the at least one processor the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 2 the method of claim 1 wherein the public data and the private data are associated with freight transport costs 3 the method of claim 2 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 4 the method of claim 2 wherein the private data comprises overhead costs and earner costs 5 the method of claim 1 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 6 the method of claim 5 further comprising adding via the at least one processor the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding via the at least one processor the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 7 the method of claim 1 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 8 system comprising at least one processor and nontransitory computerreadable storage medium having instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 9 the system of claim 8 wherein the public data and the private data are associated with freight transport costs 10 the system of claim 9 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 11 the system of claim 9 wherein the private data comprises overhead costs and earner costs 12 the system of claim 8 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 13 the system of claim 12 the nontransitory computerreadable storage medium having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 14 the system of claim 8 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 15 nontransitory computerreadable storage medium having instructions stored which when executed by at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 16 the nontransitory computerreadable storage medium of claim 15 wherein the public data and the private data are associated with freight transport costs 17 the nontransitory computerreadable storage medium of claim 16 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 18 the nontransitory computerreadable storage medium of claim 16 wherein the private data comprises overhead costs and carrier costs 19 the nontransitory computerreadable storage medium of claim 15 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 20 the nontransitory computerreadable storage medium of claim 19 having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time of the at least one compound in the chromatography bed such that equation i is satisfied equation i t res t diff equation i calculating concentration czt of the at least one compound in the mobile phase at predetermined location of the chromatography device and at predetermined time t based on the adsorption isotherm claim_text 2 the method according to claim 1 wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 3 the method according to claim 1 wherein the minimum residence time of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 4 the method according to claim 1 wherein in the calculation step czt is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 5 the method according to claim 4 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 6 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 7 the method according claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 8 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 9 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 10 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 11 the method according to claim 10 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 12 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 13 the chromatography method according to claim 12 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising iii carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 15 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 11 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "062-542-216-489-868",
    "id2": "013-300-859-082-341",
    "claims1": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "claims2": "claims claim_text 1 network slice configuration method comprising sending by first network element network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 2 the network slice configuration method of claim 1 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 3 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 4 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is sent by the first network element to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 5 the network slice configuration method of claim 1 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 6 network slice configuration method comprising receiving by second network element network slice configuration information of first network element and storing by the second network element network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 7 the network slice configuration method of claim 6 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 8 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 9 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is received by the second network element from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 10 the network slice configuration method of claim 6 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 11 first network element comprising sending module which is configured to send network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 12 the first network element of claim 11 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 13 the first network element of claim 11 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 14 the first network element of claim 11 wherein the sending module is further configured to send information of network slice supported by ta to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 15 the first network element of claim 11 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 16 second network element comprising receiving module which is configured to receive slice configuration information of first network element and storing module which is configured to storing network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 17 the second network element of claim 16 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 18 the second network element of claim 16 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 19 the second network element of claim 16 wherein the receiving module is further configured to receive information of network slice supported by ta from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 20 the second network element of claim 16 wherein the network slice information comprises single network slice selection assistance information snssai lang en",
    "label": 0
  },
  {
    "id1": "030-076-623-736-233",
    "id2": "057-360-526-753-072",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "claims2": "claims claim_text 1 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer setting detected number of active connections in counters provided each output port and controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections wherein said controlling step further includes outputting cell addressed to an output port corresponding to counter with said count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to said output port and subtracting value of said counter corresponding to an output port that has output cells at every output of cell claim_text 2 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of cell addressed to said output port from an input multiplex buffer claim_text 3 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of setting detected number of active connections in counters provided each output port and sequentially outputting cell addressed to an output port corresponding to counter with count value not set to 0 among said counters and subtracting value of counter corresponding to an output port that has output cell at every output of said cell when outputting cells accumulated in an output separation buffer to an output port claim_text 4 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output from said output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of the cells addressed to said output from an input multiplex buffer claim_text 5 an asynchronous transfer mode hereinafter referred to atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively input multiplex buffer control means controlling said input multiplex buffer so that output of cell addressed to certain output port indicated by first back pressure signal is inhibited and output separation buffer control means monitoring an accumulation amount of cells in said output separation buffer at every output port to which said cell is addressed and outputting said first back pressure signal to said input multiplex buffer control means to inhibit output of cell addressed to an output port having said accumulation amount equal to or more than predetermined threshold value detection means detecting number of active connections existing between each output port and each input multiplex buffer and said output separation buffer control means further comprises output control means controlling number of cells addressed to said output port output from an output separation buffer so that said number of cells are in proportional to number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said output control means further comprises counter each provided an output port means setting each number of active connections detected by said detection by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting counter value corresponding to an output port which has output cells at every output of cell claim_text 6 the atm switch of claim 5 wherein said output control means comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to said output separation buffer when said cells output from said output separation buffer are equal to or larger than cells input thereto and means outputting said first back pressure signal inhibiting output of cells addressed to said detected output port to said input multiplex buffer control means claim_text 7 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises counter provided each output port means setting each number of active connections detected by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting value of said counter corresponding to an output port that has output cell at every output of cell claim_text 8 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to an output separation buffer when cells output separation buffer when cells output from said output separation buffer is equal to or more than cells input thereto and means outputting said first back pressure signal inhibiting output of cell addressed to said detected output port to said input multiplex buffer control means lang en",
    "label": 0
  },
  {
    "id1": "079-134-842-572-348",
    "id2": "152-806-746-371-786",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising or replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing the library of dataset from the datastore identifying based on the task and the dataset additional data from the library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 11 the computerimplemented method of claim 1 wherein detecting the data deficit comprises identifying invalid data types and outlier values wherein the additional data further comprise approximated values corresponding to the outliers values corrected data types corresponding to the invalid data types and the approximated values comprise similar data points from the other dataset claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access the library of dataset from the datastore identify based on the task and the dataset additional data from the library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server access look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identify using the look up table the machine learning tool corresponding to the type of task based on the definition of the task form utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task lang en",
    "claims2": "claims claim_text 1 pipeline type processor asynchronous transfer mode atm cells comprisingninput buffers storing n pieces of asynchronous transmission mode atm cellsnn pairs of processor and buffer pipeline processing said each cell the processors provided in first pipeline arrangement and the buffers provided in second pipeline arrangementnan address determining section determining pipeline processing condition said address determining section determining processing instruction code starting address of said each cell based upon the information of said cell stored in said input buffer andnan n1 address transferring sections respectively provided between adjacent ones of said processors in the first pipeline arrangement each of said address transferring sections transferring said processing instruction code starting address between one of the adjacent ones of said processors on an incoming side of the first pipeline arrangement to an other of the adjacent ones of said processors on an outgoing side of the pipeline arrangementnwherein said processors sequentially execute pipeline processing based upon said processing instruction code starting address andnwherein the first pipeline arrangement is separate from the second pipeline arrangement and wherein said each cell is passed along the second pipeline arrangement of said buffers execution by said respective processors at different one of plurality of states claim_text 2 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein each of said address determining sections includesnan information storing memory storing information required retrievalnan address storing memory storing an address at which processing instruction code executed by said processor is storedna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining section claim_text 3 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 4 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci claim_text 5 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said address determining section determines storing address an instruction code according to data format claim_text 6 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said pipeline processing condition is not determined by any of said processors but instead is provided respective processing instruction code starting address each said processor claim_text 7 pipeline type processor asynchronous transfer mode atm cells provided with plurality of processors each executing partial process dedicated to each processor an atm cell being stored into buffer corresponding to each processor said pipeline type processor comprisingnan address storing memory which stores plurality of sets of predetermined process starting addresses each of processors constituting said pipeline type processor each set of predetermined process starting addresses corresponding to conditions of the atm cell to be processed in different respective statenan address retrieving section which retrieves set of predetermined process starting addresses from said address storing memory in accordance with conditions extracted from an input atm cell and transfers each predetermined processing starting address to the corresponding processor andnan execution control section which detects atm cell arrival at each buffer corresponding to each processor and instructs the execution of process to the processor when the atm cell arrives at the corresponding buffernwherein said processors are disposed in first pipeline arrangement separate from second pipeline arrangement in which said buffers are disposed claim_text 8 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein at least one of said processors includes state transition control sections which read instruction code from memory and decode and execute pipeline processing based upon said starting address claim_text 9 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprising an address transferring section provided between said processors said address transferring section transferring said processing instruction code starting address claim_text 10 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprisingnan input buffernan information storing memory storing information required retrievalna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining sectionnwherein the processing instruction code starting address is respectively different each of said processors based on plurality of states the retrieval condition that are obtained from the information storing memory in correspondence with particular condition that matches the retrieval condition claim_text 11 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 12 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci lang en",
    "label": 0
  },
  {
    "id1": "122-175-620-169-033",
    "id2": "130-909-330-439-20X",
    "claims1": "claims claim_text 1 canceled claim_text 2 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 3 the method of claim 2 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 6 the method of claim 5 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 7 the method of claim 3 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 8 the method of claim 2 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 9 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 10 the system of claim 9 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 13 the system of claim 12 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 14 the system of claim 10 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 15 the system of claim 9 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 16 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 17 the nontransitory computer storage medium of claim 16 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 20 the nontransitory computer storage medium of claim 19 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 21 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time t res of the at least one compound in the chromatography bed such that equation i is satisfied t res t diff equation i calculating concentration cz t of the at least one compound in the mobile phase at predetermined location z of the chromatography bed and at predetermined time t based on the adsorption isotherm wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 2 the method according to claim 1 wherein the minimum residence time t res of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 3 the method according to claim 1 wherein in the calculation step cz t is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 4 the method according to claim 3 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 5 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 6 the method according to claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 7 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 8 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 9 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 10 the method according to claim 9 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 11 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 12 the chromatography method according to claim 11 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 13 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 9 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "032-496-986-475-726",
    "id2": "159-445-014-625-895",
    "claims1": "claims claim_text computerimplemented method improving locality of machine learning models the method performed by data processing apparatus the method comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the method of claim 1 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the method of claim 3 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the method of one of claims 1 to 5 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text machine learning computations system comprisingn data processing apparatus and n memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the system of claim 7 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the system of claim 9 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the system of one of claims 7 to 11 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the nontransitory computer storage medium of claim 13 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 8 the method of claim 1 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 11 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "label": 1
  },
  {
    "id1": "183-449-275-297-090",
    "id2": "022-129-282-380-908",
    "claims1": "claims claim_text claims 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "claims2": "claims claim_text 1 system building learning machine comprising reference learning machine target learning machine being built component analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of components in the reference learning machine and the target learning machine and return set of output values each component on the list of components and component tuner module configured to modify different components in the target learning machine based on the set of output values and component mapping thereby resulting in tuned learning machine claim_text 2 the system of claim 1 further comprising feedback loop feeding back the tuned learning machine new target learning machine in an iterative manner claim_text 3 the system of claim 1 wherein the target learning machine is new component to be inserted into the reference learning machine claim_text 4 the system of claim 1 wherein the target learning machine replaces an existing set of components in the reference learning machine claim_text 5 the system of claim 1 wherein the reference learning machine and the target learning machine are graphbased learning machines claim_text 6 the system of claim 5 wherein the component analyzer module is node analyzer module claim_text 7 the system of claim 5 wherein the component tuner module is an interconnect tuner module claim_text 8 the system of claim 5 wherein the component mapping is mapping between nodes from the reference learning machine and nodes from the target learning machine claim_text 9 the system of claim 7 wherein the interconnect tuner module updates interconnect weights in the target learning machine claim_text 10 the system of claim 1 wherein the tuned learning machine includes components updated by the component tuner module claim_text 11 system building learning machine comprising an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 12 the system of claim 11 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture claim_text 13 the system of claim 12 further comprising feedback loop feeding back the new graphbased learning machine new initial graphbased learning machine in an iterative manner claim_text 14 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to each machine component in the initial graphbased learning machine compute an average of set of computed output values of each machine component each data point in the set of data points to produce combined output value of each machine component corresponding to each data point in the set of data points and compute machine component importance score each machine component by averaging final combined output values of each machine component corresponding to all data points in the set of data points from the set of data and dividing the average by normalization value claim_text 15 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations randomly select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the initial graphbased learning machine average set of computed output values each data point in the set of data points to produce final combined output value corresponding to each data point and compute full machine score each component in the initial graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data claim_text 16 the system of claim 15 wherein the machine analyzer is further configured to feed each data point in the set of data points from the set of data into reduced graphbased learning machine with at least some machine components excluded predetermined set of iterations randomly select groups of nodes and interconnects in the reduced graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the reduced graphbased learning machine compute an average of set of computed output values each data point to produce final combined output value corresponding to each data point and compute reduced machine score each component in the reduced graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data points in the set of data claim_text 17 the system of claim 11 wherein the machine architecture builder is further configured to control the size of the new graphbased learning machine architecture based on the set of machine component importance scores and the set of machine factors claim_text 18 the system of claim 17 wherein the machine architecture builder controls the size of the new graphbased learning machine architecture by determining whether each node will exist in the new graphbased learning machine architecture claim_text 19 system building learning machine comprising reference learning machine target learning machine being built node analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of nodes in the reference learning machine and the target learning machine and return set of output values each component on the list of nodes an interconnect tuner module configured to modify different components in the target learning machine based on the set of output values and node mapping thereby resulting in tuned learning machine an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 20 the system of claim 19 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture lang en",
    "label": 1
  },
  {
    "id1": "020-698-584-160-318",
    "id2": "052-798-027-613-66X",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "label": 1
  },
  {
    "id1": "163-517-312-183-954",
    "id2": "022-995-952-592-178",
    "claims1": "claims claim_text 1 method comprising initiating active machine learning through an active machine learning system configured to train an auxiliary machine learning model to produce at least one new labeled observation refining target machine learning model based at least on the active machine learning wherein the target machine learning model includes limitedcapacity machine learning model and retraining the auxiliary machine learning model with the at least one new labeled observation subsequent to refining the capacity of the target machine learning model claim_text 2 method claim 1 recites wherein the auxiliary machine learning model includes capacity larger than the target machine learning model claim_text 3 method claim 1 recites wherein the auxiliary machine learning model includes semantic machine learning model claim_text 4 method claim 3 recites wherein the semantic machine learning model includes bagofwords machine learning model claim_text 5 method claim 1 recites wherein initiating the active machine learning comprises selecting one or more unlabeled observations from pool of unlabeled observations claim_text 6 method claim 5 recites wherein the refining the capacity includes incrementally adding or removing features from the target machine learning model based on an output of the auxiliary machine learning model responsive to processing the one or more unlabeled observations claim_text 7 method claim 1 recites wherein the refining the capacity includes incrementally adding or removing features from the target machine learning model based at least on the initiated active machine learning of the auxiliary machine learning model claim_text 8 method claim 1 recites further comprising implementing diversity in the initiated active machine learning by at least one submodular function claim_text 9 method claim 1 recites further comprising implementing diversity in the initiated active machine learning by establishing subset labelset in pool of unlabeled observations configured to provide diverse unlabeled observations from the subset labelset claim_text 10 method claim 9 recites further comprising selecting one or more unlabeled observations from the subset labelset processing by the auxiliary machine learning model claim_text 11 method claim 1 recites further comprising reducing colorblindness of the target machine learning model based at least on disagreement between the auxiliary machine learning model and the target machine learning model claim_text 12 computerreadable medium having computerexecutable instructions thereupon that when executed by computer cause the computer to perform operations comprising selecting an unlabeled observation from pool of unlabeled observations through an auxiliary machine learning model wherein it is not known to which one of plurality of classes the unlabeled observation belongs converting the unlabeled observation to new labeled observation based on an output of the auxiliary machine learning model responsive to the unlabeled observation refining capacity of target machine learning model based on the converting wherein the target machine learning model is limitedcapacity machine learning model and retraining the auxiliary machine learning model with the new labeled observation subsequent to refining the capacity of the target machine learning model claim_text 13 computerreadable medium claim 12 recites wherein the refining the capacity includes incrementally adding at least one feature to the target machine learning model based on features contained within the new labeled observation and incrementally removing at least one feature from the target machine learning model based on the features contained within the new labeled observation claim_text 14 computerreadable medium claim 12 recites wherein the selecting the unlabeled observation includes selecting the unlabeled observations based at least on optimization of at least one submodular function claim_text 15 computerreadable medium claim 12 recites wherein the selecting the unlabeled observation includes selecting the unlabeled observations from subset labelset in the pool of unlabeled observations the subset labelset configured to provide diverse unlabeled observations claim_text 16 computerreadable medium claim 12 recites wherein the refining the capacity of the target machine learning model includes reducing colorblindness of the target machine learning model based at least on disagreement between the auxiliary machine learning model and the target machine learning model claim_text 17 an active machine learning system the system comprising an auxiliary machine learning model configured to assign first score to an unlabeled observation target machine learning model configured to assign second score to the unlabeled observation wherein the target machine learning model and the auxiliary machine learning model are from different machine learning model classes and wherein the target machine learning model is limitedcapacity machine learning model comparison component configured to compare the first score and the second score to determine probability that the target machine learning model has returned false positive or false negative result and featuring component configured to receive the output of the comparison component claim_text 18 system claim 17 recites wherein the comparison component configured to compare the first score and the second score is further configured to perform comparison comprising determining magnitude of the difference between the first score and the second score determining that the target machine learning model has returned false positive when the magnitude is negative and determining that the target machine learning model has returned false negative when the magnitude is positive claim_text 19 system claim 18 recites further comprising capacityrefining component in operative communication with the featuring component the capacityrefining component configured to extend scope of the target machine learning model to include new feature previously not within the scope of the target machine learning model when the target machine learning model has returned false positive claim_text 20 system claim 18 recites further comprising capacityrefining component in operative communication with the featuring component the capacityrefining component configured to narrow scope of the target machine learning model to remove feature previously within the scope of the target machine learning model when the target machine learning model has returned false positive lang en",
    "claims2": "claims claim_text 1 method comprising training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models claim_text 2 the method of claim 1 wherein managing the one or more machine learning features includes generating new version of the machine learning model based on the feature importance metrics claim_text 3 the method of claim 1 wherein managing the one or more machine learning features includes determining to remove one of the one or more machine learning features based on determination that its associated feature importance metric does not meet threshold value claim_text 4 the method of claim 1 wherein managing the one or more machine learning features includes retraining the machine learning model to remove at least one of the one or more machine learning features from the machine learning model claim_text 5 the method of claim 4 wherein managing the one or more machine learning features includes automatically deleting stored data of the at least one removed feature claim_text 6 the method of claim 4 wherein managing the one or more machine learning features includes automatically causing data of the at least one removed feature to be longer collected claim_text 7 the method of claim 1 wherein managing the one or more machine learning features includes modifying at least one of the one or more machine learning features and retraining the machine learning model using the at least one modified feature of the one or more machine learning features claim_text 8 the method of claim 1 wherein managing the one or more machine learning features includes generating new feature based on at least one of the one or more machine learning features and retraining the machine learning model using the new feature claim_text 9 the method of claim 1 wherein determining the feature importance metric selected one of the machine learning features includes comparing base performance of the machine learning model selected test dataset with new performance of the machine learning model modified version of the selected test dataset of the selected machine learning feature claim_text 10 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by modifying values corresponding to the selected machine learning feature using modification approach selected based on property of machine learning model claim_text 11 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by randomizing values corresponding to the selected machine learning feature claim_text 12 the method of claim 1 wherein the each feature importance metric includes numerical value representing an amount of contribution of the corresponding machine learning feature to an inference result of the machine learning model claim_text 13 the method of claim 1 wherein the each feature importance metric includes numerical value representing rank order of the corresponding machine learning feature compared to others of the machine learning features claim_text 14 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes determining ranking order of the machine learning features of the plurality of different machine learning models claim_text 15 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes identifying feature sharing across plurality of the different machine learning models claim_text 16 the method of claim 15 wherein identifying the feature sharing across the plurality of the different machine learning models includes identifying plurality of different feature importance metrics same shared feature claim_text 17 the method of claim 15 wherein managing together machine learning features of the plurality of different machine learning models includes determining whether to remove selected machine learning feature of one of the different machine learning models based on identified sharing of the selected machine learning feature among plurality of the different machine learning models claim_text 18 system comprising processor configured to train machine learning model determine feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics manage one or more machine learning features of the plurality of machine learning features of the machine learning model wherein being configured to manage the one or more machine learning features of the machine learning model includes being configured to manage together machine learning features of plurality of different machine learning models and memory coupled to the processor and configured to provide the processor with instructions claim_text 19 the system of claim 18 wherein being configured to manage together the machine learning features of the plurality of different machine learning models includes being configured to identify feature sharing across plurality of the different machine learning models claim_text 20 computer program product the computer program product being embodied in nontransitory computer readable storage medium and comprising computer instructions training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models lang en",
    "label": 1
  },
  {
    "id1": "183-449-275-297-090",
    "id2": "186-442-146-751-290",
    "claims1": "claims claim_text claims 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "claims2": "claims claim_text claims what is claimed is 1 computerimplemented method comprising accessing training data computing resource limit setting and parameters of machine learning model forming at server machine learning training strategy based on the training data and the computing resource limit setting forming machine learning model configuration based on the machine learning training strategy selecting sampled data from the training data based on the machine learning training strategy and providing the machine learning model configuration and the sampled data to machine learning platform 2 the computerimplemented method of claim 1 further comprising training using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitoring computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data 3 the computerimplemented method of claim 2 further comprising generating an updated machine learning model configuration recommendation based on the resource usage data 4 the computerimplemented method of claim 2 further comprising receiving from client device the training data the computing resource limit setting and parameters of the machine learning model and providing the resource usage data and the trained machine learning model to the client device 5 the computerimplemented method of claim 2 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model 6 the computerimplemented method of claim 2 wherein forming the machine learning training strategy comprises providing summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receiving the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learning model and an accuracy of the machine learning model 7 the computerimplemented method of claim 6 further comprising providing the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy forming an updated machine learning model configuration based on the updated machine learning training strategy and providing the updated machine learning model configuration and the sampled data to the machine learning platform 8 the computerimplemented method of claim 1 further comprises testing deployed machine learning model based on the machine learning model configuration accessing performance assessment of the deployed machine learning model and generating performance indicator of the deployed machine learning model based on the testing and the performance assessment determining that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises updating the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model 9 the computerimplemented method of claim 1 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device 10 the computerimplemented method of claim 1 wherein the machine learning training strategy comprises one of random forest classifier or gaussian process regressor 11 computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the apparatus to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform 12 the computing apparatus of claim 11 wherein the instructions further configure the apparatus to train using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitor computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data 13 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to generate an updated machine learning model configuration recommendation based on the resource usage data 14 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to receive from client device the training data the computing resource limit setting and parameters of the machine learning model and provide the resource usage data and the trained machine learning model to the client device 15 the computing apparatus of claim 12 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model 16 the computing apparatus of claim 12 wherein forming the machine learn training strategy comprises provide summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receive the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learn model and an accuracy of the machine learning model 17 the computing apparatus of claim 16 wherein the instructions further configure the apparatus to provide the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy form an updated machine learning model configuration based on the updated machine learning training strategy and provide the updated machine learning model configuration and the sampled data to the machine learning platform 18 the computing apparatus of claim 11 further comprises test deployed machine learning model based on the machine learning model configuration access performance assessment of the deployed machine learning model and generate performance indicator of the deployed machine learning model based on the testing and the performance assessment determine that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises update the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model 19 the computing apparatus of claim 11 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device 20 nontransitory computerreadable storage medium the computer readable storage medium including instructions that when executed by computer cause the computer to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform lang en",
    "label": 1
  },
  {
    "id1": "023-604-939-311-912",
    "id2": "190-059-083-058-239",
    "claims1": "claims claim_text claims 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computer readable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 machine learning ml computer system training student ml system iteratively through machine learning on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of training of the student ml system the learned parameters the first node of the first inner layer are updated and the ml computer system comprises learning coach ml system that is in communication with the student ml system wherein the learning coach ml system comprises neural network that comprises an input layer and an output layer the learning coach ml system has been trained through machine learning to determine and implement an enhancement to the student ml system based on input to the learning coach ml system from the student ml system wherein the input from the student ml system is input to the input layer of the learning coach ml system the input to the learning coach ml system from the student ml system comprises observations about an internal state of the student ml system during training of the student ml system and the observations about the internal state of the student ml system comprise values related to the learned parameters the first node on the first inner layer of the student ml system and the activation values the first node on the first inner layer of the student ml system such that the learning coach ml system determines and implements the enhancement to the student ml system based on the input from the student ml system claim_text 2 the ml computer system of claim 1 wherein the learning coach ml system comprises pattern recognition system that recognizes different patterns than the student ml system claim_text 3 the ml computer system of claim 1 wherein the learning coach ml system has different objective than the student ml system claim_text 4 the ml computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 5 the ml system of claim 4 wherein the revised hyperparameter comprises hyperparameter selected from the group consisting of mini batch size the student ml system learning rate the student ml system regularization parameter the student ml system and momentum parameter the student ml system claim_text 6 the ml computer system of claim 1 wherein the enhancement comprises structural change to the student ml system claim_text 7 the ml computer system of claim 6 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 8 the ml computer system of claim 6 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 9 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 10 the ml computer system of claim 9 wherein the selected layer of the student ml system comprises plurality of existing nodes prior to the implementation of the enhancement and after the enhancement the existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the student ml system below the selected layer and activations of the virtual nodes are controlled by the learning coach ml system claim_text 11 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and learning coach ml system controls regularization to the second set of nodes so differences between activation values the second set of nodes and activation values the first set of nodes is less than threshold value to control dropout rate of the nodes in the first and second sets claim_text 12 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing connection weights the one or more additional nodes of the student ml system claim_text 13 the ml computer system of claim 1 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 14 the ml computer system of claim 1 wherein the ml system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activation values the student ml system are stored in the high speed memory so that the student ml system can be run when the student ml system is active and the connection weights and activation values the student ml system are stored in the secondary storage when the student ml system is not active claim_text 15 the ml system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 16 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system regression claim_text 17 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system classification task claim_text 18 machinelearning ml training method comprising initially training learning coach ml system to determine and implement an enhancement to ml system wherein the learning coach ml system comprises an input layer and an output layer training iteratively through machine learning student ml system on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and the initial training the student ml system comprises following iterations of the training of the student ml system updating the learned parameters the first node of the first inner layer and training the student ml system comprises receiving by the learning coach ml system from the student ml system values related to the learned parameters and the activation values of the first node of the first inner layer of the student ml system and determining and implementing by the learning coach ml system the enhancement to the student ml system based on the values related to the learned parameters and the activation values of the first node of the inner layer of the student ml system to improve performance of the student ml system claim_text 19 the ml training method of claim 18 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 20 the ml training method of claim 18 wherein the enhancement comprises structural change to the student ml system claim_text 21 the ml training method of claim 20 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 22 the ml training method of claim 20 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 23 the ml training method of claim 22 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 24 the ml training method of claim 18 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 25 the ml training method of claim 18 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 26 the ml computer system of claim 1 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and during training of the student ml system activation values each of the multiple feature node are softtied training data examples within the cluster claim_text 27 the ml computer system of claim 26 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 28 the ml computer system of claim 27 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 29 the ml computer system of claim 27 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 30 the ml computer system of claim 29 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 31 the ml computer system of claim 30 wherein the ml learning coach is configured to set the relaxation strength hyperparameters the feature nodes claim_text 32 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to an activation value of node in separate ml system claim_text 33 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to activation values of multiple nodes in separate ml system claim_text 34 the ml computer system of claim 33 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 35 the ml computer system of claim 34 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 36 the ml computer system of claim 35 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 37 the ml computer system of claim 36 wherein the learning coach ml system is configured to set the relaxation strength hyperparameter the first node claim_text 38 the ml computer system of claim 1 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the ml computer system further comprises data switching network that selectively directs training data examples the student ml system to one or more and less than all of the multiple ml ensemble members claim_text 39 the ml computer system of claim 38 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 40 the ml computer system of claim 38 further comprising classifier classifying the training data examples the student ml system and wherein the classifier controls the data switching network training data example based on classification by the classifier the training data example claim_text 41 the ml training method of claim 18 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and training the student ml system further comprises softtying activation values each of the multiple feature nodes training data examples within the cluster claim_text 42 the ml training method of claim 41 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 43 the ml training method of claim 42 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 44 the ml training method of claim 42 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 45 the ml training method of claim 44 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 46 the ml training method of claim 45 further comprising setting by the ml learning coach the relaxation strength hyperparameters the feature nodes claim_text 47 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to an activation value of node in separate ml system claim_text 48 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to activation values of multiple nodes in separate ml system claim_text 49 the ml training method of claim 48 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 50 the ml training method of claim 49 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 51 the ml training method of claim 50 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 52 the ml training method of claim 51 further comprising setting by the learning coach ml system the relaxation strength hyperparameter the first node claim_text 53 the ml training method of claim 18 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the method further comprises selectively directing by data switching network training data examples the student ml system to one or more but less than all of the multiple ml ensemble members claim_text 54 the ml training method of claim 53 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 55 the ml training method of claim 53 further comprising classifying by classifier the training data examples the student ml system and controlling by the classifier the data switching network training data example based on classification by the classifier the training data example lang en",
    "label": 1
  },
  {
    "id1": "152-848-867-961-18X",
    "id2": "086-031-183-779-370",
    "claims1": "claims claim_text claims we claim 1 method comprising training via at least one processor of computer system using public data inputs public data machine learning model training via the at least one processor using private data inputs private data machine learning model training via the at least one processor public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing via the at least one processor the public data machine learning model using current public data input resulting in public data machine learning prediction executing via the at least one processor the private data machine learning model using current pnvate data input resulting in private data machine learning prediction and executing via the at least one processor the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 2 the method of claim 1 wherein the public data and the private data are associated with freight transport costs 3 the method of claim 2 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 4 the method of claim 2 wherein the private data comprises overhead costs and earner costs 5 the method of claim 1 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 6 the method of claim 5 further comprising adding via the at least one processor the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding via the at least one processor the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 7 the method of claim 1 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 8 system comprising at least one processor and nontransitory computerreadable storage medium having instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 9 the system of claim 8 wherein the public data and the private data are associated with freight transport costs 10 the system of claim 9 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 11 the system of claim 9 wherein the private data comprises overhead costs and earner costs 12 the system of claim 8 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 13 the system of claim 12 the nontransitory computerreadable storage medium having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 14 the system of claim 8 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 15 nontransitory computerreadable storage medium having instructions stored which when executed by at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 16 the nontransitory computerreadable storage medium of claim 15 wherein the public data and the private data are associated with freight transport costs 17 the nontransitory computerreadable storage medium of claim 16 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 18 the nontransitory computerreadable storage medium of claim 16 wherein the private data comprises overhead costs and carrier costs 19 the nontransitory computerreadable storage medium of claim 15 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 20 the nontransitory computerreadable storage medium of claim 19 having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data lang en",
    "claims2": "claims claim_text 1 an information processing device optimizing information presented by an information presenter to an information recipient comprising an attribute determination unit that determines knowledge level and an understanding level of the information recipient with respect to the information based on biological activity information of the information recipient acquired by sensor presentation information feature determination unit that determines feature of the information presented by the information presenter to the information recipient and feedback optimization unit that converts presentation format of the information into presentation format according to the knowledge level and the understanding level of the information recipient based on the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the feature of the information determined by the presentation information feature determination unit claim_text 2 the information processing device according to claim 1 wherein the presentation information feature determination unit determines the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation in case where the information is auditory information claim_text 3 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the visual recognition level based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 4 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the voice speed level based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 5 the information processing device according to claim 1 wherein the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient by the feedback optimization unit are presented to the information presenter claim_text 6 an information processing method optimizing information presented by an information presenter to an information recipient using an information processing device comprising first step in which knowledge level and an understanding level of the information recipient with respect to the information are determined based on biological activity of the information recipient acquired by sensor and feature of the information presented by the information presenter to the information recipient is determined and second step in which presentation format of the information is converted into presentation format according to the knowledge level and the understanding level of the information recipient based on the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the feature of the information claim_text 7 the information processing method according to claim 6 wherein in the first step the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation are determined in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation are determined in case where the information is auditory information claim_text 8 the information processing method according to claim 6 wherein in the first step the visual recognition level is determined based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level is determined based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 9 the information processing method according to claim 6 wherein in the first step the voice speed level is determined based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level is determined based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 10 the information processing method according to claim 6 further comprising third step in which the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient are presented to the information presenter lang en",
    "label": 0
  },
  {
    "id1": "062-542-216-489-868",
    "id2": "038-794-758-325-674",
    "claims1": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "claims2": "claims claim_text 1 system training probabilistic predictive model recommending experiment designs synthetic biology comprising nontransitory memory configured to store executable instructions and hardware processor in communication with the nontransitory memory the hardware processor programmed by the executable instructions to receive synthetic biology experimental data generate training data from the synthetic biology experimental data wherein the training data comprise plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective train using the training data plurality of level0 learners of probabilistic predictive model recommending experiment designs synthetic biology wherein an input of each of the plurality of level0 learners comprises input values of the input variables and wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable and train using i predicted values of the at least one response variable determined using the plurality of level0 learners the training inputs of the plurality of training inputs and ii the reference outputs of the plurality of reference outputs correspondence to the training inputs of the plurality of training inputs level1 learner of the probabilistic predictive model recommending experiment designs synthetic biology comprising probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable claim_text 2 10 canceled claim_text 11 the system of claim 1 wherein the synthetic biology experimental data is sparse claim_text 12 the system of claim 1 wherein number of the plurality of training inputs in the synthetic biology experiment data is number of experimental conditions number of strains number of replicates of strain of the strains or combination thereof claim_text 13 15 canceled claim_text 16 the system of claim 1 wherein one or each of the plurality of input variables andor the at least one response variable comprises promoter sequence an induction time an induction strength ribosome binding sequence copy number of gene transcription level of gene an epigenetics state of gene level of protein post translation modification state of protein level of molecule an identity of molecule level of microbe state of microbe state of microbiome titer rate yield or combination thereof optionally wherein the molecule comprises an inorganic molecule an organic molecule protein polypeptide carbohydrate sugar fatty acid lipid an alcohol fuel metabolite drug an anticancer drug biofuel flavoring molecule fertilizer molecule or combination thereof claim_text 17 21 canceled claim_text 22 the system of claim 1 wherein the predetermined response variable objective comprises maximization objective minimization objective or specification objective andor wherein the predetermined response variable objective comprises maximizing the at least one response variable minimizing the at least one response variable or adjusting the at least one response variable to predetermined value of the at least one response variable claim_text 23 the system of claim 1 wherein to train the plurality of level1 learner the hardware processor is programmed by the executable instructions to determine using the plurality of level0 learners the predicted values of the at least one response variable training inputs of the plurality of training inputs claim_text 24 the system of claim 1 wherein the level1 learner comprises bayesian ensemble of the plurality of level0 learners claim_text 25 the system of claim 1 wherein parameters of the ensemble of the plurality of level0 learners comprises i plurality of ensemble weights and ii an error variable distribution of the ensemble or standard deviation of the error variable distribution of the ensemble claim_text 26 32 canceled claim_text 33 the system of claim 1 wherein to train the level1 learner the hardware processor is programmed by the executable instructions to determine posterior distribution of the ensemble parameters given the training data or the second subset of the training data wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to determine i probability distribution of the training data or the second subset of the training data given the ensemble parameters or likelihood function of the ensemble parameters given the training data of the second subset of the training data and ii prior distribution of the ensemble parameters and wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to sample space of the ensemble parameters with frequency proportional to desired posterior distribution claim_text 34 canceled claim_text 35 canceled claim_text 36 the system of claim 1 wherein to train the plurality of level0 learners the hardware processor is programmed by the executable instructions to generate first subset of the training data and train using the first subset of the training data the plurality of level0 learners claim_text 37 50 canceled claim_text 51 the system of claim 1 wherein the hardware processor is programmed by the executable instructions to determine surrogate function with an input experiment design an input the surrogate function comprising an expected value of the at least one response variable determined using the input experiment design variance of the value of the at least one response variable determined using the input experiment design and an exploitationexploration tradeoff parameter and determine using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of synthetic biology experiment obtaining predetermined response variable objective associated with the at least one response variable claim_text 52 57 canceled claim_text 58 the system of claim 51 wherein to determine the plurality of recommended experiment designs the hardware processor is programmed by the executable instructions to determine plurality of possible recommended experiment designs each comprising possible recommended values of the input variables with surrogate function values determined using the surrogate function with predetermined characteristic and select the plurality of recommended experiment designs from the plurality of possible recommended experiment designs using an input variable difference factor based on the surrogate function values of the plurality of possible recommended experiment designs claim_text 59 64 canceled claim_text 65 the system of claim 51 wherein number of the plurality of recommended experiment designs is number of experimental conditions or number of strains the next cycle of the synthetic biology experiment claim_text 66 canceled claim_text 67 canceled claim_text 68 the system of claim 51 wherein to determine the plurality of possible recommended experiment designs the hardware processor is programmed by the executable instructions to sample space of the input variables with frequency proportional to the surrogate function or an exponential function of the surrogate function and prior distribution of the input variables claim_text 69 canceled claim_text 70 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine an upper bound andor lower bound one or each of the plurality of input variables based on training values of the corresponding input variable wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 71 canceled claim_text 72 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to receive an upper bound andor lower bound one or each of the plurality of input variables wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 73 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability distribution of the at least one response variable one or each of the plurality of recommended experiment designs claim_text 74 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of one or each of the plurality of recommended experiment designs being predetermined percentage closer to achieving the objective relative to the training data claim_text 75 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective being predetermined percentage closer to achieving the objective relative to the training data claim_text 76 canceled claim_text 77 canceled claim_text 78 method recommending experiment designs synthetic biology comprising under control of hardware processor receiving probabilistic predictive model recommending experiment designs synthetic biology comprising plurality of level0 learners and level1 learner wherein an input of each of the plurality of level0 learners comprises input values of the input variables wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable wherein the level1 learner comprises probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable wherein the plurality of level0 learners and the level1 learner are trained using training data obtained from one or more cycles of synthetic biology experiment comprising plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective determining surrogate function comprising an expected value of the level1 learner variance of the level1 learner and an exploitationexploration tradeoff parameter and determining using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of the synthetic biology experiment achieving predetermined response variable objective associated with the at least one response variable claim_text 79 85 canceled lang en",
    "label": 0
  },
  {
    "id1": "044-438-135-761-652",
    "id2": "110-881-173-371-629",
    "claims1": "claims claim_text 1 canceled claim_text 2 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 3 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation values of the first node each training data item associated to the cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 4 the machine learning system of claim 3 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 5 the machine learning system of claim 4 wherein the vector norm is the l2 norm claim_text 6 the machine learning system of claim 3 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 7 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 8 the machine learning system of claim 7 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 9 the machine learning system of claim 3 wherein the regularization term comprises relaxation strength hyperparameter claim_text 10 the machine learning computer system of claim 9 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 11 the machine learning system of claim 2 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 12 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes including first node and iteratively training the first neural network comprises in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 13 the computerimplemented machine learning method of claim 12 wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 14 the computerimplemented machine learning method of claim 13 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 15 the computerimplemented machine learning method of claim 14 wherein the vector norm is the l2 norm claim_text 16 the computerimplemented machine learning method of claim 13 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 17 the computerimplemented machine learning method of claim 12 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 18 the computerimplemented machine learning method of claim 13 wherein the regularization term comprises relaxation strength hyperparameter claim_text 19 the computerimplemented machine learning method of claim 18 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 20 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network compute activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 21 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 22 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 23 the machine learning system of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 24 the machine learning computer system of claim 23 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 25 the machine learning system of claim 20 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 26 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and iteratively training the first neural network comprises by in forward propagations through the first neural network computing activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 27 the computerimplemented machine learning method of claim 26 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 28 the computerimplemented machine learning method of claim 26 wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 29 the computerimplemented machine learning method of claim 28 wherein the regularization term comprises relaxation strength hyperparameter claim_text 30 the computerimplemented machine learning method of claim 29 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 method comprising storing at first machine learning server computer one or more machine learning configuration files first machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters storing at the first machine learning server computer plurality of machine learning training datasets displaying at the first machine learning server computer through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the first machine learning server computer particular input dataset receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable training options corresponding to particular machine learning training dataset the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options comprising list of one or more second selectable parameter and training options each of the one or more second selectable parameter and training options being displayed in the graphical user interface via radio buttons or checkboxes the selection of the one or more second selectable parameter and training options comprising selection of corresponding graphical user interface radio button or checkbox replacing in the first machine learning configuration file at the first machine learning server computer the one or more first machine learning parameters with the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options using at the first machine learning server computer the first machine learning configuration file configuring particular machine learning system and in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset using at the first machine learning server computer the particular machine learning system and the particular input dataset computing particular output dataset the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing at the first machine learning server computer confidence score threshold value determining that subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving at the first machine learning server computer through the graphical user interface second selection of the one or more second selectable parameter and training options corresponding to second machine learning training dataset of the plurality of machine learning training datasets from second machine learning server computer updating at the first machine learning server computer the first machine learning configuration file with the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset sending to separate server computer the first machine learning configuration file with instructions to configure second machine learning system with the second selection of the one or more second selectable parameter and training options using at the separate server computer the first machine learning configuration file configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing by the first machine learning server computer one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 2 the method of claim 1 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving at the separate server computer second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the second input dataset computing the second output dataset claim_text 3 the method of claim 1 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the one or more selectable training options claim_text 4 the method of claim 1 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value displaying through the graphical user interface subset of plurality of selectable training options wherein each of the subset of the plurality of selectable training options corresponds to the particular data category receiving the first selection of the one or more selectable training options from the subset of the plurality of selectable training options claim_text 5 the method of claim 1 further comprising storing at the first machine learning server computer the confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets third machine learning training dataset using the first machine learning configuration file configuring third machine learning system training the third machine learning system using the third machine learning training dataset of the plurality of machine learning training datasets using the third machine learning system and the subset of the particular input dataset computing third output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the third output dataset claim_text 6 the method of claim 1 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the particular input dataset computing the second output dataset claim_text 7 the method of claim 1 further comprising storing at the second machine learning server computer second plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the second plurality of machine learning training datasets receiving through the graphical user interface selection of both the first selection of one or more selectable training options corresponding to the particular machine learning training dataset and the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset using second machine learning configuration file configuring the second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing the second output dataset claim_text 8 the method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems has higher confidence score than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system has higher confidence score than each other test machine learning system storing second machine learning configuration file with the one or more first machine learning parameters claim_text 9 the method of claim 1 further comprising displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of the first selection of one or more selectable training options corresponding to the particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 10 computer system comprising one or more processors first machine learning server computer memory storing instructions which when executed by the one or more processors cause performance of storing one or more machine learning configuration files at the first machine learning server computer first machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters storing at the first machine learning server computer plurality of machine learning training datasets displaying at the first machine learning server computer through graphical user interface plurality of selectable parameter options each of which defining value machine learning parameter receiving at the first machine learning server computer particular input dataset receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters receiving at the first machine learning server computer through the graphical user interface first selection of one or more selectable training options corresponding to particular machine learning training dataset the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options comprising list of one or more second selectable parameter and training options each of the one or more second selectable parameter and training options being displayed in the graphical user interface via radio buttons or checkboxes the selection of the one or more second selectable parameter and training options comprising selection of corresponding graphical user interface radio button or checkbox replacing in the first machine learning configuration file at the first machine learning server computer the one or more first machine learning parameters with the first selection of the one or more selectable parameter options and the first selection of the one or more selectable training options using at the first machine learning server computer the first machine learning configuration file configuring particular machine learning system and in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset using at the first machine learning server computer the particular machine learning system and the particular input dataset computing particular output dataset the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing at the first machine learning server computer confidence score threshold value determining that subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving at the first machine learning server computer through the graphical user interface second selection of the one or more second selectable parameter and training options corresponding to second machine learning training dataset of the plurality of machine learning training datasets from second machine learning server computer updating at the first machine learning server computer the first machine learning configuration file with the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset sending to separate server computer the first machine learning configuration file with instructions to configure second machine learning system with the second selection of the one or more second selectable parameter and training options using at the separate server computer the first machine learning configuration file configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the subset of the particular input dataset computing second output dataset replacing by the first machine learning server computer one or more data items in the particular output dataset with one or more corresponding items in the second output dataset claim_text 11 the computer system of claim 10 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of first selectable network type option corresponding to machine learning system of the particular machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option receiving second input dataset receiving through the graphical user interface selection of second selectable network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the second input dataset computing the second output dataset claim_text 12 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the one or more selectable training options claim_text 13 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value displaying through the graphical user interface subset of plurality of selectable training options wherein each of the subset of the plurality of selectable training options corresponds to the particular data category receiving the first selection of the one or more selectable training options from the subset of the plurality of selectable training options claim_text 14 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing the confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprises confidence scores below the confidence score threshold value identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets third machine learning training dataset using the first machine learning configuration file configuring third machine learning system training the third machine learning system using the third machine learning training dataset of the plurality of machine learning training datasets using the third machine learning system and the subset of the particular input dataset computing third output dataset replacing one or more data items in the particular output dataset with one or more corresponding items in the third output dataset claim_text 15 the computer system of claim 10 further comprising second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable network type options each of which defining type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring the second machine learning system using the second machine learning system and the particular input dataset computing the second output dataset claim_text 16 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of storing second plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the second plurality of machine learning training datasets receiving through the graphical user interface selection of both the first selection of one or more selectable training options corresponding to the particular machine learning training dataset and the second selection of the one or more second selectable parameter and training options corresponding to the second machine learning training dataset using second machine learning configuration file configuring the second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset using the second machine learning system and the particular input dataset computing the second output dataset claim_text 17 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems has higher confidence score than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system has higher confidence score than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 18 the computer system of claim 10 wherein the instructions when executed by the one or more processors further cause performance of displaying through the graphical user interface plurality of selectable training options each of which corresponding to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of the first selection of one or more selectable training options corresponding to the particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "label": 1
  },
  {
    "id1": "023-604-939-311-912",
    "id2": "051-909-779-474-634",
    "claims1": "claims claim_text claims 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computer readable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "026-671-265-300-529",
    "id2": "036-552-384-938-884",
    "claims1": "claims claim_text 1 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 3 the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 6 the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 8 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 9 the system of claim 8 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 10 the system of claim 9 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 13 the system of claim 9 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 14 the system of claim 8 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 15 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 16 the nontransitory computer storage medium of claim 15 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 17 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 20 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text we claim 1 method 400 determining positioning method user equipment ue in network the method comprising loading by loading unit 302 at location server 300 plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods receiving by transceiver unit 304 at the location server 300 location request associated with service from the ue wherein the location request relates to determining location of the ue selecting by an execution unit 306 at the location server 300 first positioning method from the plurality of positioning methods based on the location request selecting by the execution unit 306 at the location server 300 fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeating by the execution unit 306 one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with nonsupported status selecting by the execution unit 306 at the location server 300 cell identifier cellid based positioning method in an event each of the selected fallback positioning methods from the set of fallback positioning methods is associated with nonsupported status 2 the method 400 claimed in claim 1 wherein each of the plurality of positioning methods is based on one of service identifier service id associated with the service and quality of service qos information associated with the location request 3 the method 400 claimed in claim 2 wherein post the receiving of the location request associated with the service from the ue the method comprises checking by processing unit 308 at the location server 300 an availability of at least one of the service id and the qos information in the received location request 4 the method 400 claimed in claim 2 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 5 the method 400 claimed in claim 1 wherein the location server 300 is location management function lmf 6 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 the first positioning method from the plurality of positioning methods the method comprises determining by the execution unit 306 one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 7 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 at the location server 300 the fallback positioning method from the set of fallback positioning methods the method comprises determining one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 8 the method 400 claimed in claim 6 wherein the cellid identifies base station associated with the ue 9 system 300 determining positioning method user equipment ue in network the system comprising location server 300 wherein the location server 300 comprises loading unit 302 configured to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 connected to at least the loading unit 302 the transceiver unit 304 configured to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 connected to at least the transceiver unit 304 the execution unit 306 configured to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method 10 the system 300 claimed in claim 9 wherein each of the plurality of positioning methods is based on at least one of service identifier service id associated with the service and quality of service qos information associated with the location request 11 the system 300 claimed in claim 10 wherein post receiving the location request associated with the service from the ue the system comprises processing unit 308 configured to check an availability of at least one of the service id and the qos information in the received location request 12 the system 300 claimed in claim 10 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 13 the system 300 claimed in claim 9 wherein the location server 300 is location management function lmf 14 the system 300 claimed in claim 9 wherein prior to the selecting the first positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 15 the system 300 claimed in claim 9 wherein prior to the selecting the fallback positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 16 the system 300 claimed in claim 9 wherein the cellid identifies base station associated with the ue 17 nontransitory computerreadable storage medium storing instructions determining positioning method of user equipment ue in network the storage medium comprising executable code which when executed by one or more units of system comprising location server 300 causes loading unit 302 to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status and select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method lang en",
    "label": 0
  },
  {
    "id1": "165-934-558-356-800",
    "id2": "091-570-367-989-78X",
    "claims1": "claims claim_text 1 48 canceled claim_text 49 method comprising receiving from producer node producer node capabilities comprising machine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity claim_text 50 the method according to claim 49 wherein the at least one machine learning mode associated with the machine learning entity comprises at least one fallback operating mode claim_text 51 the method according to claim 49 further comprising selecting at least one machine learning based functionality among the machine learning based functionalities and causing transmission of an activation request to activate the selected at least one machine learning based functionality to the producer node claim_text 52 the method according to claim 51 further comprising receiving from the producer node an activation request response the activation request response comprising an indication whether the at least one machine learning based functionality is activated claim_text 53 the method according to claim 49 further comprising receiving from the producer node an activation request to activate at least one machine learning based functionality among the machine learning based functionalities claim_text 54 the method according to claim 53 further comprising causing transmission of machine learning based functionality activation response to the producer node claim_text 55 the method according to claim 49 further comprising receiving from the producer node change request to change at least one machine learning based functionality among the machine learning based functionalities claim_text 56 the method according to claim 55 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learning based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality claim_text 57 method comprising identifying producer node capabilities comprising machine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity and causing transmission of the producer node capabilities to consumer node claim_text 58 the method according to claim 57 wherein the at least one machine learning mode associated with the machine learning entity comprises at least one fallback operating mode claim_text 59 the method according to claim 57 further comprising receiving from the consumer node an activation request to activate at least one machine learning based functionality among the machine learning based functionalities claim_text 60 the method according to claim 59 further comprising causing transmission of an activation request response to the consumer node the activation request response comprising an indication whether the at least one machine learning based functionality is activated claim_text 61 the method according to claim 57 further comprising causing transmission of an activation request to activate at least one machine learning based functionality among the machine learning based functionalities to the consumer node claim_text 62 the method according to claim 61 further comprising receiving machine learning based functionality activation response from the consumer node claim_text 63 the method according to claim 57 further comprising causing transmission of change request to change at least one machine learning based functionality among the machine learning based functionalities to the consumer node claim_text 64 the method according to claim 63 wherein the change request comprises cause indication the change the cause indication comprising one of the following deactivation indication to deactivate the machine learning based functionality switch indication to switch to another machine learning based functionality pause indication to pause the machine learning based functionality set time period and reset indication to reset or restart the machine learning based functionality claim_text 65 consumer node comprising at least one processor and at least one memory including computer program code the at least one memory and the computer program code configured to with the at least one processor cause the consumer node to at least perform receive from producer node producer node capabilities comprising machine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity claim_text 66 producer node comprising at least one processor and at least one memory including computer program code the at least one memory and the computer program code configured to with the at least one processor cause the producer node to at least perform identify producer node capabilities comprising machine learning based assistance capability the machine learning based assistance capability comprising machine learning based functionalities each machine learning based functionality comprising machine learning entity and at least one machine learning mode associated with the machine learning entity and cause transmission of the producer node capabilities to consumer node claim_text 67 nontransitory computerreadable medium comprising computer program comprising instructions causing an apparatus to perform the method of claim 49 claim_text 68 nontransitory computerreadable medium comprising computer program comprising instructions causing an apparatus to perform the method of claim 57 lang en",
    "claims2": "claims claim_text 1 method of aiding in diagnosing whether subject has depression comprising analyzing blood sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid alphahydroxyisobutyric acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the biological sample is blood plasma claim_text 8 the method of claim 1 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 9 the method of claim 8 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 10 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and the first sample is obtained from the subject at first time point and wherein the analysis method the blood sample is mass spectrometry analyzing second biological sample that was removed from subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 11 the method of claim 10 wherein the method further comprises comparing the levels of the biomarkers in the first sample the levels of the biomarkers in the second sample andor the results of the comparison of the levels of the biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the biomarkers claim_text 12 the method of claim 10 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 13 the method of claim 12 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 14 method of assessing the efficacy of composition treating depression comprising analyzing blood sample removed from subject having depression and currently or previously being treated with composition to determine the levels of biomarkers depression wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to levels of the biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the biomarkers c depressionnegative reference levels of the biomarkers d depressionprogressionpositive reference levels of the biomarkers andor depressionregressionpositive reference levels of the biomarkers claim_text 15 the method of claim 14 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 16 the method of claim 15 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 17 method assessing the efficacy of composition in treating depression comprising analyzing first blood sample obtained from subject to determine the levels of biomarkers depression the first sample obtained from the subject at first time point wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry administering the composition to the subject analyzing second biological sample obtained from the subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point after administration of the composition and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 18 the method of claim 17 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids lang en",
    "label": 0
  },
  {
    "id1": "044-438-135-761-652",
    "id2": "052-798-027-613-66X",
    "claims1": "claims claim_text 1 canceled claim_text 2 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 3 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation values of the first node each training data item associated to the cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 4 the machine learning system of claim 3 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 5 the machine learning system of claim 4 wherein the vector norm is the l2 norm claim_text 6 the machine learning system of claim 3 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 7 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 8 the machine learning system of claim 7 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 9 the machine learning system of claim 3 wherein the regularization term comprises relaxation strength hyperparameter claim_text 10 the machine learning computer system of claim 9 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 11 the machine learning system of claim 2 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 12 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes including first node and iteratively training the first neural network comprises in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 13 the computerimplemented machine learning method of claim 12 wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 14 the computerimplemented machine learning method of claim 13 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 15 the computerimplemented machine learning method of claim 14 wherein the vector norm is the l2 norm claim_text 16 the computerimplemented machine learning method of claim 13 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 17 the computerimplemented machine learning method of claim 12 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 18 the computerimplemented machine learning method of claim 13 wherein the regularization term comprises relaxation strength hyperparameter claim_text 19 the computerimplemented machine learning method of claim 18 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 20 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network compute activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 21 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 22 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 23 the machine learning system of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 24 the machine learning computer system of claim 23 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 25 the machine learning system of claim 20 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 26 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and iteratively training the first neural network comprises by in forward propagations through the first neural network computing activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 27 the computerimplemented machine learning method of claim 26 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 28 the computerimplemented machine learning method of claim 26 wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 29 the computerimplemented machine learning method of claim 28 wherein the regularization term comprises relaxation strength hyperparameter claim_text 30 the computerimplemented machine learning method of claim 29 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "label": 1
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "080-897-152-234-414",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text claims 1 an apparatus wireless communication including memory and at least one processor coupled to the memory and based at least in part on information stored in the memory the at least one processor is configured to process information with machine learning associated with model identifier id machine learning function or machine learning use case and report data via the wireless communication based on configuration associated with the model id the machine learning function or the machine learning use case 2 the apparatus of claim 1 further comprising transceiver coupled to the at least one processor wherein the at least one processor is further configured to receive the configuration identifying the model id the machine learning function or the machine learning use case wherein reporting the data includes transmitting the data based on condition timing or periodicity indicated in the configuration the model id the machine learning function or the machine learning use case 3 the apparatus of claim 2 wherein the apparatus is the wireless communication at user equipment ue and the at least one processor is configured to receive the configuration from network node and report the data to the network node 4 the apparatus of claim 2 wherein the apparatus is the wireless communication at network node and the at least one processor is configured to receive the configuration from user equipment ue and report the data to the ue 5 the apparatus of claim 2 wherein the apparatus is the wireless communication at first network node and the at least one processor is configured to receive the configuration from second network node and report the data to the second network node 6 the apparatus of claim 2 wherein the apparatus is the wireless communication at first user equipment ue and the at least one processor is configured to receive the configuration from second ue and report the data to the second ue 7 the apparatus of claim 1 wherein the configuration associated with the model id the machine learning function or the machine learning use case indicates one or more of data reporting method at least one input parameter the machine learning unprocessed data to obtain model input parameters at least one measurement to obtain the model input parameters at least one data processing module timing data reporting condition the data reporting or periodicity the data reporting 8 the apparatus of claim 1 wherein the at least one processor is further configured to report different data based on multiple configurations each configuration associated with different the model id different machine learning function or different machine learning use case 9 the apparatus of claim 1 wherein the at least one processor is further configured to receive data reporting activation the model id the machine learning function or the machine learning use case and to report the data in response to the activation 10 the apparatus of claim 1 wherein the at least one processor is further configured to receive data reporting deactivation the model id the machine learning function or the machine learning use case and stop the reporting of the data the model id the machine learning function or the machine learning use case in response to the deactivation 11 the apparatus of claim 1 wherein the at least one processor is configured to report the data in at least one of radio resource control rrc message medium access controlcontrol element macce uplink control information uci or downlink control information dci 12 the apparatus of claim 1 wherein the configuration associated with the model id the machine learning function or the machine learning use case includes data validation configuration wherein the at least one processor is further configured to validate the data prior to reporting based on criteria of the data validation configuration the model id the machine learning function or the machine learning use case 13 the apparatus of claim 12 wherein the data validation configuration including one or more of at least one rule data validation associated with the model id the machine learning function or the machine learning use case at least one data statistic associated with the model id the machine learning function or the machine learning use case or at least one data property associated with the model id the machine learning function or the machine learning use case 14 the apparatus of claim 12 wherein the at least one processor is further configured to identify at least one of an inference or training output based on the machine learning that does not meet the criteria of the data validation configuration associated with the model id the machine learning function or the machine learning use case and indicate data validation failure according to the configuration the model id the machine learning function or the machine learning use case 15 the apparatus of claim 14 wherein indicating the data validation failure further indicates transition to procedure without the machine learning 16 an apparatus wireless communication including memory and at least one processor coupled to the memory and based at least in part on information stored in the memory the at least one processor is configured to provide configuration machine learning associated with model identifier id machine learning function or machine learning use case and receive report of data based on the configuration associated with the model id the machine learning function or the machine learning use case 17 the apparatus of claim 16 wherein the configuration associated with the model id the machine learning function or the machine learning use case indicates one or more of data reporting method at least one input parameter the machine learning unprocessed data to obtain model input parameters at least one measurement to obtain the model input parameters at least one data processing module timing data reporting condition the data reporting or periodicity the data reporting 18 the apparatus of claim 16 further comprising transceiver coupled to the at least one processor wherein the at least one processor is further configured to receive reports of different data based on multiple configurations each configuration associated with different model id different machine learning function or different machine learning use case 19 the apparatus of claim 16 wherein the at least one processor is further configured to provide data reporting activation the model id the machine learning function or the machine learning use case wherein the data is received in response to the activation 20 the apparatus of claim 16 wherein the at least one processor is further configured to provide data reporting deactivation the model id the machine learning function or the machine learning use case 21 the apparatus of claim 16 wherein the at least one processor is configured to receive the data in at least one of radio resource control rrc message medium access controlcontrol element macce uplink control information uci or downlink control information dci 22 the apparatus of claim 16 wherein the configuration associated with the model id includes data validation configuration including one or more of at least one rule data validation associated with the model id the machine learning function or the machine learning use case at least one data statistic associated with the model id the machine learning function or the machine learning use case or at least one data property associated with the model id the machine learning function or the machine learning use case 23 the apparatus of claim 22 wherein the at least one processor is further configured to receive data validation failure indicating at least one of an inference or training output based on the machine learning that does not meet validation criteria of the data validation configuration associated with the model id the machine learning function or the machine learning use case 24 the apparatus of claim 23 wherein indicating the data validation failure further indicates transition to procedure without the machine learning 25 an apparatus registering machine learning model including memory and at least one processor coupled to the memory and based at least in part on information stored in the memory the at least one processor is configured to register the machine learning model collection and reporting of data based on wireless communication and provide at least one of an input feature the machine learning model data processing module obtaining the input feature the machine learning model or data validation scheme the machine learning model 26 the apparatus of claim 25 wherein the data validation scheme includes one or more of at least one rule data validation associated with the machine learning model at least one first data statistic associated with training data the machine learning model at least one second data statistic associated with inference data the machine learning model at least one first data property associated with the training data the machine learning model or at least one second data property associated with the inference data the machine learning model 27 method of wireless communication including processing information with machine learning associated with model identifier id machine learning function or machine learning use case and reporting data via the wireless communication based on configuration associated with the model id the machine learning function or the machine learning use case 28 the method of claim 27 further comprising receiving the configuration identifying the model id the machine learning function or the machine learning use case wherein reporting the data includes transmitting the data based on condition timing or periodicity indicated in the configuration the model id the machine learning function or the machine learning use case 29 the method of claim 27 further including reporting different data based on multiple configurations each configuration associated with different the model id different machine learning function or different machine learning use case 30 the method of claim 27 wherein the configuration associated with the model id the machine learning function or the machine learning use case includes data validation configuration the method further comprising validating the data prior to the reporting based on criteria of the data validation configuration the model id the machine learning function or the machine learning use case lang en",
    "label": 1
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "189-406-053-095-096",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time of the at least one compound in the chromatography bed such that equation i is satisfied equation i t res t diff equation i calculating concentration czt of the at least one compound in the mobile phase at predetermined location of the chromatography device and at predetermined time t based on the adsorption isotherm claim_text 2 the method according to claim 1 wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 3 the method according to claim 1 wherein the minimum residence time of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 4 the method according to claim 1 wherein in the calculation step czt is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 5 the method according to claim 4 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 6 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 7 the method according claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 8 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 9 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 10 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 11 the method according to claim 10 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 12 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 13 the chromatography method according to claim 12 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising iii carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 15 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 11 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "130-984-086-698-559",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 computerimplemented method of training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 2 the method of claim 1 wherein the first machine learning task and the second machine learning task are different supervised learning tasks claim_text 3 the method of claim 1 wherein the first machine learning task and the second machine learning tasks are different reinforcement learning tasks claim_text 4 the method of claim 1 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the objective function claim_text 6 the method of claim 4 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text 7 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text 8 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text 9 the method of claim 1 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text 13 the method of claim 4 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response claim_text 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text 15 the method of claim 1 the method further comprising providing the trained machine learning model use in processing data after training the machine learning model on the second machine learning task claim_text 16 the method of claim 1 wherein the first and second machine learning tasks each comprise reinforcement learning task and wherein the reinforcement learning task is controlling an agent to interact with an environment to achieve goal claim_text 17 the method of claim 1 wherein the first and second machine learning tasks each comprise classification task and wherein the classification task is processing data to classify the data claim_text 18 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 19 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task lang en",
    "label": 1
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "054-176-610-461-380",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 computerimplemented method comprising storing at machine learning server computer one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable neural network type options and plurality of selectable parameter options each of which defines value machine learning parameter wherein the plurality of selectable neural network type options each defines type of machine learning system receiving at the machine learning server computer particular input dataset receiving through the graphical user interface selection of first selectable neural network type option and selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters configuring particular machine learning system based at least in part on the selection of the first selectable neural network type option replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring the particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset wherein the particular machine learning training dataset is different from the particular input dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset wherein the plurality of selectable neural network type options comprises naive bayes encoder decoder and long short term memory storing at the machine learning server computer confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value computing second output dataset replacing one or more data items in the subset of the particular output dataset with one or more corresponding items in the second output dataset claim_text 2 the computerimplemented method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters the method further comprising receiving at the machine learning server computer second input dataset receiving through the graphical user interface selection of second selectable neural network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable neural network type option using the second machine learning configuration file configuring second machine learning system computing the second output dataset using the second machine learning system and the second input dataset claim_text 3 the computerimplemented method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets claim_text 4 the computerimplemented method of claim 3 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the plurality of selectable training options claim_text 5 the computerimplemented method of claim 3 further comprising identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset computing the second output dataset using the second machine learning system and the subset of the particular input dataset claim_text 6 the computerimplemented method of claim 5 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subsets of the plurality of selectable training options corresponds to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 7 the computerimplemented method of claim 3 further comprising storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets computing the second output dataset using the second machine learning system and the subset of the particular input dataset claim_text 8 the computerimplemented method of claim 1 second machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and wherein the method further comprises displaying through the graphical user interface plurality of selectable network type options each of which defines type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system computing the second output dataset using the second machine learning system and the particular input dataset claim_text 9 the computerimplemented method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset computing the second output dataset using the second machine learning system and the particular input dataset claim_text 10 the computerimplemented method of claim 1 further comprising configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 11 the computerimplemented method of claim 1 further comprising storing at the machine learning server computer plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset claim_text 12 computer system comprising one or more processors one or more nontransitory computerreadable storage media storing one or more sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing one or more machine learning configuration files particular machine learning configuration file of the one or more machine learning configuration files comprising instructions configuring machine learning system of particular machine learning type with one or more first machine learning parameters displaying through graphical user interface plurality of selectable neural network type options and plurality of selectable parameter options each of which defines value machine learning parameter wherein the plurality of selectable neural network type options each defines type of machine learning system receiving particular input dataset receiving through the graphical user interface selection of first selectable neural network type option and selection of one or more selectable parameter options corresponding to one or more second machine learning parameters different from the one or more first machine learning parameters configuring particular machine learning system based at least in part on the selection of the first selectable neural network type option replacing in the particular machine learning configuration file the one or more first machine learning parameters with the one or more second machine learning parameters using the particular machine learning configuration file configuring the particular machine learning system using the particular machine learning system and the particular input dataset computing particular output dataset receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset wherein the particular machine learning training dataset is different from the particular input dataset in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset wherein the plurality of selectable neural network type options comprises naive bayes encoder decoder and long short term memory storing confidence score threshold value the particular output dataset comprising each of plurality of data items in the particular output dataset an output confidence score determining that subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value computing second output dataset replacing one or more data items in the subset of the particular output dataset with one or more corresponding items in the second output dataset claim_text 13 the computer system of claim 12 second machine learning configuration file of the one or more machine learning configuration files comprising one or more second sequences of instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters and which second sequences of instructions when executed by the one or more processors cause the one or more processors to execute receiving second input dataset receiving through the graphical user interface selection of second selectable neural network type option corresponding to machine learning system of the second machine learning type based at least in part on the selection of the second selectable neural network type option using the second machine learning configuration file configuring second machine learning system computing the second output dataset using the second machine learning system and the second input dataset claim_text 14 the computer system of claim 12 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets claim_text 15 the computer system of claim 14 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset receiving through the graphical user interface input categorization data identifying particular data category the particular input dataset determining that each of the plurality of machine learning training datasets corresponds to the particular data category and in response displaying the plurality of selectable training options claim_text 16 the computer system of claim 14 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute identifying subset of the particular input dataset that corresponds to the subset of the plurality of data items in the particular output dataset receiving through the graphical user interface selection of second selectable training option corresponding to second machine learning training dataset of the plurality of machine learning training datasets using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset computing the second output dataset using the second machine learning system and the subset of the particular input dataset claim_text 17 the computer system of claim 16 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category in response to determining that the subset of the plurality of data items in the particular output dataset comprise confidence scores below the confidence score threshold value displaying through the graphical user interface subset of the plurality of selectable training options wherein each of the subsets of the plurality of selectable training options corresponds to the particular data category receiving the selection of the second selectable training option from the subset of the plurality of selectable training options claim_text 18 the computer system of claim 14 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing each machine learning training dataset of the plurality of machine learning training datasets category data identifying data category the machine learning training dataset the particular machine learning training dataset corresponding to particular data category identifying one or more machine learning training datasets corresponding to the particular data category automatically selecting from the one or more machine learning training datasets second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system training the second machine learning system using the second machine learning training dataset of the plurality of machine learning training datasets computing the second output dataset using the second machine learning system and the subset of the particular input dataset claim_text 19 the computer system of claim 12 second machine learning configuration file of the one or more machine learning configuration files comprising one or more second sequences of instructions configuring machine learning system of second machine learning type with one or more third machine learning parameters which second sequences of instructions when executed by the one or more processors cause the one or more processors to execute displaying through the graphical user interface plurality of selectable network type options each of which defines type of machine learning system receiving through the graphical user interface selection of both first selectable network type option corresponding to machine learning system of the particular machine learning type and second selectable network type option corresponding to machine learning system of the second machine learning type configuring the particular machine learning system based at least in part on the selection of the first selectable network type option based at least in part on the selection of the second selectable network type option using the second machine learning configuration file configuring second machine learning system computing the second output dataset using the second machine learning system and the particular input dataset computing second output dataset claim_text 20 the computer system of claim 12 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of both particular selectable training option corresponding to particular machine learning training dataset and second selectable option corresponding to second machine learning training dataset using the particular machine learning configuration file configuring second machine learning system in response to configuring the particular machine learning system training the particular machine learning system using the particular machine learning training dataset in response to configuring the second machine learning system training the second machine learning system using the second machine learning training dataset computing the second output dataset using the second machine learning system and the particular input dataset claim_text 21 the computer system of claim 12 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute configuring plurality of test machine learning systems of the particular machine learning type each of which comprising different machine learning parameters using test input dataset and the plurality of test machine learning systems computing plurality of test output datasets determining that an output of particular test machine learning system of the plurality of test machine learning systems is more accurate than outputs of each other test machine learning system of the plurality of test machine learning systems the particular test machine learning system comprising the one or more first machine learning parameters in response to determining that the output of the particular test machine learning system is more accurate than each other test machine learning system storing the particular machine learning configuration file with the one or more first machine learning parameters claim_text 22 the computer system of claim 12 further comprising sequences of instructions which when executed by the one or more processors cause the one or more processors to execute storing plurality of machine learning training datasets displaying through the graphical user interface plurality of selectable training options each of which corresponds to machine learning training dataset of the plurality of machine learning training datasets receiving through the graphical user interface selection of particular selectable training option corresponding to particular machine learning training dataset in response to computing the particular output dataset updating the particular machine learning training dataset using the particular input dataset and the particular output dataset lang en",
    "label": 1
  },
  {
    "id1": "018-288-975-416-794",
    "id2": "022-386-433-016-642",
    "claims1": "claims claim_text 1 system building learning machine comprising reference learning machine target learning machine being built component analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of components in the reference learning machine and the target learning machine and return set of output values each component on the list of components and component tuner module configured to modify different components in the target learning machine based on the set of output values and component mapping thereby resulting in tuned learning machine claim_text 2 the system of claim 1 further comprising feedback loop feeding back the tuned learning machine new target learning machine in an iterative manner claim_text 3 the system of claim 1 wherein the target learning machine is new component to be inserted into the reference learning machine claim_text 4 the system of claim 1 wherein the target learning machine replaces an existing set of components in the reference learning machine claim_text 5 the system of claim 1 wherein the reference learning machine and the target learning machine are graphbased learning machines claim_text 6 the system of claim 5 wherein the component analyzer module is node analyzer module claim_text 7 the system of claim 5 wherein the component tuner module is an interconnect tuner module claim_text 8 the system of claim 5 wherein the component mapping is mapping between nodes from the reference learning machine and nodes from the target learning machine claim_text 9 the system of claim 7 wherein the interconnect tuner module updates interconnect weights in the target learning machine claim_text 10 the system of claim 1 wherein the tuned learning machine includes components updated by the component tuner module claim_text 11 system building learning machine comprising an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 12 the system of claim 11 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture claim_text 13 the system of claim 12 further comprising feedback loop feeding back the new graphbased learning machine new initial graphbased learning machine in an iterative manner claim_text 14 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to each machine component in the initial graphbased learning machine compute an average of set of computed output values of each machine component each data point in the set of data points to produce combined output value of each machine component corresponding to each data point in the set of data points and compute machine component importance score each machine component by averaging final combined output values of each machine component corresponding to all data points in the set of data points from the set of data and dividing the average by normalization value claim_text 15 the system of claim 11 wherein the machine analyzer is further configured to feed each data point in set of data points from the set of data into the initial graphbased learning machine predetermined set of iterations randomly select groups of nodes and interconnects in the initial graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the initial graphbased learning machine average set of computed output values each data point in the set of data points to produce final combined output value corresponding to each data point and compute full machine score each component in the initial graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data claim_text 16 the system of claim 15 wherein the machine analyzer is further configured to feed each data point in the set of data points from the set of data into reduced graphbased learning machine with at least some machine components excluded predetermined set of iterations randomly select groups of nodes and interconnects in the reduced graphbased learning machine with each data point in the set of data points to compute an output value corresponding to one of the nodes of the reduced graphbased learning machine compute an average of set of computed output values each data point to produce final combined output value corresponding to each data point and compute reduced machine score each component in the reduced graphbased learning machine by averaging final combined output value corresponding to all data points in the set of data points in the set of data claim_text 17 the system of claim 11 wherein the machine architecture builder is further configured to control the size of the new graphbased learning machine architecture based on the set of machine component importance scores and the set of machine factors claim_text 18 the system of claim 17 wherein the machine architecture builder controls the size of the new graphbased learning machine architecture by determining whether each node will exist in the new graphbased learning machine architecture claim_text 19 system building learning machine comprising reference learning machine target learning machine being built node analyzer module configured to analyze inputs from the reference learning machine the target learning machine set of test signals and list of nodes in the reference learning machine and the target learning machine and return set of output values each component on the list of nodes an interconnect tuner module configured to modify different components in the target learning machine based on the set of output values and node mapping thereby resulting in tuned learning machine an initial graphbased learning machine machine analyzer configured to analyze components of the initial graphbased learning machine based on set of data to generate set of machine component importance scores and machine architecture builder configured to build graphbased learning machine architecture based on the set of machine component importance scores and set of machine factors claim_text 20 the system of claim 19 wherein new graphbased learning machine is built wherein the architecture of the new graphbased learning machine is the same the graphbased learning machine architecture lang en",
    "claims2": "claims claim_text 1 method in data processing system comprising at least one processor and at least one memory the at least one memory comprising instructions executed by the at least one processor to cause the at least one processor to implement machine learning framework wherein the machine learning framework operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 2 the method of claim 1 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 3 the method of claim 1 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 4 the method of claim 1 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 5 the method of claim 4 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 6 the method of claim 1 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 7 the method of claim 1 wherein the machine learning framework further operates to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 8 computer program product comprising computer readable storage medium having computer readable program stored therein wherein the computer readable program when executed on computing device causes the computing device to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 9 the computer program product of claim 8 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 10 the computer program product of claim 8 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 11 the computer program product of claim 8 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 12 the computer program product of claim 11 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 13 the computer program product of claim 8 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 14 the computer program product of claim 8 wherein the machine learning framework is further configured to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 15 an apparatus comprising processor and memory coupled to the processor wherein the memory comprises instructions which when executed by the processor cause the processor to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operate to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 16 the apparatus of claim 15 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in trained machine learning model index data storage trained machine learning model metadata model each trained machine learning algorithm in the plurality of trained machine learning models claim_text 17 the apparatus of claim 15 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository lang en",
    "label": 1
  },
  {
    "id1": "044-438-135-761-652",
    "id2": "140-416-112-830-70X",
    "claims1": "claims claim_text 1 canceled claim_text 2 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 3 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation values of the first node each training data item associated to the cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 4 the machine learning system of claim 3 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 5 the machine learning system of claim 4 wherein the vector norm is the l2 norm claim_text 6 the machine learning system of claim 3 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 7 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 8 the machine learning system of claim 7 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 9 the machine learning system of claim 3 wherein the regularization term comprises relaxation strength hyperparameter claim_text 10 the machine learning computer system of claim 9 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 11 the machine learning system of claim 2 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 12 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes including first node and iteratively training the first neural network comprises in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 13 the computerimplemented machine learning method of claim 12 wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 14 the computerimplemented machine learning method of claim 13 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 15 the computerimplemented machine learning method of claim 14 wherein the vector norm is the l2 norm claim_text 16 the computerimplemented machine learning method of claim 13 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 17 the computerimplemented machine learning method of claim 12 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 18 the computerimplemented machine learning method of claim 13 wherein the regularization term comprises relaxation strength hyperparameter claim_text 19 the computerimplemented machine learning method of claim 18 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 20 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network compute activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 21 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 22 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 23 the machine learning system of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 24 the machine learning computer system of claim 23 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 25 the machine learning system of claim 20 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 26 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and iteratively training the first neural network comprises by in forward propagations through the first neural network computing activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 27 the computerimplemented machine learning method of claim 26 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 28 the computerimplemented machine learning method of claim 26 wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 29 the computerimplemented machine learning method of claim 28 wherein the regularization term comprises relaxation strength hyperparameter claim_text 30 the computerimplemented machine learning method of claim 29 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train an ensemble with set of training data items wherein the ensemble comprises multiple machine learning ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to train the ensemble in first iteration of the training by selectively directing with preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 2 the machine learning computer system of claim 1 wherein the preliminary classifier is not trained with the ensemble claim_text 3 the machine learning computer system of claim 2 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 4 the machine learning computer system of claim 2 wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to further train the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 5 the machine learning computer system of claim 1 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 6 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 7 the machine learning computer system of claim 6 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 8 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 9 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores machine learning ml system with set of training data items wherein the ml system comprises an ensemble that comprises multiple ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and preliminary classifier that is trained through machine learning and iteratively training the ml system comprises in first iteration of the training selectively directing by the preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members and training the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 10 the computerimplemented machine learning method of claim 9 wherein the preliminary classifier is not trained with the ensemble claim_text 11 the computerimplemented machine learning method of claim 10 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 12 the computerimplemented machine learning method of claim 10 further comprising training by the one or more programmed processor cores the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 13 the computerimplemented machine learning method of claim 9 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 14 the computerimplemented machine learning method of claim 9 wherein training the preliminary classifier comprises training the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 15 the computerimplemented machine learning method of claim 9 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 16 the computerimplemented machine learning method of claim 14 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item lang en",
    "label": 1
  },
  {
    "id1": "126-559-373-005-266",
    "id2": "013-300-859-082-341",
    "claims1": "claims claim_text 1 method of training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 2 the method of claim 1 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 3 the method of claim 2 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 4 the method of claim 3 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 5 the method of claim 1 wherein the subject node comprises feature node claim_text 6 the method of claim 5 further comprising inserting by the computer system the feature node into second neural network claim_text 7 method of training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the method comprising backpropagating by first computer system partial derivatives of cost function through the first neural network wherein backpropagating the partial derivatives comprises softtying by the first computer system an activation value of subject node of the first neural network to an activation value of second node claim_text 8 the method of claim 7 wherein softtying the activation values comprises including by the first computer system regularization term in the cost function the subject node and the second node common data training example claim_text 9 the method of claim 8 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 10 the method of claim 9 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 11 the method of claim 10 further comprising determining by learning coach system the relaxation strength parameter claim_text 12 the method of claim 11 wherein determining the relaxation strength parameter comprises changing by the learning coach the relaxation strength parameter during the training claim_text 13 the method of claim 8 wherein the second node is part of the first neural network claim_text 14 the method of claim 8 wherein the second node is part of second neural network that is different from the first neural network claim_text 15 the method of claim 8 wherein softtying comprises softtying by the first computer system the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 16 the method of claim 15 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 17 the method of claim 16 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 18 the method of claim 17 wherein the second network comprises classifier and the third network comprises feature detector claim_text 19 computer system training first neural network wherein the first neural network comprises plurality of layers each layer comprises at least one node training data examples training the first neural network are assigned to one of plurality of training data example clusters and the plurality of training data example clusters comprises first training data example cluster the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying activation values of subject node of the first neural network across all training data examples assigned to the first training data example cluster claim_text 20 the computer system of claim 19 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node during the backpropagation of the partial derivatives of the cost function claim_text 21 the computer system of claim 20 wherein the regularization term is based on difference between the activation value of the subject node first training data example that is assigned to the first training data example cluster and an average activation value the subject node across all training data examples assigned to the first training data example cluster claim_text 22 the computer system of claim 21 wherein the regularization term is based on product of relaxation strength parameter and the difference between the activation value of the subject node the first training data example and the average activation value the subject node across all training data examples assigned to the first cluster claim_text 23 the computer system of claim 19 wherein the subject node comprises feature node claim_text 24 the computer system of claim 23 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to insert the feature node into second neural network claim_text 25 computer system training first neural network wherein the first neural network comprises plurality of layers and each layer comprises at least one node the computer system comprising processor core and memory in communication with the processor core wherein the memory stores computer instructions that when executed by the processor core cause the processor to backpropagate partial derivatives of cost function through the first neural network by softtying an activation value of subject node of the first neural network to an activation value of second node claim_text 26 the computer system of claim 25 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie the activation values by including regularization term in the cost function the subject node and the second node common data training example claim_text 27 the computer system of claim 26 wherein the regularization term is based on mean activation value of the subject node and the second node the common data training example claim_text 28 the computer system of claim 27 wherein the regularization term is based on product of relaxation strength parameter and the mean activation value claim_text 29 the computer system of claim 28 further comprising learning coach system determining the relaxation strength parameter claim_text 30 the computer system of claim 29 wherein the learning coach is changing the relaxation strength parameter during the training claim_text 31 the computer system of claim 26 wherein the second node is part of the first neural network claim_text 32 the computer system of claim 26 wherein the second node is part of second neural network that is different from the first neural network claim_text 33 the computer system of claim 26 wherein the memory stores further computer instructions that when executed by the processor core cause the processor core to softtie by softtying the activation value of the subject node of the first neural network to activation values of plurality of softtied nodes wherein the plurality of softtied nodes comprises the second node claim_text 34 the computer system of claim 33 wherein the regularization term is based on mean of the activation values of the subject node and the plurality of softtied nodes the common data training example claim_text 35 the computer system of claim 34 wherein the second node is part of second neural network that is different from the first neural network and the plurality of softtied nodes comprises third node that is part of third neural network claim_text 36 the computer system of claim 35 wherein the second network comprises classifier and the third network comprises feature detector lang en",
    "claims2": "claims claim_text 1 network slice configuration method comprising sending by first network element network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 2 the network slice configuration method of claim 1 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 3 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 4 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is sent by the first network element to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 5 the network slice configuration method of claim 1 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 6 network slice configuration method comprising receiving by second network element network slice configuration information of first network element and storing by the second network element network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 7 the network slice configuration method of claim 6 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 8 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 9 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is received by the second network element from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 10 the network slice configuration method of claim 6 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 11 first network element comprising sending module which is configured to send network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 12 the first network element of claim 11 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 13 the first network element of claim 11 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 14 the first network element of claim 11 wherein the sending module is further configured to send information of network slice supported by ta to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 15 the first network element of claim 11 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 16 second network element comprising receiving module which is configured to receive slice configuration information of first network element and storing module which is configured to storing network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 17 the second network element of claim 16 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 18 the second network element of claim 16 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 19 the second network element of claim 16 wherein the receiving module is further configured to receive information of network slice supported by ta from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 20 the second network element of claim 16 wherein the network slice information comprises single network slice selection assistance information snssai lang en",
    "label": 0
  },
  {
    "id1": "031-656-825-993-606",
    "id2": "057-360-526-753-072",
    "claims1": "claims claim_text 1 method of introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the method comprising pretraining by computer system first set of one or more additional nodes wherein pretraining the first set of one or more additional nodes is separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes adding by the computer system the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network training by the computer system the first deep neural network wherein the training comprises in forward propagation phase forward propagating outputs from the first layer to the second layer and to the first set of one or more additional nodes forward propagating outputs from the first set of one or more additional nodes to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and forward propagating outputs from the second layer to the third layer and in backpropagation phase backpropagating partial derivatives of an error cost function from the third layer to the second layer backpropagating partial derivatives of the error cost function from the second layer to the first layer backpropagating partial derivatives of the error cost function from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 2 the method of claim 1 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and pretraining the first set of one or more additional nodes comprises training the first set of one or more additional nodes through training the second deep neural network claim_text 3 the method of claim 2 wherein the second deep neural network comprises classifier with finite set of one or more classification categories claim_text 4 the method of claim 3 each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories claim_text 5 the method of claim 4 wherein each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 6 the method of claim 2 wherein the second deep neural network comprises pattern recognition model claim_text 7 the method of claim 6 further comprising encoding latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 8 the method of claim 1 wherein training the first deep neural network comprises controlling dropout rate the first set of one or more additional nodes claim_text 9 the method of claim 8 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes with learning coach wherein the learning coach is trained through machine learning to learn an optimum dropout rate the first set of one or more additional nodes claim_text 10 the method of claim 9 wherein controlling the dropout rate comprises controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network claim_text 11 the method of claim 10 wherein the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 12 the method of claim 11 further comprising after training the first deep neural network removing the first set of one or more additional nodes from the first deep neural network claim_text 13 the method of claim 11 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 14 the method of claim 13 wherein regularizing the activation values of the nodes of the second set of one or more additional nodes comprising controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 15 the method of claim 14 wherein the learning coach has different learning objective than the first deep neural network claim_text 16 the method of claim 1 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one pretraining comprises pretraining by the computer system second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes adding comprises adding by the computer system the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and training the first deep neural network comprises regularizing by the computer system activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 17 computer system introducing explicit knowledge to first deep neural network wherein the first deep neural network comprises plurality of layers including an input layer an output layer one or more inner layers between the input layer and the output layer such that the first network comprises at least first layer second layer and third layer the second layer is one of the one or more inner layers of the first deep neural network the second layer is between the input layer and the third layer the second layer is between the first layer and the output layer the second layer is between the first layer and third layer each of the plurality of layers including the first second and third layers comprises one or more nodes each of the one or more nodes of the second layer receive an input from the one or more nodes of the first layer and each of the one or more nodes of the second layer output an output to the one or more nodes of the third layer the computer system comprising one or more processor cores and memory in communication with the one or more processor cores wherein the memory stores computer instructions that when executed by the one or more processor cores cause the one or more processor cores to pretrain first set of one or more additional nodes separate from training of the first deep neural network and wherein pretraining the first set of one or more additional nodes comprises determining activation values the one or more additional nodes of the first set particular set of data input values to the first set of one or more additional nodes after pretraining the first set of one or more additional nodes add the first set of one or more additional nodes to the first deep neural network such that the first set of one or more additional nodes have outputs connected inputs of the second layer and the first set of one or more additional nodes have inputs connected to outputs of the first layer and after adding the first set of one or more additional nodes to the first deep neural network train the first deep neural network such that in forward propagation phase outputs from the first layer are forward propagated to the second layer and to the first set of one or more additional nodes outputs from the first set of one or more additional nodes are forward propagated to the second layer wherein the outputs from the first set of one or more additional nodes are based on the pretraining of the first set of one or more additional nodes the particular set of data input values and outputs from the second layer are forward propagated to the third layer and in backpropagation phase partial derivatives of an error cost function are backpropagated from the third layer to the second layer partial derivatives of the error cost function are backpropagated from the second layer to the first layer partial derivatives of the error cost function are backpropagated from the first set of one or more additional nodes to the first layer and without backpropagating partial derivatives of the error cost function from the second layer to the first set of one or more additional nodes claim_text 18 the computer system of claim 17 wherein prior to adding the first set of one or more additional the first set of one or more additional nodes are part of second deep neural network wherein the second deep neural network is separate from the first deep neural network and the memory stores computer instructions that when executed by the one or more processor cores cause the one on more processor cores to pretrain the first set of one or more additional nodes by training the first set of one or more additional nodes through training the second deep neural network claim_text 19 the computer system of claim 18 wherein the second deep neural network comprises classifier with finite set of one or more classification categories each node of the first set of one or more additional nodes has an associated classification category of the finite set of one or more classification categories and each node of the first set of one or more additional nodes the activation values the node are classification scores the associated classification category the node the particular set of data input values claim_text 20 the computer system of claim 18 wherein the second deep neural network comprises pattern recognition model and the memory further stores instructions that when executed by the one or more processors cause the one or more processors to encode latent variables from the pattern recognition model the activation values the first set of one or more additional nodes claim_text 21 the computer system of claim 17 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train the first deep neural network by controlling dropout rate the first set of one or more additional nodes claim_text 22 the computer system of claim 21 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to train through machine learning learning coach to learn an optimum dropout rate the first set of one or more additional nodes and control the dropout rate by controlling the dropout rate the first set of one or more additional nodes with the learning coach claim_text 23 the computer system of claim 22 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to control the dropout rate by controlling the dropout rate the first set of one or more additional nodes to realize an objective the first deep neural network and the objective comprises the first deep neural network to learn explicit knowledge from the first set of or more additional nodes claim_text 24 the computer system of claim 22 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes claim_text 25 the computer system of claim 24 wherein the memory further stores instructions that when executed by the one or more processors cause the one or more processors to regularize the activation values of the nodes of the second set of one or more additional nodes by controlling by the learning coach regularization of the nodes of the second set of one or more additional nodes claim_text 26 the computer system of claim 17 wherein the first set of one or more additional nodes comprises n nodes where n is greater than or equal to one the memory further stores instructions that when executed by the one or more processors cause the one or more processors to pretrain second set of one or more additional nodes wherein the second set comprises n nodes such that each node of the second set of additional nodes has corresponding node of the first set of additional nodes add the second set of one or more additional nodes to the first deep neural network such that the second set of one or more additional nodes have outputs connected inputs of the second layer and the second set of one or more additional nodes have inputs connected to outputs of the first layer and in the training the first deep neural network regularize activation values of the nodes of the second set of one or more additional nodes to agree with the activation values of the corresponding node of the first set of additional nodes lang en",
    "claims2": "claims claim_text 1 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer setting detected number of active connections in counters provided each output port and controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections wherein said controlling step further includes outputting cell addressed to an output port corresponding to counter with said count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to said output port and subtracting value of said counter corresponding to an output port that has output cells at every output of cell claim_text 2 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of cell addressed to said output port from an input multiplex buffer claim_text 3 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of setting detected number of active connections in counters provided each output port and sequentially outputting cell addressed to an output port corresponding to counter with count value not set to 0 among said counters and subtracting value of counter corresponding to an output port that has output cell at every output of said cell when outputting cells accumulated in an output separation buffer to an output port claim_text 4 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output from said output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of the cells addressed to said output from an input multiplex buffer claim_text 5 an asynchronous transfer mode hereinafter referred to atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively input multiplex buffer control means controlling said input multiplex buffer so that output of cell addressed to certain output port indicated by first back pressure signal is inhibited and output separation buffer control means monitoring an accumulation amount of cells in said output separation buffer at every output port to which said cell is addressed and outputting said first back pressure signal to said input multiplex buffer control means to inhibit output of cell addressed to an output port having said accumulation amount equal to or more than predetermined threshold value detection means detecting number of active connections existing between each output port and each input multiplex buffer and said output separation buffer control means further comprises output control means controlling number of cells addressed to said output port output from an output separation buffer so that said number of cells are in proportional to number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said output control means further comprises counter each provided an output port means setting each number of active connections detected by said detection by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting counter value corresponding to an output port which has output cells at every output of cell claim_text 6 the atm switch of claim 5 wherein said output control means comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to said output separation buffer when said cells output from said output separation buffer are equal to or larger than cells input thereto and means outputting said first back pressure signal inhibiting output of cells addressed to said detected output port to said input multiplex buffer control means claim_text 7 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises counter provided each output port means setting each number of active connections detected by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting value of said counter corresponding to an output port that has output cell at every output of cell claim_text 8 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to an output separation buffer when cells output separation buffer when cells output from said output separation buffer is equal to or more than cells input thereto and means outputting said first back pressure signal inhibiting output of cell addressed to said detected output port to said input multiplex buffer control means lang en",
    "label": 0
  },
  {
    "id1": "183-449-275-297-090",
    "id2": "190-059-083-058-239",
    "claims1": "claims claim_text claims 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "claims2": "claims claim_text 1 machine learning ml computer system training student ml system iteratively through machine learning on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of training of the student ml system the learned parameters the first node of the first inner layer are updated and the ml computer system comprises learning coach ml system that is in communication with the student ml system wherein the learning coach ml system comprises neural network that comprises an input layer and an output layer the learning coach ml system has been trained through machine learning to determine and implement an enhancement to the student ml system based on input to the learning coach ml system from the student ml system wherein the input from the student ml system is input to the input layer of the learning coach ml system the input to the learning coach ml system from the student ml system comprises observations about an internal state of the student ml system during training of the student ml system and the observations about the internal state of the student ml system comprise values related to the learned parameters the first node on the first inner layer of the student ml system and the activation values the first node on the first inner layer of the student ml system such that the learning coach ml system determines and implements the enhancement to the student ml system based on the input from the student ml system claim_text 2 the ml computer system of claim 1 wherein the learning coach ml system comprises pattern recognition system that recognizes different patterns than the student ml system claim_text 3 the ml computer system of claim 1 wherein the learning coach ml system has different objective than the student ml system claim_text 4 the ml computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 5 the ml system of claim 4 wherein the revised hyperparameter comprises hyperparameter selected from the group consisting of mini batch size the student ml system learning rate the student ml system regularization parameter the student ml system and momentum parameter the student ml system claim_text 6 the ml computer system of claim 1 wherein the enhancement comprises structural change to the student ml system claim_text 7 the ml computer system of claim 6 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 8 the ml computer system of claim 6 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 9 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 10 the ml computer system of claim 9 wherein the selected layer of the student ml system comprises plurality of existing nodes prior to the implementation of the enhancement and after the enhancement the existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the student ml system below the selected layer and activations of the virtual nodes are controlled by the learning coach ml system claim_text 11 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and learning coach ml system controls regularization to the second set of nodes so differences between activation values the second set of nodes and activation values the first set of nodes is less than threshold value to control dropout rate of the nodes in the first and second sets claim_text 12 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing connection weights the one or more additional nodes of the student ml system claim_text 13 the ml computer system of claim 1 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 14 the ml computer system of claim 1 wherein the ml system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activation values the student ml system are stored in the high speed memory so that the student ml system can be run when the student ml system is active and the connection weights and activation values the student ml system are stored in the secondary storage when the student ml system is not active claim_text 15 the ml system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 16 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system regression claim_text 17 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system classification task claim_text 18 machinelearning ml training method comprising initially training learning coach ml system to determine and implement an enhancement to ml system wherein the learning coach ml system comprises an input layer and an output layer training iteratively through machine learning student ml system on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and the initial training the student ml system comprises following iterations of the training of the student ml system updating the learned parameters the first node of the first inner layer and training the student ml system comprises receiving by the learning coach ml system from the student ml system values related to the learned parameters and the activation values of the first node of the first inner layer of the student ml system and determining and implementing by the learning coach ml system the enhancement to the student ml system based on the values related to the learned parameters and the activation values of the first node of the inner layer of the student ml system to improve performance of the student ml system claim_text 19 the ml training method of claim 18 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 20 the ml training method of claim 18 wherein the enhancement comprises structural change to the student ml system claim_text 21 the ml training method of claim 20 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 22 the ml training method of claim 20 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 23 the ml training method of claim 22 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 24 the ml training method of claim 18 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 25 the ml training method of claim 18 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 26 the ml computer system of claim 1 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and during training of the student ml system activation values each of the multiple feature node are softtied training data examples within the cluster claim_text 27 the ml computer system of claim 26 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 28 the ml computer system of claim 27 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 29 the ml computer system of claim 27 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 30 the ml computer system of claim 29 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 31 the ml computer system of claim 30 wherein the ml learning coach is configured to set the relaxation strength hyperparameters the feature nodes claim_text 32 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to an activation value of node in separate ml system claim_text 33 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to activation values of multiple nodes in separate ml system claim_text 34 the ml computer system of claim 33 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 35 the ml computer system of claim 34 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 36 the ml computer system of claim 35 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 37 the ml computer system of claim 36 wherein the learning coach ml system is configured to set the relaxation strength hyperparameter the first node claim_text 38 the ml computer system of claim 1 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the ml computer system further comprises data switching network that selectively directs training data examples the student ml system to one or more and less than all of the multiple ml ensemble members claim_text 39 the ml computer system of claim 38 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 40 the ml computer system of claim 38 further comprising classifier classifying the training data examples the student ml system and wherein the classifier controls the data switching network training data example based on classification by the classifier the training data example claim_text 41 the ml training method of claim 18 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and training the student ml system further comprises softtying activation values each of the multiple feature nodes training data examples within the cluster claim_text 42 the ml training method of claim 41 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 43 the ml training method of claim 42 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 44 the ml training method of claim 42 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 45 the ml training method of claim 44 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 46 the ml training method of claim 45 further comprising setting by the ml learning coach the relaxation strength hyperparameters the feature nodes claim_text 47 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to an activation value of node in separate ml system claim_text 48 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to activation values of multiple nodes in separate ml system claim_text 49 the ml training method of claim 48 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 50 the ml training method of claim 49 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 51 the ml training method of claim 50 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 52 the ml training method of claim 51 further comprising setting by the learning coach ml system the relaxation strength hyperparameter the first node claim_text 53 the ml training method of claim 18 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the method further comprises selectively directing by data switching network training data examples the student ml system to one or more but less than all of the multiple ml ensemble members claim_text 54 the ml training method of claim 53 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 55 the ml training method of claim 53 further comprising classifying by classifier the training data examples the student ml system and controlling by the classifier the data switching network training data example based on classification by the classifier the training data example lang en",
    "label": 1
  },
  {
    "id1": "020-796-323-879-358",
    "id2": "151-548-458-500-410",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyzing the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposing and displaying on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui according to one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyze the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decompose and display on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui according to one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof an executable portion that analyzes the extracted metadata wherein the extracted metadata includes provenance metadata and an executable portion that in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposes and displays on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui according to one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 47 canceled claim_text 48 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by broker component maintaining provider register containing information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the method comprising receiving request desired machine learning model from machine learning model consumer determining based on the information contained in the provider register machine learning model among the machine learning models provided by the machine learning model providers that matches the desired machine learning model and sending response to the machine learning model consumer providing information associated with the determined machine learning model claim_text 49 the method of claim 48 wherein the request includes information characterizing the desired machine learning model and wherein determining the machine learning model that matches the desired machine learning model includes matching the information characterizing the desired machine learning model with the information contained in the provider register claim_text 50 the method of claim 49 wherein the information characterizing the desired machine learning model includes at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 51 the method of claim 48 wherein the information contained in the provider register includes each machine learning model provided by one of the plurality of machine learning model providers at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 52 the method of claim 48 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes notification on the availability of the determined machine learning model and wherein the response including the notification is sent to the machine learning model consumer conditionally when the determined machine learning model matches the desired machine learning model better than machine learning model previously sent to the machine learning model consumer matching the desired machine learning model claim_text 53 the method of claim 52 further comprising receiving upon receiving the request the desired machine learning model registration message from machine learning model provider to register its machine learning models with the provider register wherein determining the machine learning model that matches the desired machine learning model includes checking the machine learning models registered by the registration message on match with the desired machine learning model if match with the desired machine learning model is determined sending request the determined machine learning model to the machine learning model provider providing the determined machine learning model and receiving the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the request claim_text 54 the method of claim 48 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes an output value output by the determined machine learning model in response to the one or more input values claim_text 55 the method of claim 54 further comprising sending the one or more input values to the machine learning model provider providing the determined machine learning model input to the desired machine learning model and receiving an output value output by the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the one or more input values claim_text 56 the method of claim 48 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes access information to the machine learning model provider providing the determined machine learning model claim_text 57 the method of claim 48 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 58 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model consumer and comprising sending request desired machine learning model to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers and receiving response from the broker component providing information associated with machine learning model determined by the broker component among the machine learning models provided by the machine learning model providers matching the desired machine learning model claim_text 59 the method of claim 58 wherein the request includes information characterizing the desired machine learning model the information characterizing the desired machine learning model including at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 60 the method of claim 58 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response from the broker component includes notification on the availability of the determined machine learning model and optionally the determined machine learning model claim_text 61 the method of claim 58 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model claim_text 62 the method of claim 58 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response from the broker component includes access information to the machine learning model provider providing the determined machine learning model claim_text 63 the method of claim 62 further comprising sending to the machine learning model provider providing the determined machine learning model using the access information request to use the desired machine learning model wherein the request includes one or more input values to be passed input to the desired machine learning model and receiving from the machine learning model provider providing the determined machine learning model an output value output by the determined machine learning model in response to the one or more input values claim_text 64 the method of claim 58 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 65 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model provider of the plurality of machine learning model providers and comprising sending to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the provider register enabling the broker component to determine machine learning model among the machine learning models provided by the plurality of machine learning model providers that matches desired machine learning model requested by machine learning model consumer registration message to register machine learning models provided by the machine learning model provider with the provider register of the broker component claim_text 66 the method of claim 65 wherein the registration message includes each machine learning model provided by the machine learning model provider at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 67 the method of claim 65 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system lang en",
    "label": 1
  },
  {
    "id1": "160-348-204-331-805",
    "id2": "161-152-626-124-189",
    "claims1": "claims claim_text 1 34 canceled claim_text 35 computerimplemented method executing machinelearning model performed in first network node the method comprising developing first machinelearning model based on first set of data and using machinelearning algorithm communicating to second network node the first machinelearning model receiving from the second network node information about difference between the first machinelearning model and second machinelearning model receiving request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtaining information indicative of an execution policy and depending on the obtained information indicative of the execution policy either executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result claim_text 36 the method according to claim 35 wherein partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model at the second network node to obtain result comprises partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model based on the information about difference between the first machinelearning model and the second machinelearning model claim_text 37 the method according to claim 35 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises any one or more of information that the second machinelearning model is different from the first machinelearning model information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model information identifying difference between the first machinelearning model and the second machinelearning model or the second machinelearning model claim_text 38 the method according to claim 35 wherein the information indicative of an execution policy is obtained from policy node or is obtained from memory in the first network node claim_text 39 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating execution of the machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model said information further comprises information indicating that at least part of said machinelearning model should be executed in an enclaved mode claim_text 40 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating partial execution of the first machinelearning model and partial execution of the second machinelearning model said information further comprising information indicating that at least one component of said first machinelearning model or of said second machinelearning model should be executed in an enclaved mode claim_text 41 the method according to claim 35 wherein the first machinelearning model and the second machinelearning model are representable computational graphs claim_text 42 the method according to claim 41 wherein the computational graphs are directed acyclic graphs claim_text 43 the method according to claim 41 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 44 the method according to claim 35 wherein the step of executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result comprises at least partially executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model in an enclaved memory segment claim_text 45 the method according to claim 35 wherein the step of partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result comprises one or more of executing at least one component of the first machinelearning model in an enclaved memory segment causing the second network node to partially execute the second machinelearning model in an enclaved memory segment partially executing the first machinelearning model to form first partial result communicating to the second network node the first partial result causing the partial execution of the second machinelearning model using the first partial result at the second network node such that second partial result is formed at the second network node receiving from the second network node the second partial result or partially executing the first machinelearning model using the second partial result to form final result claim_text 46 first network node executing machinelearning model the first network node comprising an interface configured allowing communication with other network nodes and processing circuitry operatively associated with the interface and configured to develop first machinelearning model based on first set of data and using machinelearning algorithm communicate the first machinelearning model to second network node receive from the second network node information about difference between the first machinelearning model and second machinelearning model receive request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtain information indicative of an execution policy and depending on the obtained information indicative of an execution policy either execute machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially execute the first machinelearning model and cause the second network node to partially execute the second machinelearning model to obtain result claim_text 47 method executing machinelearning model performed in second network node the method comprising receiving from first network node first machinelearning model developing second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicating to the first network node information about difference between the first machinelearning model and the second machinelearning model receiving from the first network node partial result partially executing the second machinelearning model using the first partial result to form second partial result and communicating to the first network node the second partial result claim_text 48 the method according to claim 47 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises information that the second machinelearning model is different from the first machinelearning model andor information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model andor information identifying difference between the first machinelearning model and the second machinelearning model andor the second machinelearning model claim_text 49 the method according to claim 47 wherein the machinelearning model and the second machinelearning model are representable computational graphs claim_text 50 the method according to claim 49 wherein the computational graphs are directed acyclic graphs claim_text 51 the method according to claim 49 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 52 the method according to claim 47 wherein the step of partially executing the second machinelearning model using the first partial result to form second partial result comprises executing at least one component of the second machinelearning model at the second node in an enclaved memory segment claim_text 53 network node executing machinelearning model the network node referred to second network node and comprising an interface configured allowing communication with other network nodes processing circuitry operatively associated with the interface and configured to receive from first network node first machinelearning model develop second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicate to the first network node information about difference between the first machinelearning model and the second machinelearning model receive from the first network node partial result partially execute the second machinelearning model using the first partial result to form second partial result and communicate to the first network node the second partial result lang en",
    "claims2": "claims claim_text claims what is claimed is 1 machine learning computer system comprising first student machine learning system that using machine learning automatically learns from and make predictions on input source data and first learning coach machine learning system that is in communication with the first student machine learning system wherein input to the first learning coach machine learning system comprises data about an internal state of the first student machine learning system and the learning coach machine learning system using machine learning automatically learns and implements an enhancement to the first student machine learning system based on the data about the internal state of the first student machine learning system to improve operation of the first student machine learning system 2 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes different patterns than the first student machine learning system 3 the machine learning computer system of claim 1 wherein the first student machine learning system has different objective than the first student machine learning system 4 the machine learning computer system of claim 1 wherein the first student machine learning system comprises deep neural network 5 the machine learning computer system of claim 4 wherein the first learning coach machine learning system comprises machine learning architecture that is not deep neural network 6 the machine learning computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning system that improve learning by the first student machine learning system 7 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise minibatch size the first student machine learning system 8 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise learning rate the first student machine learning system 9 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise regularization parameter the first student machine learning system 10 the machine learning computer system of claim 6 wherein the one or more revised hyperparameters comprise momentum parameter the first student machine learning system 11 the machine learning computer system of claim 1 wherein the enhancement comprises structural change the first student machine learning system 12 the machine learning computer system of claim 11 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning system 13 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the first student machine learning system 14 the machine learning computer system of claim 13 wherein existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the network of the first student machine learning system below the selected layer and activations of the virtual nodes are controlled by the first learning coach machine learning system 15 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and first learning coach machine learning system controls regularization to the second set of nodes so that activations the second set of nodes tend to agree with the first set of nodes to control dropout rate of the nodes in the first and second sets 16 the machine learning computer system of claim 15 wherein performance objective of the first learning coach machine learning system in controlling the regularization is different from performance objective of the first student machine learning system 17 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing connection weights the additional nodes of the network of the first student machine learning system 18 the machine learning computer system of claim 11 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning system 19 the machine learning computer system of claim 1 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control the learning of the first student machine learning system 20 the machine learning computer system of claim 1 wherein the machine learning system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activations the first student machine learning system are stored in the high speed memory so that the first student machine learning system can be run when the first student machine learning system is active and the connection weights and activations the first student machine learning system are stored in the secondary storage when the first student machine learning system is not active 21 the machine learning system of claim 1 wherein the first student machine learning system comprises graphics processing unit that comprises multiple processing cores on single integrated circuit 22 the machine learning system of claim 21 wherein the first learning coach machine learning system comprises graphic processing unit 23 the machine learning system of claim 1 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises controlling data flow to members of the ensemble 24 the machine learning system of claim 1 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises adding new member to the ensemble 25 computer system comprising first set of one or more processing cores first set of one or more computer readable media first student machine learning module maintained on the first set of one or more computer readable media that when executed by the first set of one or more processing cores causes the first set of one or more processing cores to using machine learning automatically learn from and make predictions on input source data second set of one or more processing cores second set of one or more computer readable media and first learning coach machine learning module maintained on the second set of computer readable media that when executed by the second set of one or more processing cores causes the second set of one or more processing cores to receive input data about an internal state of the first student machine learning module and using machine learning automatically learn and implement change to the first student machine learning module based on the data about the internal state of the first student machine learning module to improve operation of the first student machine learning module 26 the machine learning computer system of claim 25 wherein the first student machine learning module comprises deep neural network 27 the machine learning computer system of claim 26 wherein the first learning coach machine learning module utilizes machine learning architecture that is not deep neural network 28 the machine learning computer system of claim 25 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning module that improve the performance of the first student machine learning module 29 the machine learning computer system of claim 25 wherein the enhancement comprises structural change the first student machine learning module 30 the machine learning computer system of claim 25 wherein the enhancement comprises structural change the first student machine learning module 31 the machine learning computer system of claim 30 wherein the first student machine learning module comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning module 32 the machine learning computer system of claim 30 wherein the first student machine learning module comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning module 33 the machine learning computer system of claim 25 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning module to control the learning of the first student machine learning module 34 the machine learning computer system of claim 25 wherein the first set of one or more processing cores comprises first graphics processing unit that comprises multiple processing cores on first single integrated circuit 35 the machine learning computer system of claim 34 wherein the second set of one or more processing cores comprises second graphics processing unit that comprises multiple processing cores on second single integrated circuit 36 method of improving operation of first student machine learning system that using machine learning automatically learns from and makes predictions on input source data the method comprising receiving by first learning coach machine learning system from the first student machine learning system data about an internal state of the first student machine learning system using machine learning automatically learning and implementing by the first learning coach machine learning system an enhancement to the first student machine learning system based on the data about the internal state of the first student machine learning system to improve operation of the first student machine learning system 37 the method of claim 36 wherein the enhancement comprises one or more revised hyperparameters the first student machine learning system that improve learning by the first student machine learning system 38 the method of claim 36 wherein the enhancement comprises structural change the first student machine learning system 39 the method of claim 38 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the network of the first student machine learning system 40 the method of claim 38 wherein the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the network of the first student machine learning system 41 the method of claim 36 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control learning of the first student machine learning system 42 the method of claim 36 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises controlling data flow to members of the ensemble 43 the method of claim 36 wherein the first student machine learning system comprises an ensemble of machine learning systems and wherein the enhancement comprises adding new member to the ensemble lang en",
    "label": 1
  },
  {
    "id1": "079-134-842-572-348",
    "id2": "152-806-746-371-786",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising or replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing the library of dataset from the datastore identifying based on the task and the dataset additional data from the library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 11 the computerimplemented method of claim 1 wherein detecting the data deficit comprises identifying invalid data types and outlier values wherein the additional data further comprise approximated values corresponding to the outliers values corrected data types corresponding to the invalid data types and the approximated values comprise similar data points from the other dataset claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access the library of dataset from the datastore identify based on the task and the dataset additional data from the library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server access look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identify using the look up table the machine learning tool corresponding to the type of task based on the definition of the task form utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task lang en",
    "claims2": "claims claim_text 1 pipeline type processor asynchronous transfer mode atm cells comprisingninput buffers storing n pieces of asynchronous transmission mode atm cellsnn pairs of processor and buffer pipeline processing said each cell the processors provided in first pipeline arrangement and the buffers provided in second pipeline arrangementnan address determining section determining pipeline processing condition said address determining section determining processing instruction code starting address of said each cell based upon the information of said cell stored in said input buffer andnan n1 address transferring sections respectively provided between adjacent ones of said processors in the first pipeline arrangement each of said address transferring sections transferring said processing instruction code starting address between one of the adjacent ones of said processors on an incoming side of the first pipeline arrangement to an other of the adjacent ones of said processors on an outgoing side of the pipeline arrangementnwherein said processors sequentially execute pipeline processing based upon said processing instruction code starting address andnwherein the first pipeline arrangement is separate from the second pipeline arrangement and wherein said each cell is passed along the second pipeline arrangement of said buffers execution by said respective processors at different one of plurality of states claim_text 2 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein each of said address determining sections includesnan information storing memory storing information required retrievalnan address storing memory storing an address at which processing instruction code executed by said processor is storedna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining section claim_text 3 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 4 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci claim_text 5 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said address determining section determines storing address an instruction code according to data format claim_text 6 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said pipeline processing condition is not determined by any of said processors but instead is provided respective processing instruction code starting address each said processor claim_text 7 pipeline type processor asynchronous transfer mode atm cells provided with plurality of processors each executing partial process dedicated to each processor an atm cell being stored into buffer corresponding to each processor said pipeline type processor comprisingnan address storing memory which stores plurality of sets of predetermined process starting addresses each of processors constituting said pipeline type processor each set of predetermined process starting addresses corresponding to conditions of the atm cell to be processed in different respective statenan address retrieving section which retrieves set of predetermined process starting addresses from said address storing memory in accordance with conditions extracted from an input atm cell and transfers each predetermined processing starting address to the corresponding processor andnan execution control section which detects atm cell arrival at each buffer corresponding to each processor and instructs the execution of process to the processor when the atm cell arrives at the corresponding buffernwherein said processors are disposed in first pipeline arrangement separate from second pipeline arrangement in which said buffers are disposed claim_text 8 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein at least one of said processors includes state transition control sections which read instruction code from memory and decode and execute pipeline processing based upon said starting address claim_text 9 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprising an address transferring section provided between said processors said address transferring section transferring said processing instruction code starting address claim_text 10 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprisingnan input buffernan information storing memory storing information required retrievalna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining sectionnwherein the processing instruction code starting address is respectively different each of said processors based on plurality of states the retrieval condition that are obtained from the information storing memory in correspondence with particular condition that matches the retrieval condition claim_text 11 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 12 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci lang en",
    "label": 0
  },
  {
    "id1": "114-059-893-173-320",
    "id2": "022-995-952-592-178",
    "claims1": "claims claim_text 1 method implemented by federated learning server in first management domain the method comprising obtaining first machine learning model from machine learning model management center performing federated learning in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model and sending the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text 2 the method of claim 1 further comprising sending machine learning model requirement information to the machine learning model management center wherein obtaining the first machine learning model comprises receiving the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 3 the method of claim 2 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 4 the method of claim 3 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 5 the method of claim 1 further comprising sending access permission information of the second machine learning model to the machine learning model management center claim_text 6 the method of claim 1 further comprising sending the second machine learning model to federated learning clients claim_text 7 the method of claim 1 further comprising determining that an application effect of the second machine learning model meets preset condition claim_text 8 the method of claim 1 wherein performing the federated learning comprises sending the first machine learning model to federated learning clients to enable the federated learning clients to perform the federated learning based on the first machine learning model and network service data and to obtain intermediate machine learning models of the federated learning client obtaining the intermediate machine learning models from the federated learning clients and aggregating the intermediate machine learning models to obtain the second machine learning model claim_text 9 method implemented by machine learning model management center and comprising sending first machine learning model to first federated learning server in first management domain receiving second machine learning model from the first federated learning server wherein the second machine learning model is based on first federated learning in the first management domain using the first machine learning model and first local network service data in the first management domain and replacing the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text 10 the method of claim 9 wherein before sending the first machine learning model the method further comprises receiving machine learning model requirement information from the first federated learning server and determining the first machine learning model based on the machine learning model requirement information claim_text 11 the method of claim 10 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 12 the method of claim 11 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 13 the method of claim 9 wherein the second machine learning model is based on first training framework wherein the method further comprises converting the second machine learning model into third machine learning model based on second training framework and wherein the third machine learning model and the second machine learning model correspond to same model service information claim_text 14 the method of claim 9 further comprising receiving access permission information of the second machine learning model from the first federated learning server claim_text 15 the method of claim 9 further comprising sending the second machine learning model to second federated learning server in the second management domain receiving fourth machine learning model from the second federated learning server wherein the fourth machine learning model is based on second federated learning in the second management domain using the second machine learning model and second local network service data in the second management domain and replacing the second machine learning model with the fourth machine learning model claim_text 16 federated learning system comprising federated learning server in first management domain and configured to obtain first machine learning model from machine learning model management center send the first machine learning model obtain intermediate machine learning models aggregate the intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain and federated learning clients in the first management domain and configured to receive the first machine learning model from the federated learning server and perform first federated learning based on the first machine learning model and local network service data in the first management domain to obtain the intermediate machine learning models claim_text 17 the federated learning system of claim 16 wherein the federated learning server is further configured to send the second machine learning model to the federated learning clients and wherein the federated learning clients are further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text 18 the federated learning system of claim 16 wherein the federated learning server is further configured to send machine learning model requirement information to the machine learning model management center and receive the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 19 the federated learning system of claim 16 wherein the federated learning server is further configured to send access permission information of the second machine learning model to the machine learning model management center claim_text 20 the federated learning system of claim 16 wherein the federated learning server is further configured to determine that an application effect of the second machine learning model meets preset condition lang en",
    "claims2": "claims claim_text 1 method comprising training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models claim_text 2 the method of claim 1 wherein managing the one or more machine learning features includes generating new version of the machine learning model based on the feature importance metrics claim_text 3 the method of claim 1 wherein managing the one or more machine learning features includes determining to remove one of the one or more machine learning features based on determination that its associated feature importance metric does not meet threshold value claim_text 4 the method of claim 1 wherein managing the one or more machine learning features includes retraining the machine learning model to remove at least one of the one or more machine learning features from the machine learning model claim_text 5 the method of claim 4 wherein managing the one or more machine learning features includes automatically deleting stored data of the at least one removed feature claim_text 6 the method of claim 4 wherein managing the one or more machine learning features includes automatically causing data of the at least one removed feature to be longer collected claim_text 7 the method of claim 1 wherein managing the one or more machine learning features includes modifying at least one of the one or more machine learning features and retraining the machine learning model using the at least one modified feature of the one or more machine learning features claim_text 8 the method of claim 1 wherein managing the one or more machine learning features includes generating new feature based on at least one of the one or more machine learning features and retraining the machine learning model using the new feature claim_text 9 the method of claim 1 wherein determining the feature importance metric selected one of the machine learning features includes comparing base performance of the machine learning model selected test dataset with new performance of the machine learning model modified version of the selected test dataset of the selected machine learning feature claim_text 10 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by modifying values corresponding to the selected machine learning feature using modification approach selected based on property of machine learning model claim_text 11 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by randomizing values corresponding to the selected machine learning feature claim_text 12 the method of claim 1 wherein the each feature importance metric includes numerical value representing an amount of contribution of the corresponding machine learning feature to an inference result of the machine learning model claim_text 13 the method of claim 1 wherein the each feature importance metric includes numerical value representing rank order of the corresponding machine learning feature compared to others of the machine learning features claim_text 14 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes determining ranking order of the machine learning features of the plurality of different machine learning models claim_text 15 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes identifying feature sharing across plurality of the different machine learning models claim_text 16 the method of claim 15 wherein identifying the feature sharing across the plurality of the different machine learning models includes identifying plurality of different feature importance metrics same shared feature claim_text 17 the method of claim 15 wherein managing together machine learning features of the plurality of different machine learning models includes determining whether to remove selected machine learning feature of one of the different machine learning models based on identified sharing of the selected machine learning feature among plurality of the different machine learning models claim_text 18 system comprising processor configured to train machine learning model determine feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics manage one or more machine learning features of the plurality of machine learning features of the machine learning model wherein being configured to manage the one or more machine learning features of the machine learning model includes being configured to manage together machine learning features of plurality of different machine learning models and memory coupled to the processor and configured to provide the processor with instructions claim_text 19 the system of claim 18 wherein being configured to manage together the machine learning features of the plurality of different machine learning models includes being configured to identify feature sharing across plurality of the different machine learning models claim_text 20 computer program product the computer program product being embodied in nontransitory computer readable storage medium and comprising computer instructions training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models lang en",
    "label": 1
  },
  {
    "id1": "023-224-291-896-852",
    "id2": "151-301-910-836-435",
    "claims1": "claims claim_text computerimplemented method of training machine learning model having plurality of parametersn wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and n wherein the method comprisesn determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task n obtaining training data training the machine learning model on second different machine learning task and n training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task n wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task n wherein the machine learning model comprises deep neural network model that includes an input layer and one or more hidden layers that each apply nonlinear transformation to received input to generate an output wherein the machine learning model generates the output based on the received input and on the values of the parameters of the model and n wherein the first and second machine learning tasks each comprise reinforcement learning task in which robotic agent interacts with realworld environment to perform the task claim_text the method of claim 1 wherein the robotic agent comprises static or moving machine or vehicle claim_text the method of any one of claims 12 wherein training the machine learning model on the training data comprises nadjusting the first values of the parameters to optimize an objective function that includesn i first term that measures performance of the machine learning model on the second machine learning task and n ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text the method of claim 1 2 or 3 training the machine learning model on the training data includes each training example in the training datan processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output n determining gradient of an objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and n adjusting the current values of the parameters using the gradient to optimize the objective function and n wherein the objective function includes discounted reward term dependent upon an expected reward from the robotic agent taking an action in state claim_text the method of claim 3 or 4 wherein training the machine learning model on the training data comprises each training example in the training datan processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output n determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and n adjusting the current values of the parameters using the gradient to optimize the objective function claim_text the method of any one of claims 35 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text the method of any one of claims 16 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises ndetermining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text the method of any one of claims 17 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises ndetermining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text the method of any one of claims 18 further comprising nafter training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning modeln obtaining third training data training the machine learning model on third different machine learning task and n training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task n wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text the method of claim 9 further comprisingn determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and n wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includesn i first term that measures performance of the machine learning model on the third machine learning task and n ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task n iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text the method of any one of claims 38 when dependent upon claim 3 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response in particular wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any one of claims 113 claim_text computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 113 lang en",
    "claims2": "claims claim_text 1 computerimplemented method comprising accessing training data computing resource limit setting and parameters of machine learning model forming at server machine learning training strategy based on the training data and the computing resource limit setting forming machine learning model configuration based on the machine learning training strategy selecting sampled data from the training data based on the machine learning training strategy and providing the machine learning model configuration and the sampled data to machine learning platform claim_text 2 the computerimplemented method of claim 1 further comprising training using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitoring computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data claim_text 3 the computerimplemented method of claim 2 further comprising generating an updated machine learning model configuration recommendation based on the resource usage data claim_text 4 the computerimplemented method of claim 2 further comprising receiving from client device the training data the computing resource limit setting and parameters of the machine learning model and providing the resource usage data and the trained machine learning model to the client device claim_text 5 the computerimplemented method of claim 2 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model claim_text 6 the computerimplemented method of claim 2 wherein forming the machine learning training strategy comprises providing summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receiving the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learning model and an accuracy of the machine learning model claim_text 7 the computerimplemented method of claim 6 further comprising providing the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy forming an updated machine learning model configuration based on the updated machine learning training strategy and providing the updated machine learning model configuration and the sampled data to the machine learning platform claim_text 8 the computerimplemented method of claim 1 further comprises testing deployed machine learning model based on the machine learning model configuration accessing performance assessment of the deployed machine learning model and generating performance indicator of the deployed machine learning model based on the testing and the performance assessment determining that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises updating the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model claim_text 9 the computerimplemented method of claim 1 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device claim_text 10 the computerimplemented method of claim 1 wherein the machine learning training strategy comprises one of random forest classifier or gaussian process regressor claim_text 11 computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the apparatus to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform claim_text 12 the computing apparatus of claim 11 wherein the instructions further configure the apparatus to train using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitor computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data claim_text 13 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to generate an updated machine learning model configuration recommendation based on the resource usage data claim_text 14 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to receive from client device the training data the computing resource limit setting and parameters of the machine learning model and provide the resource usage data and the trained machine learning model to the client device claim_text 15 the computing apparatus of claim 12 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model claim_text 16 the computing apparatus of claim 12 wherein forming the machine learn training strategy comprises provide summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receive the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learn model and an accuracy of the machine learning model claim_text 17 the computing apparatus of claim 16 wherein the instructions further configure the apparatus to provide the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy form an updated machine learning model configuration based on the updated machine learning training strategy and provide the updated machine learning model configuration and the sampled data to the machine learning platform claim_text 18 the computing apparatus of claim 11 further comprises test deployed machine learning model based on the machine learning model configuration access performance assessment of the deployed machine learning model and generate performance indicator of the deployed machine learning model based on the testing and the performance assessment determine that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises update the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model claim_text 19 the computing apparatus of claim 11 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device claim_text 20 nontransitory computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform lang en",
    "label": 1
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "033-247-762-034-912",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix claim_text 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold claim_text 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold claim_text 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states claim_text 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals claim_text 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound claim_text 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs claim_text 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states claim_text 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals claim_text 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals claim_text 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result claim_text 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result claim_text 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result claim_text 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result claim_text 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module claim_text 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "label": 1
  },
  {
    "id1": "020-925-613-909-867",
    "id2": "130-909-330-439-20X",
    "claims1": "claims claim_text 1 canceled claim_text 2 computerimplemented method comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 3 the computerimplemented method of claim 2 further comprising receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 4 the computerimplemented method of claim 2 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 5 the computerimplemented method of claim 2 further comprising monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 6 the computerimplemented method of claim 2 further comprising in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 7 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 8 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 9 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 10 the computerimplemented method of claim 2 further comprising testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 11 the computerimplemented method of claim 2 wherein updating the machine learning model further comprises updating definition of task of the machine learning model accessing from datastore additional data based on the updated definition of the task and forming second machine learning model based on the additional data and the updated definition of the task claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 13 the computing apparatus of claim 12 wherein the operations further comprise receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 14 the computing apparatus of claim 12 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 15 the computing apparatus of claim 12 wherein the operations further comprise monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 16 the computing apparatus of claim 12 wherein the operations further comprise in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 17 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 18 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 19 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 20 the computing apparatus of claim 12 wherein the operations further comprise testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 21 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by server cause the server to perform operations comprising accessing by one or more processors of the server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time t res of the at least one compound in the chromatography bed such that equation i is satisfied t res t diff equation i calculating concentration cz t of the at least one compound in the mobile phase at predetermined location z of the chromatography bed and at predetermined time t based on the adsorption isotherm wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 2 the method according to claim 1 wherein the minimum residence time t res of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 3 the method according to claim 1 wherein in the calculation step cz t is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 4 the method according to claim 3 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 5 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 6 the method according to claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 7 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 8 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 9 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 10 the method according to claim 9 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 11 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 12 the chromatography method according to claim 11 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 13 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 9 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "116-391-006-009-952",
    "id2": "086-031-183-779-370",
    "claims1": "claims claim_text claims 1 method of training machine leaming model having plurality of parameters wherein the machine learning model has been trained on first machine leaming task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine leaming task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine leaming task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine leaming model achieving acceptable performance on the first machine learning task 2 the method of claim 1 wherein the first machine leaming task and the second machine leaming task are different supervised learning tasks 3 the method of claim 1 wherein the first machine leaming task and the second machine leaming tasks are different reinforcement learning tasks 4 the method of any one of claims 1 3 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine leaming model in accordance with current values of parameters of the machine leaming model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the obj ective function 6 the method of any one of claims 4 or 5 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter 7 the method of any one of claims 1 6 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task 8 the method of any one of claims 1 7 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine leaming task comprises determining fisher information matrix fim of the plurality of parameters of the machine leaming model with respect to the first machine leaming task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim 9 the method of any one of claims 1 8 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter 13 the method of any one of claims 48 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models 15 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 16 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 1 14 lang en",
    "claims2": "claims claim_text 1 an information processing device optimizing information presented by an information presenter to an information recipient comprising an attribute determination unit that determines knowledge level and an understanding level of the information recipient with respect to the information based on biological activity information of the information recipient acquired by sensor presentation information feature determination unit that determines feature of the information presented by the information presenter to the information recipient and feedback optimization unit that converts presentation format of the information into presentation format according to the knowledge level and the understanding level of the information recipient based on the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the feature of the information determined by the presentation information feature determination unit claim_text 2 the information processing device according to claim 1 wherein the presentation information feature determination unit determines the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation in case where the information is auditory information claim_text 3 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the visual recognition level based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 4 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the voice speed level based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 5 the information processing device according to claim 1 wherein the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient by the feedback optimization unit are presented to the information presenter claim_text 6 an information processing method optimizing information presented by an information presenter to an information recipient using an information processing device comprising first step in which knowledge level and an understanding level of the information recipient with respect to the information are determined based on biological activity of the information recipient acquired by sensor and feature of the information presented by the information presenter to the information recipient is determined and second step in which presentation format of the information is converted into presentation format according to the knowledge level and the understanding level of the information recipient based on the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the feature of the information claim_text 7 the information processing method according to claim 6 wherein in the first step the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation are determined in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation are determined in case where the information is auditory information claim_text 8 the information processing method according to claim 6 wherein in the first step the visual recognition level is determined based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level is determined based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 9 the information processing method according to claim 6 wherein in the first step the voice speed level is determined based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level is determined based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 10 the information processing method according to claim 6 further comprising third step in which the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient are presented to the information presenter lang en",
    "label": 0
  },
  {
    "id1": "036-124-399-531-410",
    "id2": "038-794-758-325-674",
    "claims1": "claims claim_text 1 machine learning computer system comprising first student machine learning system that comprises deep neural network that comprises one or more inner layers wherein each of the one or more inner layers comprises at least one node the one or more inner layers comprises first inner layer having first node the first student machine learning system trained iteratively using machine learning automatically learns from and make predictions on input source data the first node on the first inner layer outputs an activation value each set of inputs values to the first node the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of the training of the first student machine learning system the learned parameters the first node of the first inner layer are updated and first learning coach machine learning system that is in communication with the first student machine learning system wherein the first learning coach machine learning system comprises neural network input to the first learning coach machine learning system comprises values related to the learned parameters and the activation values of the first node of the first inner layer of the deep neural network of the first student machine learning system during the training of the first student machine learning system and the learning coach machine learning system using machine learning automatically learns and implements an enhancement to the first student machine learning system based on the values related to the learned parameters and the activation values of the first node of the first inner layer of the deep neural network of the first student machine learning system to improve operation of the first student machine learning system claim_text 2 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes different patterns than the first student machine learning system claim_text 3 the machine learning computer system of claim 1 wherein the first student machine learning system has different objective than the first student machine learning system claim_text 4 the machine learning computer system of claim 1 wherein the values related to learned parameters and activation values comprise observed andor computed values claim_text 5 the machine learning computer system of claim 4 wherein the computed values comprise partial derivatives the nodes of the one or more inner layers of the deep neural network of the first student machine learning system claim_text 6 the machine learning computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the deep neural network of the first student machine learning system that improve learning by the deep neural network of the first student machine learning system claim_text 7 the machine learning system of claim 6 wherein the revised hyperparameter comprises hyperparameter selected from the group consisting of mini batch size the first student machine learning system learning rate the first student machine learning system regularization parameter the first student machine learning system and momentum parameter the first student machine learning system claim_text 8 the machine learning computer system of claim 1 wherein the enhancement comprises structural change to the deep neural network of the first student machine learning system claim_text 9 the machine learning computer system of claim 8 wherein the structural change comprises one or more additional layers to be added to the deep neural network of the first student machine learning system claim_text 10 the machine learning computer system of claim 8 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the deep neural network of the first student machine learning system claim_text 11 the machine learning computer system of claim 10 wherein the learning coach machine implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the first student machine learning system claim_text 12 the machine learning computer system of claim 11 wherein the selected layer of the deep neural network of the first student machine learning system comprises plurality of existing nodes prior to the implementation of the enhancement and after the enhancement the existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the deep neural network of the first student machine learning system below the selected layer and activations of the virtual nodes are controlled by the first learning coach machine learning system claim_text 13 the machine learning computer system of claim 10 wherein the learning coach machine implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and first learning coach machine learning system controls regularization to the second set of nodes so differences between activation values the second set of nodes and activation values the first set of nodes is less than threshold value to control dropout rate of the nodes in the first and second sets claim_text 14 the machine learning computer system of claim 10 wherein the learning coach machine implements the one or more additional nodes by providing connection weights the additional nodes of the deep neural network of the first student machine learning system claim_text 15 the machine learning computer system of claim 1 wherein the wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control the learning of the deep neural network of the first student machine learning system claim_text 16 the machine learning computer system of claim 1 wherein the machine learning system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activations the first student machine learning system are stored in the high speed memory so that the first student machine learning system can be run when the first student machine learning system is active and the connection weights and activations the first student machine learning system are stored in the secondary storage when the first student machine learning system is not active claim_text 17 the machine learning system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes patterns of learning performance of machine learning system claim_text 18 the machine learning system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the deep neural network of the first student machine learning system claim_text 19 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises deep neural network claim_text 20 the machine learning computer system of claim 1 wherein the first learning coach machine learning system models learning performance of the first student machine learning system regression claim_text 21 the machine learning computer system of claim 1 wherein the first learning coach machine learning system models learning performance of the first student machine learning system classification task claim_text 22 method of improving operation of first student machine learning system the method comprising that training iteratively using machine learning the first student machine learning system to automatically learn from and make predictions on input source data wherein the first student machine learning module comprises deep neural network that comprises one or more inner layers wherein each of the one or more inner layers comprises at least one node the one or more inner layers comprises first inner layer having first node the first node on the first inner layer outputs an activation value each set of inputs values to the first node the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of the training of the first student machine learning system the learned parameters the first node of the first inner layer are updated and during the training of the first student machine learning system receiving by first learning coach machine learning system from the first student machine learning system values related to the learned parameters and the activation values of the first node of the first inner layer of the deep neural network of the first student machine learning system wherein the first learning coach machine learning system comprises neural network and using machine learning automatically learning and implementing by the first learning coach machine learning system an enhancement to the first student machine learning system based on the values related to the learned parameters and the activation values of the first node of the inner layer of the deep neural network of the first student machine learning system to improve operation of the first student machine learning system lang en",
    "claims2": "claims claim_text 1 system training probabilistic predictive model recommending experiment designs synthetic biology comprising nontransitory memory configured to store executable instructions and hardware processor in communication with the nontransitory memory the hardware processor programmed by the executable instructions to receive synthetic biology experimental data generate training data from the synthetic biology experimental data wherein the training data comprise plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective train using the training data plurality of level0 learners of probabilistic predictive model recommending experiment designs synthetic biology wherein an input of each of the plurality of level0 learners comprises input values of the input variables and wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable and train using i predicted values of the at least one response variable determined using the plurality of level0 learners the training inputs of the plurality of training inputs and ii the reference outputs of the plurality of reference outputs correspondence to the training inputs of the plurality of training inputs level1 learner of the probabilistic predictive model recommending experiment designs synthetic biology comprising probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable claim_text 2 10 canceled claim_text 11 the system of claim 1 wherein the synthetic biology experimental data is sparse claim_text 12 the system of claim 1 wherein number of the plurality of training inputs in the synthetic biology experiment data is number of experimental conditions number of strains number of replicates of strain of the strains or combination thereof claim_text 13 15 canceled claim_text 16 the system of claim 1 wherein one or each of the plurality of input variables andor the at least one response variable comprises promoter sequence an induction time an induction strength ribosome binding sequence copy number of gene transcription level of gene an epigenetics state of gene level of protein post translation modification state of protein level of molecule an identity of molecule level of microbe state of microbe state of microbiome titer rate yield or combination thereof optionally wherein the molecule comprises an inorganic molecule an organic molecule protein polypeptide carbohydrate sugar fatty acid lipid an alcohol fuel metabolite drug an anticancer drug biofuel flavoring molecule fertilizer molecule or combination thereof claim_text 17 21 canceled claim_text 22 the system of claim 1 wherein the predetermined response variable objective comprises maximization objective minimization objective or specification objective andor wherein the predetermined response variable objective comprises maximizing the at least one response variable minimizing the at least one response variable or adjusting the at least one response variable to predetermined value of the at least one response variable claim_text 23 the system of claim 1 wherein to train the plurality of level1 learner the hardware processor is programmed by the executable instructions to determine using the plurality of level0 learners the predicted values of the at least one response variable training inputs of the plurality of training inputs claim_text 24 the system of claim 1 wherein the level1 learner comprises bayesian ensemble of the plurality of level0 learners claim_text 25 the system of claim 1 wherein parameters of the ensemble of the plurality of level0 learners comprises i plurality of ensemble weights and ii an error variable distribution of the ensemble or standard deviation of the error variable distribution of the ensemble claim_text 26 32 canceled claim_text 33 the system of claim 1 wherein to train the level1 learner the hardware processor is programmed by the executable instructions to determine posterior distribution of the ensemble parameters given the training data or the second subset of the training data wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to determine i probability distribution of the training data or the second subset of the training data given the ensemble parameters or likelihood function of the ensemble parameters given the training data of the second subset of the training data and ii prior distribution of the ensemble parameters and wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to sample space of the ensemble parameters with frequency proportional to desired posterior distribution claim_text 34 canceled claim_text 35 canceled claim_text 36 the system of claim 1 wherein to train the plurality of level0 learners the hardware processor is programmed by the executable instructions to generate first subset of the training data and train using the first subset of the training data the plurality of level0 learners claim_text 37 50 canceled claim_text 51 the system of claim 1 wherein the hardware processor is programmed by the executable instructions to determine surrogate function with an input experiment design an input the surrogate function comprising an expected value of the at least one response variable determined using the input experiment design variance of the value of the at least one response variable determined using the input experiment design and an exploitationexploration tradeoff parameter and determine using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of synthetic biology experiment obtaining predetermined response variable objective associated with the at least one response variable claim_text 52 57 canceled claim_text 58 the system of claim 51 wherein to determine the plurality of recommended experiment designs the hardware processor is programmed by the executable instructions to determine plurality of possible recommended experiment designs each comprising possible recommended values of the input variables with surrogate function values determined using the surrogate function with predetermined characteristic and select the plurality of recommended experiment designs from the plurality of possible recommended experiment designs using an input variable difference factor based on the surrogate function values of the plurality of possible recommended experiment designs claim_text 59 64 canceled claim_text 65 the system of claim 51 wherein number of the plurality of recommended experiment designs is number of experimental conditions or number of strains the next cycle of the synthetic biology experiment claim_text 66 canceled claim_text 67 canceled claim_text 68 the system of claim 51 wherein to determine the plurality of possible recommended experiment designs the hardware processor is programmed by the executable instructions to sample space of the input variables with frequency proportional to the surrogate function or an exponential function of the surrogate function and prior distribution of the input variables claim_text 69 canceled claim_text 70 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine an upper bound andor lower bound one or each of the plurality of input variables based on training values of the corresponding input variable wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 71 canceled claim_text 72 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to receive an upper bound andor lower bound one or each of the plurality of input variables wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 73 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability distribution of the at least one response variable one or each of the plurality of recommended experiment designs claim_text 74 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of one or each of the plurality of recommended experiment designs being predetermined percentage closer to achieving the objective relative to the training data claim_text 75 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective being predetermined percentage closer to achieving the objective relative to the training data claim_text 76 canceled claim_text 77 canceled claim_text 78 method recommending experiment designs synthetic biology comprising under control of hardware processor receiving probabilistic predictive model recommending experiment designs synthetic biology comprising plurality of level0 learners and level1 learner wherein an input of each of the plurality of level0 learners comprises input values of the input variables wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable wherein the level1 learner comprises probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable wherein the plurality of level0 learners and the level1 learner are trained using training data obtained from one or more cycles of synthetic biology experiment comprising plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective determining surrogate function comprising an expected value of the level1 learner variance of the level1 learner and an exploitationexploration tradeoff parameter and determining using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of the synthetic biology experiment achieving predetermined response variable objective associated with the at least one response variable claim_text 79 85 canceled lang en",
    "label": 0
  },
  {
    "id1": "190-059-083-058-239",
    "id2": "051-909-779-474-634",
    "claims1": "claims claim_text 1 machine learning ml computer system training student ml system iteratively through machine learning on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of training of the student ml system the learned parameters the first node of the first inner layer are updated and the ml computer system comprises learning coach ml system that is in communication with the student ml system wherein the learning coach ml system comprises neural network that comprises an input layer and an output layer the learning coach ml system has been trained through machine learning to determine and implement an enhancement to the student ml system based on input to the learning coach ml system from the student ml system wherein the input from the student ml system is input to the input layer of the learning coach ml system the input to the learning coach ml system from the student ml system comprises observations about an internal state of the student ml system during training of the student ml system and the observations about the internal state of the student ml system comprise values related to the learned parameters the first node on the first inner layer of the student ml system and the activation values the first node on the first inner layer of the student ml system such that the learning coach ml system determines and implements the enhancement to the student ml system based on the input from the student ml system claim_text 2 the ml computer system of claim 1 wherein the learning coach ml system comprises pattern recognition system that recognizes different patterns than the student ml system claim_text 3 the ml computer system of claim 1 wherein the learning coach ml system has different objective than the student ml system claim_text 4 the ml computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 5 the ml system of claim 4 wherein the revised hyperparameter comprises hyperparameter selected from the group consisting of mini batch size the student ml system learning rate the student ml system regularization parameter the student ml system and momentum parameter the student ml system claim_text 6 the ml computer system of claim 1 wherein the enhancement comprises structural change to the student ml system claim_text 7 the ml computer system of claim 6 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 8 the ml computer system of claim 6 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 9 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 10 the ml computer system of claim 9 wherein the selected layer of the student ml system comprises plurality of existing nodes prior to the implementation of the enhancement and after the enhancement the existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the student ml system below the selected layer and activations of the virtual nodes are controlled by the learning coach ml system claim_text 11 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and learning coach ml system controls regularization to the second set of nodes so differences between activation values the second set of nodes and activation values the first set of nodes is less than threshold value to control dropout rate of the nodes in the first and second sets claim_text 12 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing connection weights the one or more additional nodes of the student ml system claim_text 13 the ml computer system of claim 1 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 14 the ml computer system of claim 1 wherein the ml system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activation values the student ml system are stored in the high speed memory so that the student ml system can be run when the student ml system is active and the connection weights and activation values the student ml system are stored in the secondary storage when the student ml system is not active claim_text 15 the ml system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 16 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system regression claim_text 17 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system classification task claim_text 18 machinelearning ml training method comprising initially training learning coach ml system to determine and implement an enhancement to ml system wherein the learning coach ml system comprises an input layer and an output layer training iteratively through machine learning student ml system on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and the initial training the student ml system comprises following iterations of the training of the student ml system updating the learned parameters the first node of the first inner layer and training the student ml system comprises receiving by the learning coach ml system from the student ml system values related to the learned parameters and the activation values of the first node of the first inner layer of the student ml system and determining and implementing by the learning coach ml system the enhancement to the student ml system based on the values related to the learned parameters and the activation values of the first node of the inner layer of the student ml system to improve performance of the student ml system claim_text 19 the ml training method of claim 18 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 20 the ml training method of claim 18 wherein the enhancement comprises structural change to the student ml system claim_text 21 the ml training method of claim 20 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 22 the ml training method of claim 20 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 23 the ml training method of claim 22 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 24 the ml training method of claim 18 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 25 the ml training method of claim 18 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 26 the ml computer system of claim 1 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and during training of the student ml system activation values each of the multiple feature node are softtied training data examples within the cluster claim_text 27 the ml computer system of claim 26 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 28 the ml computer system of claim 27 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 29 the ml computer system of claim 27 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 30 the ml computer system of claim 29 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 31 the ml computer system of claim 30 wherein the ml learning coach is configured to set the relaxation strength hyperparameters the feature nodes claim_text 32 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to an activation value of node in separate ml system claim_text 33 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to activation values of multiple nodes in separate ml system claim_text 34 the ml computer system of claim 33 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 35 the ml computer system of claim 34 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 36 the ml computer system of claim 35 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 37 the ml computer system of claim 36 wherein the learning coach ml system is configured to set the relaxation strength hyperparameter the first node claim_text 38 the ml computer system of claim 1 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the ml computer system further comprises data switching network that selectively directs training data examples the student ml system to one or more and less than all of the multiple ml ensemble members claim_text 39 the ml computer system of claim 38 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 40 the ml computer system of claim 38 further comprising classifier classifying the training data examples the student ml system and wherein the classifier controls the data switching network training data example based on classification by the classifier the training data example claim_text 41 the ml training method of claim 18 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and training the student ml system further comprises softtying activation values each of the multiple feature nodes training data examples within the cluster claim_text 42 the ml training method of claim 41 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 43 the ml training method of claim 42 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 44 the ml training method of claim 42 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 45 the ml training method of claim 44 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 46 the ml training method of claim 45 further comprising setting by the ml learning coach the relaxation strength hyperparameters the feature nodes claim_text 47 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to an activation value of node in separate ml system claim_text 48 the ml training method of claim 18 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to activation values of multiple nodes in separate ml system claim_text 49 the ml training method of claim 48 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 50 the ml training method of claim 49 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 51 the ml training method of claim 50 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 52 the ml training method of claim 51 further comprising setting by the learning coach ml system the relaxation strength hyperparameter the first node claim_text 53 the ml training method of claim 18 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the method further comprises selectively directing by data switching network training data examples the student ml system to one or more but less than all of the multiple ml ensemble members claim_text 54 the ml training method of claim 53 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 55 the ml training method of claim 53 further comprising classifying by classifier the training data examples the student ml system and controlling by the classifier the data switching network training data example based on classification by the classifier the training data example lang en",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "130-984-086-698-559",
    "id2": "036-552-384-938-884",
    "claims1": "claims claim_text 1 computerimplemented method of training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 2 the method of claim 1 wherein the first machine learning task and the second machine learning task are different supervised learning tasks claim_text 3 the method of claim 1 wherein the first machine learning task and the second machine learning tasks are different reinforcement learning tasks claim_text 4 the method of claim 1 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the objective function claim_text 6 the method of claim 4 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text 7 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text 8 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text 9 the method of claim 1 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text 13 the method of claim 4 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response claim_text 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text 15 the method of claim 1 the method further comprising providing the trained machine learning model use in processing data after training the machine learning model on the second machine learning task claim_text 16 the method of claim 1 wherein the first and second machine learning tasks each comprise reinforcement learning task and wherein the reinforcement learning task is controlling an agent to interact with an environment to achieve goal claim_text 17 the method of claim 1 wherein the first and second machine learning tasks each comprise classification task and wherein the classification task is processing data to classify the data claim_text 18 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 19 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task lang en",
    "claims2": "claims claim_text we claim 1 method 400 determining positioning method user equipment ue in network the method comprising loading by loading unit 302 at location server 300 plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods receiving by transceiver unit 304 at the location server 300 location request associated with service from the ue wherein the location request relates to determining location of the ue selecting by an execution unit 306 at the location server 300 first positioning method from the plurality of positioning methods based on the location request selecting by the execution unit 306 at the location server 300 fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeating by the execution unit 306 one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with nonsupported status selecting by the execution unit 306 at the location server 300 cell identifier cellid based positioning method in an event each of the selected fallback positioning methods from the set of fallback positioning methods is associated with nonsupported status 2 the method 400 claimed in claim 1 wherein each of the plurality of positioning methods is based on one of service identifier service id associated with the service and quality of service qos information associated with the location request 3 the method 400 claimed in claim 2 wherein post the receiving of the location request associated with the service from the ue the method comprises checking by processing unit 308 at the location server 300 an availability of at least one of the service id and the qos information in the received location request 4 the method 400 claimed in claim 2 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 5 the method 400 claimed in claim 1 wherein the location server 300 is location management function lmf 6 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 the first positioning method from the plurality of positioning methods the method comprises determining by the execution unit 306 one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 7 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 at the location server 300 the fallback positioning method from the set of fallback positioning methods the method comprises determining one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 8 the method 400 claimed in claim 6 wherein the cellid identifies base station associated with the ue 9 system 300 determining positioning method user equipment ue in network the system comprising location server 300 wherein the location server 300 comprises loading unit 302 configured to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 connected to at least the loading unit 302 the transceiver unit 304 configured to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 connected to at least the transceiver unit 304 the execution unit 306 configured to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method 10 the system 300 claimed in claim 9 wherein each of the plurality of positioning methods is based on at least one of service identifier service id associated with the service and quality of service qos information associated with the location request 11 the system 300 claimed in claim 10 wherein post receiving the location request associated with the service from the ue the system comprises processing unit 308 configured to check an availability of at least one of the service id and the qos information in the received location request 12 the system 300 claimed in claim 10 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 13 the system 300 claimed in claim 9 wherein the location server 300 is location management function lmf 14 the system 300 claimed in claim 9 wherein prior to the selecting the first positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 15 the system 300 claimed in claim 9 wherein prior to the selecting the fallback positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 16 the system 300 claimed in claim 9 wherein the cellid identifies base station associated with the ue 17 nontransitory computerreadable storage medium storing instructions determining positioning method of user equipment ue in network the storage medium comprising executable code which when executed by one or more units of system comprising location server 300 causes loading unit 302 to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status and select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method lang en",
    "label": 0
  },
  {
    "id1": "127-177-660-785-528",
    "id2": "091-570-367-989-78X",
    "claims1": "claims claim_text claims what is claimed is 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning mode requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 8 the method of claim 1 wherein generating the updated machine learning mode comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 1 1 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "claims2": "claims claim_text 1 method of aiding in diagnosing whether subject has depression comprising analyzing blood sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid alphahydroxyisobutyric acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the biological sample is blood plasma claim_text 8 the method of claim 1 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 9 the method of claim 8 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 10 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and the first sample is obtained from the subject at first time point and wherein the analysis method the blood sample is mass spectrometry analyzing second biological sample that was removed from subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 11 the method of claim 10 wherein the method further comprises comparing the levels of the biomarkers in the first sample the levels of the biomarkers in the second sample andor the results of the comparison of the levels of the biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the biomarkers claim_text 12 the method of claim 10 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 13 the method of claim 12 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 14 method of assessing the efficacy of composition treating depression comprising analyzing blood sample removed from subject having depression and currently or previously being treated with composition to determine the levels of biomarkers depression wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to levels of the biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the biomarkers c depressionnegative reference levels of the biomarkers d depressionprogressionpositive reference levels of the biomarkers andor depressionregressionpositive reference levels of the biomarkers claim_text 15 the method of claim 14 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 16 the method of claim 15 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 17 method assessing the efficacy of composition in treating depression comprising analyzing first blood sample obtained from subject to determine the levels of biomarkers depression the first sample obtained from the subject at first time point wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry administering the composition to the subject analyzing second biological sample obtained from the subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point after administration of the composition and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 18 the method of claim 17 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids lang en",
    "label": 0
  },
  {
    "id1": "034-722-477-434-593",
    "id2": "189-406-053-095-096",
    "claims1": "claims claim_text 1 computer system configuring plurality of machine learning pipelines into machine learning pipeline ensemble the system comprising one or more computer processors one or more computer readable storage media computer program instructions the computer program instructions being stored on the one or more computer readable storage media execution by the one or more computer processors and the computer program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 2 the computer system of claim 1 wherein the plurality of machine learning pipelines are heterogeneous claim_text 3 the computer system of claim 1 wherein the instructions the reinforcement learning agent coupled to the machine learning pipeline to adjust the configuration values of the coupled machine learning pipeline further include instructions to receive performance information associated with the uncoupled machine learning pipeline determine an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjust the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 4 the computer system of claim 3 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 5 the computer system of claim 4 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 6 the computer system of claim 1 further comprising instructions to determine similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merge the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 7 the computer system of claim 3 further comprising instructions to generate new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 8 the computer system of claim 1 wherein the configuration parameter values of the coupled and uncoupled machine learning pipelines include at least one value selected from the group consisting of training dataset learning environment machine learning pipeline structure an objective function and hyperparameter set claim_text 9 the computer system of claim 1 wherein performance information of the uncoupled machine learning pipeline includes at least one performance metric selected from the group consisting of prediction accuracy value prediction accuracy value drift diversity of predictions running time and an entropy value claim_text 10 the computer system of claim 1 further comprising program instructions to generate machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 11 the computer system of claim 1 further comprising instructions to generate the plurality of machine learning pipelines from plurality of input datasets and couple each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent claim_text 12 computer program product configuring plurality of machine learning pipelines into machine learning pipeline ensemble the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media the program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 13 computerimplemented method configuring plurality of machine learning pipelines into machine learning pipeline ensemble the method comprising determining by reinforcement learning agent coupled to machine learning pipeline performance information of the machine learning pipeline receiving by the reinforcement learning agent configuration parameter values of uncoupled machine learning pipelines of the plurality of machine learning pipelines and adjusting by the reinforcement learning agent configuration parameter values of the machine learning pipeline based on the performance information of the machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipelines claim_text 14 the computerimplemented method of claim 13 further comprising receiving performance information associated with the uncoupled machine learning pipeline determining an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjusting the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 15 the computerimplemented method of claim 14 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 16 the computerimplemented method of claim 15 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 17 the computerimplemented method of claim 13 further comprising determining similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merging the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 18 the computerimplemented method of claim 14 further comprising generating new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 19 the computerimplemented method of claim 13 further comprising generating machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 20 the computerimplemented method of claim 13 further comprising generating the plurality of machine learning pipelines from plurality of input datasets and coupling each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time of the at least one compound in the chromatography bed such that equation i is satisfied equation i t res t diff equation i calculating concentration czt of the at least one compound in the mobile phase at predetermined location of the chromatography device and at predetermined time t based on the adsorption isotherm claim_text 2 the method according to claim 1 wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 3 the method according to claim 1 wherein the minimum residence time of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 4 the method according to claim 1 wherein in the calculation step czt is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 5 the method according to claim 4 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 6 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 7 the method according claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 8 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 9 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 10 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 11 the method according to claim 10 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 12 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 13 the chromatography method according to claim 12 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising iii carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 15 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 11 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "093-548-301-929-508",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 8 the method of claim 1 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model claim_text 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 11 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "label": 1
  },
  {
    "id1": "122-175-620-169-033",
    "id2": "013-300-859-082-341",
    "claims1": "claims claim_text 1 canceled claim_text 2 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 3 the method of claim 2 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 6 the method of claim 5 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 7 the method of claim 3 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 8 the method of claim 2 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 9 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 10 the system of claim 9 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 13 the system of claim 12 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 14 the system of claim 10 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 15 the system of claim 9 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 16 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 17 the nontransitory computer storage medium of claim 16 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 20 the nontransitory computer storage medium of claim 19 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 21 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 network slice configuration method comprising sending by first network element network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 2 the network slice configuration method of claim 1 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 3 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 4 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is sent by the first network element to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 5 the network slice configuration method of claim 1 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 6 network slice configuration method comprising receiving by second network element network slice configuration information of first network element and storing by the second network element network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 7 the network slice configuration method of claim 6 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 8 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 9 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is received by the second network element from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 10 the network slice configuration method of claim 6 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 11 first network element comprising sending module which is configured to send network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 12 the first network element of claim 11 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 13 the first network element of claim 11 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 14 the first network element of claim 11 wherein the sending module is further configured to send information of network slice supported by ta to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 15 the first network element of claim 11 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 16 second network element comprising receiving module which is configured to receive slice configuration information of first network element and storing module which is configured to storing network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 17 the second network element of claim 16 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 18 the second network element of claim 16 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 19 the second network element of claim 16 wherein the receiving module is further configured to receive information of network slice supported by ta from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 20 the second network element of claim 16 wherein the network slice information comprises single network slice selection assistance information snssai lang en",
    "label": 0
  },
  {
    "id1": "026-671-265-300-529",
    "id2": "057-360-526-753-072",
    "claims1": "claims claim_text 1 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 3 the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 6 the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 8 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 9 the system of claim 8 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 10 the system of claim 9 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 13 the system of claim 9 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 14 the system of claim 8 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 15 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 16 the nontransitory computer storage medium of claim 15 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 17 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 20 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer setting detected number of active connections in counters provided each output port and controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections wherein said controlling step further includes outputting cell addressed to an output port corresponding to counter with said count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to said output port and subtracting value of said counter corresponding to an output port that has output cells at every output of cell claim_text 2 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of cell addressed to said output port from an input multiplex buffer claim_text 3 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of setting detected number of active connections in counters provided each output port and sequentially outputting cell addressed to an output port corresponding to counter with count value not set to 0 among said counters and subtracting value of counter corresponding to an output port that has output cell at every output of said cell when outputting cells accumulated in an output separation buffer to an output port claim_text 4 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output from said output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of the cells addressed to said output from an input multiplex buffer claim_text 5 an asynchronous transfer mode hereinafter referred to atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively input multiplex buffer control means controlling said input multiplex buffer so that output of cell addressed to certain output port indicated by first back pressure signal is inhibited and output separation buffer control means monitoring an accumulation amount of cells in said output separation buffer at every output port to which said cell is addressed and outputting said first back pressure signal to said input multiplex buffer control means to inhibit output of cell addressed to an output port having said accumulation amount equal to or more than predetermined threshold value detection means detecting number of active connections existing between each output port and each input multiplex buffer and said output separation buffer control means further comprises output control means controlling number of cells addressed to said output port output from an output separation buffer so that said number of cells are in proportional to number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said output control means further comprises counter each provided an output port means setting each number of active connections detected by said detection by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting counter value corresponding to an output port which has output cells at every output of cell claim_text 6 the atm switch of claim 5 wherein said output control means comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to said output separation buffer when said cells output from said output separation buffer are equal to or larger than cells input thereto and means outputting said first back pressure signal inhibiting output of cells addressed to said detected output port to said input multiplex buffer control means claim_text 7 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises counter provided each output port means setting each number of active connections detected by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting value of said counter corresponding to an output port that has output cell at every output of cell claim_text 8 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to an output separation buffer when cells output separation buffer when cells output from said output separation buffer is equal to or more than cells input thereto and means outputting said first back pressure signal inhibiting output of cell addressed to said detected output port to said input multiplex buffer control means lang en",
    "label": 0
  },
  {
    "id1": "152-848-867-961-18X",
    "id2": "152-806-746-371-786",
    "claims1": "claims claim_text claims we claim 1 method comprising training via at least one processor of computer system using public data inputs public data machine learning model training via the at least one processor using private data inputs private data machine learning model training via the at least one processor public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing via the at least one processor the public data machine learning model using current public data input resulting in public data machine learning prediction executing via the at least one processor the private data machine learning model using current pnvate data input resulting in private data machine learning prediction and executing via the at least one processor the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 2 the method of claim 1 wherein the public data and the private data are associated with freight transport costs 3 the method of claim 2 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 4 the method of claim 2 wherein the private data comprises overhead costs and earner costs 5 the method of claim 1 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 6 the method of claim 5 further comprising adding via the at least one processor the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding via the at least one processor the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 7 the method of claim 1 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 8 system comprising at least one processor and nontransitory computerreadable storage medium having instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 9 the system of claim 8 wherein the public data and the private data are associated with freight transport costs 10 the system of claim 9 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 11 the system of claim 9 wherein the private data comprises overhead costs and earner costs 12 the system of claim 8 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 13 the system of claim 12 the nontransitory computerreadable storage medium having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 14 the system of claim 8 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 15 nontransitory computerreadable storage medium having instructions stored which when executed by at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 16 the nontransitory computerreadable storage medium of claim 15 wherein the public data and the private data are associated with freight transport costs 17 the nontransitory computerreadable storage medium of claim 16 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 18 the nontransitory computerreadable storage medium of claim 16 wherein the private data comprises overhead costs and carrier costs 19 the nontransitory computerreadable storage medium of claim 15 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 20 the nontransitory computerreadable storage medium of claim 19 having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data lang en",
    "claims2": "claims claim_text 1 pipeline type processor asynchronous transfer mode atm cells comprisingninput buffers storing n pieces of asynchronous transmission mode atm cellsnn pairs of processor and buffer pipeline processing said each cell the processors provided in first pipeline arrangement and the buffers provided in second pipeline arrangementnan address determining section determining pipeline processing condition said address determining section determining processing instruction code starting address of said each cell based upon the information of said cell stored in said input buffer andnan n1 address transferring sections respectively provided between adjacent ones of said processors in the first pipeline arrangement each of said address transferring sections transferring said processing instruction code starting address between one of the adjacent ones of said processors on an incoming side of the first pipeline arrangement to an other of the adjacent ones of said processors on an outgoing side of the pipeline arrangementnwherein said processors sequentially execute pipeline processing based upon said processing instruction code starting address andnwherein the first pipeline arrangement is separate from the second pipeline arrangement and wherein said each cell is passed along the second pipeline arrangement of said buffers execution by said respective processors at different one of plurality of states claim_text 2 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein each of said address determining sections includesnan information storing memory storing information required retrievalnan address storing memory storing an address at which processing instruction code executed by said processor is storedna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining section claim_text 3 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 4 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci claim_text 5 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said address determining section determines storing address an instruction code according to data format claim_text 6 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said pipeline processing condition is not determined by any of said processors but instead is provided respective processing instruction code starting address each said processor claim_text 7 pipeline type processor asynchronous transfer mode atm cells provided with plurality of processors each executing partial process dedicated to each processor an atm cell being stored into buffer corresponding to each processor said pipeline type processor comprisingnan address storing memory which stores plurality of sets of predetermined process starting addresses each of processors constituting said pipeline type processor each set of predetermined process starting addresses corresponding to conditions of the atm cell to be processed in different respective statenan address retrieving section which retrieves set of predetermined process starting addresses from said address storing memory in accordance with conditions extracted from an input atm cell and transfers each predetermined processing starting address to the corresponding processor andnan execution control section which detects atm cell arrival at each buffer corresponding to each processor and instructs the execution of process to the processor when the atm cell arrives at the corresponding buffernwherein said processors are disposed in first pipeline arrangement separate from second pipeline arrangement in which said buffers are disposed claim_text 8 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein at least one of said processors includes state transition control sections which read instruction code from memory and decode and execute pipeline processing based upon said starting address claim_text 9 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprising an address transferring section provided between said processors said address transferring section transferring said processing instruction code starting address claim_text 10 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprisingnan input buffernan information storing memory storing information required retrievalna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining sectionnwherein the processing instruction code starting address is respectively different each of said processors based on plurality of states the retrieval condition that are obtained from the information storing memory in correspondence with particular condition that matches the retrieval condition claim_text 11 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 12 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci lang en",
    "label": 0
  },
  {
    "id1": "020-925-613-909-867",
    "id2": "130-984-086-698-559",
    "claims1": "claims claim_text 1 canceled claim_text 2 computerimplemented method comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 3 the computerimplemented method of claim 2 further comprising receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 4 the computerimplemented method of claim 2 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 5 the computerimplemented method of claim 2 further comprising monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 6 the computerimplemented method of claim 2 further comprising in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 7 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 8 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 9 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 10 the computerimplemented method of claim 2 further comprising testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 11 the computerimplemented method of claim 2 wherein updating the machine learning model further comprises updating definition of task of the machine learning model accessing from datastore additional data based on the updated definition of the task and forming second machine learning model based on the additional data and the updated definition of the task claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 13 the computing apparatus of claim 12 wherein the operations further comprise receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 14 the computing apparatus of claim 12 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 15 the computing apparatus of claim 12 wherein the operations further comprise monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 16 the computing apparatus of claim 12 wherein the operations further comprise in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 17 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 18 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 19 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 20 the computing apparatus of claim 12 wherein the operations further comprise testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 21 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by server cause the server to perform operations comprising accessing by one or more processors of the server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data lang en",
    "claims2": "claims claim_text 1 computerimplemented method of training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 2 the method of claim 1 wherein the first machine learning task and the second machine learning task are different supervised learning tasks claim_text 3 the method of claim 1 wherein the first machine learning task and the second machine learning tasks are different reinforcement learning tasks claim_text 4 the method of claim 1 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the objective function claim_text 6 the method of claim 4 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text 7 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text 8 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text 9 the method of claim 1 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text 13 the method of claim 4 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response claim_text 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text 15 the method of claim 1 the method further comprising providing the trained machine learning model use in processing data after training the machine learning model on the second machine learning task claim_text 16 the method of claim 1 wherein the first and second machine learning tasks each comprise reinforcement learning task and wherein the reinforcement learning task is controlling an agent to interact with an environment to achieve goal claim_text 17 the method of claim 1 wherein the first and second machine learning tasks each comprise classification task and wherein the classification task is processing data to classify the data claim_text 18 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 19 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task lang en",
    "label": 1
  },
  {
    "id1": "020-796-323-879-358",
    "id2": "160-348-204-331-805",
    "claims1": "claims claim_text 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyzing the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposing and displaying on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 7 the method of claim 5 further including automatically updating the interactive visualization gui according to one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings claim_text 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof analyze the extracted metadata wherein the extracted metadata includes provenance metadata and in conjunction with generating the interactive visualization gui of the machine learning pipeline decompose and display on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui according to one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings claim_text 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computerreadable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into machine learning composition logic an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the machine learning composition logic wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof an executable portion that analyzes the extracted metadata wherein the extracted metadata includes provenance metadata and an executable portion that in conjunction with generating the interactive visualization gui of the machine learning pipeline decomposes and displays on the interactive visualization gui specific algorithms parameters and technical characteristics used to generate each machine learning model in the machine learning model pipeline according to the analysis of the extracted metadata and wherein the decomposing and displaying includes displaying on the interactive visualization gui list of machine learning composition structures inclusive of specified data and specified machine learning tasks used to generate each machine learning model in the machine learning model pipeline claim_text 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the machine learning composition logic claim_text 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata the provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata the provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines claim_text 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata the provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria claim_text 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition logic rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition logic used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria claim_text 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines the user interacts with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the machine learning composition logic one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings claim_text 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui according to one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the machine learning composition logic or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 34 canceled claim_text 35 computerimplemented method executing machinelearning model performed in first network node the method comprising developing first machinelearning model based on first set of data and using machinelearning algorithm communicating to second network node the first machinelearning model receiving from the second network node information about difference between the first machinelearning model and second machinelearning model receiving request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtaining information indicative of an execution policy and depending on the obtained information indicative of the execution policy either executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result claim_text 36 the method according to claim 35 wherein partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model at the second network node to obtain result comprises partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model based on the information about difference between the first machinelearning model and the second machinelearning model claim_text 37 the method according to claim 35 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises any one or more of information that the second machinelearning model is different from the first machinelearning model information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model information identifying difference between the first machinelearning model and the second machinelearning model or the second machinelearning model claim_text 38 the method according to claim 35 wherein the information indicative of an execution policy is obtained from policy node or is obtained from memory in the first network node claim_text 39 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating execution of the machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model said information further comprises information indicating that at least part of said machinelearning model should be executed in an enclaved mode claim_text 40 the method according to claim 35 wherein when the information indicative of the execution policy comprises information indicating partial execution of the first machinelearning model and partial execution of the second machinelearning model said information further comprising information indicating that at least one component of said first machinelearning model or of said second machinelearning model should be executed in an enclaved mode claim_text 41 the method according to claim 35 wherein the first machinelearning model and the second machinelearning model are representable computational graphs claim_text 42 the method according to claim 41 wherein the computational graphs are directed acyclic graphs claim_text 43 the method according to claim 41 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 44 the method according to claim 35 wherein the step of executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result comprises at least partially executing machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model in an enclaved memory segment claim_text 45 the method according to claim 35 wherein the step of partially executing the first machinelearning model and causing the second network node to partially execute the second machinelearning model to obtain result comprises one or more of executing at least one component of the first machinelearning model in an enclaved memory segment causing the second network node to partially execute the second machinelearning model in an enclaved memory segment partially executing the first machinelearning model to form first partial result communicating to the second network node the first partial result causing the partial execution of the second machinelearning model using the first partial result at the second network node such that second partial result is formed at the second network node receiving from the second network node the second partial result or partially executing the first machinelearning model using the second partial result to form final result claim_text 46 first network node executing machinelearning model the first network node comprising an interface configured allowing communication with other network nodes and processing circuitry operatively associated with the interface and configured to develop first machinelearning model based on first set of data and using machinelearning algorithm communicate the first machinelearning model to second network node receive from the second network node information about difference between the first machinelearning model and second machinelearning model receive request the execution of machinelearning model responsive to receiving the request the execution of the machinelearning model obtain information indicative of an execution policy and depending on the obtained information indicative of an execution policy either execute machinelearning model based on the first machinelearning model and the information about difference between the first machinelearning model and the second machinelearning model to obtain result or partially execute the first machinelearning model and cause the second network node to partially execute the second machinelearning model to obtain result claim_text 47 method executing machinelearning model performed in second network node the method comprising receiving from first network node first machinelearning model developing second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicating to the first network node information about difference between the first machinelearning model and the second machinelearning model receiving from the first network node partial result partially executing the second machinelearning model using the first partial result to form second partial result and communicating to the first network node the second partial result claim_text 48 the method according to claim 47 wherein the information about difference between the first machinelearning model and the second machinelearning model comprises information that the second machinelearning model is different from the first machinelearning model andor information that the second machinelearning model is different from the first machinelearning model or information that the second machinelearning model is not different from the first machinelearning model andor information identifying difference between the first machinelearning model and the second machinelearning model andor the second machinelearning model claim_text 49 the method according to claim 47 wherein the machinelearning model and the second machinelearning model are representable computational graphs claim_text 50 the method according to claim 49 wherein the computational graphs are directed acyclic graphs claim_text 51 the method according to claim 49 wherein the first machinelearning model and the second machinelearning model are each one of the following neural networks support vector machines decision trees and random forests claim_text 52 the method according to claim 47 wherein the step of partially executing the second machinelearning model using the first partial result to form second partial result comprises executing at least one component of the second machinelearning model at the second node in an enclaved memory segment claim_text 53 network node executing machinelearning model the network node referred to second network node and comprising an interface configured allowing communication with other network nodes processing circuitry operatively associated with the interface and configured to receive from first network node first machinelearning model develop second machinelearning model based on the first machinelearning model and second set of data and using the machinelearning algorithm communicate to the first network node information about difference between the first machinelearning model and the second machinelearning model receive from the first network node partial result partially execute the second machinelearning model using the first partial result to form second partial result and communicate to the first network node the second partial result lang en",
    "label": 1
  },
  {
    "id1": "151-301-910-836-435",
    "id2": "130-909-330-439-20X",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing training data computing resource limit setting and parameters of machine learning model forming at server machine learning training strategy based on the training data and the computing resource limit setting forming machine learning model configuration based on the machine learning training strategy selecting sampled data from the training data based on the machine learning training strategy and providing the machine learning model configuration and the sampled data to machine learning platform claim_text 2 the computerimplemented method of claim 1 further comprising training using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitoring computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data claim_text 3 the computerimplemented method of claim 2 further comprising generating an updated machine learning model configuration recommendation based on the resource usage data claim_text 4 the computerimplemented method of claim 2 further comprising receiving from client device the training data the computing resource limit setting and parameters of the machine learning model and providing the resource usage data and the trained machine learning model to the client device claim_text 5 the computerimplemented method of claim 2 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model claim_text 6 the computerimplemented method of claim 2 wherein forming the machine learning training strategy comprises providing summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receiving the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learning model and an accuracy of the machine learning model claim_text 7 the computerimplemented method of claim 6 further comprising providing the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy forming an updated machine learning model configuration based on the updated machine learning training strategy and providing the updated machine learning model configuration and the sampled data to the machine learning platform claim_text 8 the computerimplemented method of claim 1 further comprises testing deployed machine learning model based on the machine learning model configuration accessing performance assessment of the deployed machine learning model and generating performance indicator of the deployed machine learning model based on the testing and the performance assessment determining that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises updating the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model claim_text 9 the computerimplemented method of claim 1 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device claim_text 10 the computerimplemented method of claim 1 wherein the machine learning training strategy comprises one of random forest classifier or gaussian process regressor claim_text 11 computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the apparatus to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform claim_text 12 the computing apparatus of claim 11 wherein the instructions further configure the apparatus to train using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitor computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data claim_text 13 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to generate an updated machine learning model configuration recommendation based on the resource usage data claim_text 14 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to receive from client device the training data the computing resource limit setting and parameters of the machine learning model and provide the resource usage data and the trained machine learning model to the client device claim_text 15 the computing apparatus of claim 12 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model claim_text 16 the computing apparatus of claim 12 wherein forming the machine learn training strategy comprises provide summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receive the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learn model and an accuracy of the machine learning model claim_text 17 the computing apparatus of claim 16 wherein the instructions further configure the apparatus to provide the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy form an updated machine learning model configuration based on the updated machine learning training strategy and provide the updated machine learning model configuration and the sampled data to the machine learning platform claim_text 18 the computing apparatus of claim 11 further comprises test deployed machine learning model based on the machine learning model configuration access performance assessment of the deployed machine learning model and generate performance indicator of the deployed machine learning model based on the testing and the performance assessment determine that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises update the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model claim_text 19 the computing apparatus of claim 11 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device claim_text 20 nontransitory computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time t res of the at least one compound in the chromatography bed such that equation i is satisfied t res t diff equation i calculating concentration cz t of the at least one compound in the mobile phase at predetermined location z of the chromatography bed and at predetermined time t based on the adsorption isotherm wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 2 the method according to claim 1 wherein the minimum residence time t res of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 3 the method according to claim 1 wherein in the calculation step cz t is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 4 the method according to claim 3 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 5 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 6 the method according to claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 7 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 8 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 9 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 10 the method according to claim 9 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 11 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 12 the chromatography method according to claim 11 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 13 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 9 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "030-076-623-736-233",
    "id2": "086-031-183-779-370",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set claim_text 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses the machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 wherein updating the machine learning model further comprises detect data deficit based on the performance of the machine learning model access from the datastore additional data that remedy the data deficit and form utilizing the machine learning algorithm another machine learning model based on the additional data and the task claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 computerreadable storage medium the computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring lang en",
    "claims2": "claims claim_text 1 an information processing device optimizing information presented by an information presenter to an information recipient comprising an attribute determination unit that determines knowledge level and an understanding level of the information recipient with respect to the information based on biological activity information of the information recipient acquired by sensor presentation information feature determination unit that determines feature of the information presented by the information presenter to the information recipient and feedback optimization unit that converts presentation format of the information into presentation format according to the knowledge level and the understanding level of the information recipient based on the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the feature of the information determined by the presentation information feature determination unit claim_text 2 the information processing device according to claim 1 wherein the presentation information feature determination unit determines the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation in case where the information is auditory information claim_text 3 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the visual recognition level based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 4 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the voice speed level based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 5 the information processing device according to claim 1 wherein the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient by the feedback optimization unit are presented to the information presenter claim_text 6 an information processing method optimizing information presented by an information presenter to an information recipient using an information processing device comprising first step in which knowledge level and an understanding level of the information recipient with respect to the information are determined based on biological activity of the information recipient acquired by sensor and feature of the information presented by the information presenter to the information recipient is determined and second step in which presentation format of the information is converted into presentation format according to the knowledge level and the understanding level of the information recipient based on the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the feature of the information claim_text 7 the information processing method according to claim 6 wherein in the first step the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation are determined in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation are determined in case where the information is auditory information claim_text 8 the information processing method according to claim 6 wherein in the first step the visual recognition level is determined based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level is determined based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 9 the information processing method according to claim 6 wherein in the first step the voice speed level is determined based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level is determined based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 10 the information processing method according to claim 6 further comprising third step in which the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient are presented to the information presenter lang en",
    "label": 0
  },
  {
    "id1": "167-129-411-617-157",
    "id2": "183-449-275-297-090",
    "claims1": "claims claim_text 1 machine learning ml computer system training student ml system iteratively through machine learning on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and following iterations of training of the student ml system the learned parameters the first node of the first inner layer are updated and the ml computer system comprises learning coach ml system that is in communication with the student ml system wherein the learning coach ml system comprises neural network that comprises an input layer and an output layer the learning coach ml system has been trained through machine learning to determine and implement an enhancement to the student ml system based on input to the learning coach ml system from the student ml system wherein the input from the student ml system is input to the input layer of the learning coach ml system the input to the learning coach ml system from the student ml system comprises observations about an internal state of the student ml system during training of the student ml system and the observations about the internal state of the student ml system comprise values related to the learned parameters the first node on the first inner layer of the student ml system and the activation values the first node on the first inner layer of the student ml system such that the learning coach ml system determines and implements the enhancement to the student ml system based on the input from the student ml system claim_text 2 the ml computer system of claim 1 wherein the learning coach ml system comprises pattern recognition system that recognizes different patterns than the student ml system claim_text 3 the ml computer system of claim 1 wherein the learning coach ml system has different objective than the student ml system claim_text 4 the ml computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 5 the ml system of claim 4 wherein the revised hyperparameter comprises hyperparameter selected from the group consisting of mini batch size the student ml system learning rate the student ml system regularization parameter the student ml system and momentum parameter the student ml system claim_text 6 the ml computer system of claim 1 wherein the enhancement comprises structural change to the student ml system claim_text 7 the ml computer system of claim 6 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 8 the ml computer system of claim 6 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 9 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 10 the ml computer system of claim 9 wherein the selected layer of the student ml system comprises plurality of existing nodes prior to the implementation of the enhancement and after the enhancement the existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the student ml system below the selected layer and activations of the virtual nodes are controlled by the learning coach ml system claim_text 11 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and learning coach ml system controls regularization to the second set of nodes so differences between activation values the second set of nodes and activation values the first set of nodes is less than threshold value to control dropout rate of the nodes in the first and second sets claim_text 12 the ml computer system of claim 8 wherein the learning coach ml system implements the one or more additional nodes by providing connection weights the one or more additional nodes of the student ml system claim_text 13 the ml computer system of claim 1 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 14 the ml computer system of claim 1 wherein the ml system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activation values the student ml system are stored in the high speed memory so that the student ml system can be run when the student ml system is active and the connection weights and activation values the student ml system are stored in the secondary storage when the student ml system is not active claim_text 15 the ml system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 16 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system regression claim_text 17 the ml computer system of claim 1 wherein the learning coach ml system models learning performance of the student ml system classification task claim_text 18 the ml computer system of claim 1 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and during training of the student ml system activation values each of the multiple feature node are softtied training data examples within the cluster claim_text 19 the ml computer system of claim 18 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 20 the ml computer system of claim 19 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 21 the ml computer system of claim 19 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 22 the ml computer system of claim 21 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 23 the ml computer system of claim 22 wherein the ml learning coach is configured to set the relaxation strength hyperparameters the feature nodes claim_text 24 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to an activation value of node in separate ml system claim_text 25 the ml computer system of claim 1 wherein during training of the student ml system an activation value from the first node of the student ml system is softtied to activation values of multiple nodes in separate ml system claim_text 26 the ml computer system of claim 25 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 27 the ml computer system of claim 26 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 28 the ml computer system of claim 27 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 29 the ml computer system of claim 28 wherein the learning coach ml system is configured to set the relaxation strength hyperparameter the first node claim_text 30 the ml computer system of claim 1 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the ml computer system further comprises data switching network that selectively directs training data examples the student ml system to one or more and less than all of the multiple ml ensemble members claim_text 31 the ml computer system of claim 30 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 32 the ml computer system of claim 30 further comprising classifier classifying the training data examples the student ml system and wherein the classifier controls the data switching network training data example based on classification by the classifier the training data example claim_text 33 machinelearning ml training method comprising initially training learning coach ml system to determine and implement an enhancement to ml system wherein the learning coach ml system comprises an input layer and an output layer training iteratively through machine learning student ml system on training data to perform machine learning task wherein the student ml system comprises an input layer an output layer and at least first inner layer between the input and output layers each layer comprises at least one node such that the first inner layer comprises at least first node the first node of the first inner layer outputs an activation value each set of input values to the first node of the first inner layer the activation value is determined based on an activation function the first node of the first inner layer and based on learned parameters the first node of the first inner layer and the initial training the student ml system comprises following iterations of the training of the student ml system updating the learned parameters the first node of the first inner layer and training the student ml system comprises receiving by the learning coach ml system from the student ml system values related to the learned parameters and the activation values of the first node of the first inner layer of the student ml system and determining and implementing by the learning coach ml system the enhancement to the student ml system based on the values related to the learned parameters and the activation values of the first node of the inner layer of the student ml system to improve performance of the student ml system claim_text 34 the ml training method of claim 33 wherein the enhancement comprises one or more revised hyperparameters the student ml system that improve learning by the student ml system claim_text 35 the ml training method of claim 33 wherein the enhancement comprises structural change to the student ml system claim_text 36 the ml training method of claim 35 wherein the structural change comprises one or more additional inners layers to be added to the student ml system between the input layer and the output layer of the student ml system claim_text 37 the ml training method of claim 35 wherein the structural change comprises one or more additional nodes to be added to selected layer of the one or more inner layers of the student ml system claim_text 38 the ml training method of claim 37 wherein the learning coach ml system implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the student ml system claim_text 39 the ml training method of claim 33 wherein the enhancement comprises selectively controlling training data input to the student ml system to control the learning of the student ml system claim_text 40 the ml training method of claim 33 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the student ml system claim_text 41 the ml training method of claim 33 wherein the student ml system comprises multiple feature nodes wherein the feature nodes comprise two or more nodes of the student ml system that are feature nodes cluster of data examples within category and training the student ml system further comprises softtying activation values each of the multiple feature nodes training data examples within the cluster claim_text 42 the ml training method of claim 41 wherein softtying the activation values each of the multiple feature nodes training data examples within the cluster comprises using regularization term in cost function each of the multiple feature nodes wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on difference between the activation value of the feature node the training data example and an average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 43 the ml training method of claim 42 wherein the regularization term each training data example in the cluster each of the multiple feature nodes is based on l2 norm of the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 44 the ml training method of claim 42 wherein the regularization term each of the multiple feature nodes comprises relaxation strength hyperparameter claim_text 45 the ml training method of claim 44 wherein the relaxation strength hyperparameter one of the feature nodes multiplies the difference between the activation value of the feature node the training data example and the average activation value across all of the multiple feature nodes all training data examples in the cluster claim_text 46 the ml training method of claim 45 further comprising setting by the ml learning coach the relaxation strength hyperparameters the feature nodes claim_text 47 the ml training method of claim 33 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to an activation value of node in separate ml system claim_text 48 the ml training method of claim 33 wherein training the student ml system comprises softtying an activation value from the first node of the student ml system to activation values of multiple nodes in separate ml system claim_text 49 the ml training method of claim 48 wherein softtying the activation value from the first node of the student ml system to the activation values from the multiple nodes in the separate ml system comprises using regularization term in cost function the first node in the student ml system first training data example wherein the regularization term is based on difference between the activation value the first node of the student ml system the first training data example and an average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 50 the ml training method of claim 49 wherein the regularization term the first node comprises relaxation strength hyperparameter claim_text 51 the ml training method of claim 50 wherein the relaxation strength hyperparameter multiplies the difference between the activation value of the first node the first training data example and the average activation value across all of the multiple nodes in the separate ml to which the first node is soft tied the first training data example claim_text 52 the ml training method of claim 51 further comprising setting by the learning coach ml system the relaxation strength hyperparameter the first node claim_text 53 the ml training method of claim 33 wherein the student ml system is an ensemble that comprises multiple ml ensemble members and the method further comprises selectively directing by data switching network training data examples the student ml system to one or more but less than all of the multiple ml ensemble members claim_text 54 the ml training method of claim 53 wherein the data switching network selectively directs the training data examples to the one or more of the multiple ml ensemble members based on control signals from the learning coach ml system claim_text 55 the ml training method of claim 53 further comprising classifying by classifier the training data examples the student ml system and controlling by the classifier the data switching network training data example based on classification by the classifier the training data example lang en",
    "claims2": "claims claim_text claims 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "label": 1
  },
  {
    "id1": "032-496-986-475-726",
    "id2": "038-794-758-325-674",
    "claims1": "claims claim_text computerimplemented method improving locality of machine learning models the method performed by data processing apparatus the method comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the method of claim 1 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the method of claim 3 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the method of one of claims 1 to 5 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text machine learning computations system comprisingn data processing apparatus and n memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the system of claim 7 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the system of claim 9 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the system of one of claims 7 to 11 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the nontransitory computer storage medium of claim 13 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 system training probabilistic predictive model recommending experiment designs synthetic biology comprising nontransitory memory configured to store executable instructions and hardware processor in communication with the nontransitory memory the hardware processor programmed by the executable instructions to receive synthetic biology experimental data generate training data from the synthetic biology experimental data wherein the training data comprise plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective train using the training data plurality of level0 learners of probabilistic predictive model recommending experiment designs synthetic biology wherein an input of each of the plurality of level0 learners comprises input values of the input variables and wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable and train using i predicted values of the at least one response variable determined using the plurality of level0 learners the training inputs of the plurality of training inputs and ii the reference outputs of the plurality of reference outputs correspondence to the training inputs of the plurality of training inputs level1 learner of the probabilistic predictive model recommending experiment designs synthetic biology comprising probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable claim_text 2 10 canceled claim_text 11 the system of claim 1 wherein the synthetic biology experimental data is sparse claim_text 12 the system of claim 1 wherein number of the plurality of training inputs in the synthetic biology experiment data is number of experimental conditions number of strains number of replicates of strain of the strains or combination thereof claim_text 13 15 canceled claim_text 16 the system of claim 1 wherein one or each of the plurality of input variables andor the at least one response variable comprises promoter sequence an induction time an induction strength ribosome binding sequence copy number of gene transcription level of gene an epigenetics state of gene level of protein post translation modification state of protein level of molecule an identity of molecule level of microbe state of microbe state of microbiome titer rate yield or combination thereof optionally wherein the molecule comprises an inorganic molecule an organic molecule protein polypeptide carbohydrate sugar fatty acid lipid an alcohol fuel metabolite drug an anticancer drug biofuel flavoring molecule fertilizer molecule or combination thereof claim_text 17 21 canceled claim_text 22 the system of claim 1 wherein the predetermined response variable objective comprises maximization objective minimization objective or specification objective andor wherein the predetermined response variable objective comprises maximizing the at least one response variable minimizing the at least one response variable or adjusting the at least one response variable to predetermined value of the at least one response variable claim_text 23 the system of claim 1 wherein to train the plurality of level1 learner the hardware processor is programmed by the executable instructions to determine using the plurality of level0 learners the predicted values of the at least one response variable training inputs of the plurality of training inputs claim_text 24 the system of claim 1 wherein the level1 learner comprises bayesian ensemble of the plurality of level0 learners claim_text 25 the system of claim 1 wherein parameters of the ensemble of the plurality of level0 learners comprises i plurality of ensemble weights and ii an error variable distribution of the ensemble or standard deviation of the error variable distribution of the ensemble claim_text 26 32 canceled claim_text 33 the system of claim 1 wherein to train the level1 learner the hardware processor is programmed by the executable instructions to determine posterior distribution of the ensemble parameters given the training data or the second subset of the training data wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to determine i probability distribution of the training data or the second subset of the training data given the ensemble parameters or likelihood function of the ensemble parameters given the training data of the second subset of the training data and ii prior distribution of the ensemble parameters and wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to sample space of the ensemble parameters with frequency proportional to desired posterior distribution claim_text 34 canceled claim_text 35 canceled claim_text 36 the system of claim 1 wherein to train the plurality of level0 learners the hardware processor is programmed by the executable instructions to generate first subset of the training data and train using the first subset of the training data the plurality of level0 learners claim_text 37 50 canceled claim_text 51 the system of claim 1 wherein the hardware processor is programmed by the executable instructions to determine surrogate function with an input experiment design an input the surrogate function comprising an expected value of the at least one response variable determined using the input experiment design variance of the value of the at least one response variable determined using the input experiment design and an exploitationexploration tradeoff parameter and determine using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of synthetic biology experiment obtaining predetermined response variable objective associated with the at least one response variable claim_text 52 57 canceled claim_text 58 the system of claim 51 wherein to determine the plurality of recommended experiment designs the hardware processor is programmed by the executable instructions to determine plurality of possible recommended experiment designs each comprising possible recommended values of the input variables with surrogate function values determined using the surrogate function with predetermined characteristic and select the plurality of recommended experiment designs from the plurality of possible recommended experiment designs using an input variable difference factor based on the surrogate function values of the plurality of possible recommended experiment designs claim_text 59 64 canceled claim_text 65 the system of claim 51 wherein number of the plurality of recommended experiment designs is number of experimental conditions or number of strains the next cycle of the synthetic biology experiment claim_text 66 canceled claim_text 67 canceled claim_text 68 the system of claim 51 wherein to determine the plurality of possible recommended experiment designs the hardware processor is programmed by the executable instructions to sample space of the input variables with frequency proportional to the surrogate function or an exponential function of the surrogate function and prior distribution of the input variables claim_text 69 canceled claim_text 70 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine an upper bound andor lower bound one or each of the plurality of input variables based on training values of the corresponding input variable wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 71 canceled claim_text 72 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to receive an upper bound andor lower bound one or each of the plurality of input variables wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 73 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability distribution of the at least one response variable one or each of the plurality of recommended experiment designs claim_text 74 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of one or each of the plurality of recommended experiment designs being predetermined percentage closer to achieving the objective relative to the training data claim_text 75 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective being predetermined percentage closer to achieving the objective relative to the training data claim_text 76 canceled claim_text 77 canceled claim_text 78 method recommending experiment designs synthetic biology comprising under control of hardware processor receiving probabilistic predictive model recommending experiment designs synthetic biology comprising plurality of level0 learners and level1 learner wherein an input of each of the plurality of level0 learners comprises input values of the input variables wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable wherein the level1 learner comprises probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable wherein the plurality of level0 learners and the level1 learner are trained using training data obtained from one or more cycles of synthetic biology experiment comprising plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective determining surrogate function comprising an expected value of the level1 learner variance of the level1 learner and an exploitationexploration tradeoff parameter and determining using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of the synthetic biology experiment achieving predetermined response variable objective associated with the at least one response variable claim_text 79 85 canceled lang en",
    "label": 0
  },
  {
    "id1": "020-925-613-909-867",
    "id2": "199-776-926-658-216",
    "claims1": "claims claim_text 1 canceled claim_text 2 computerimplemented method comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 3 the computerimplemented method of claim 2 further comprising receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 4 the computerimplemented method of claim 2 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 5 the computerimplemented method of claim 2 further comprising monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 6 the computerimplemented method of claim 2 further comprising in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 7 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 8 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 9 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 10 the computerimplemented method of claim 2 further comprising testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 11 the computerimplemented method of claim 2 wherein updating the machine learning model further comprises updating definition of task of the machine learning model accessing from datastore additional data based on the updated definition of the task and forming second machine learning model based on the additional data and the updated definition of the task claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 13 the computing apparatus of claim 12 wherein the operations further comprise receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 14 the computing apparatus of claim 12 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 15 the computing apparatus of claim 12 wherein the operations further comprise monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 16 the computing apparatus of claim 12 wherein the operations further comprise in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 17 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 18 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 19 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 20 the computing apparatus of claim 12 wherein the operations further comprise testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 21 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by server cause the server to perform operations comprising accessing by one or more processors of the server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data lang en",
    "claims2": "claims claim_text claims what is claimed is 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space lang en",
    "label": 1
  },
  {
    "id1": "022-386-433-016-642",
    "id2": "034-722-477-434-593",
    "claims1": "claims claim_text 1 method in data processing system comprising at least one processor and at least one memory the at least one memory comprising instructions executed by the at least one processor to cause the at least one processor to implement machine learning framework wherein the machine learning framework operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 2 the method of claim 1 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 3 the method of claim 1 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 4 the method of claim 1 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 5 the method of claim 4 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 6 the method of claim 1 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 7 the method of claim 1 wherein the machine learning framework further operates to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 8 computer program product comprising computer readable storage medium having computer readable program stored therein wherein the computer readable program when executed on computing device causes the computing device to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 9 the computer program product of claim 8 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 10 the computer program product of claim 8 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 11 the computer program product of claim 8 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 12 the computer program product of claim 11 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 13 the computer program product of claim 8 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 14 the computer program product of claim 8 wherein the machine learning framework is further configured to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 15 an apparatus comprising processor and memory coupled to the processor wherein the memory comprises instructions which when executed by the processor cause the processor to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operate to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 16 the apparatus of claim 15 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in trained machine learning model index data storage trained machine learning model metadata model each trained machine learning algorithm in the plurality of trained machine learning models claim_text 17 the apparatus of claim 15 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository lang en",
    "claims2": "claims claim_text 1 computer system configuring plurality of machine learning pipelines into machine learning pipeline ensemble the system comprising one or more computer processors one or more computer readable storage media computer program instructions the computer program instructions being stored on the one or more computer readable storage media execution by the one or more computer processors and the computer program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 2 the computer system of claim 1 wherein the plurality of machine learning pipelines are heterogeneous claim_text 3 the computer system of claim 1 wherein the instructions the reinforcement learning agent coupled to the machine learning pipeline to adjust the configuration values of the coupled machine learning pipeline further include instructions to receive performance information associated with the uncoupled machine learning pipeline determine an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjust the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 4 the computer system of claim 3 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 5 the computer system of claim 4 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 6 the computer system of claim 1 further comprising instructions to determine similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merge the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 7 the computer system of claim 3 further comprising instructions to generate new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 8 the computer system of claim 1 wherein the configuration parameter values of the coupled and uncoupled machine learning pipelines include at least one value selected from the group consisting of training dataset learning environment machine learning pipeline structure an objective function and hyperparameter set claim_text 9 the computer system of claim 1 wherein performance information of the uncoupled machine learning pipeline includes at least one performance metric selected from the group consisting of prediction accuracy value prediction accuracy value drift diversity of predictions running time and an entropy value claim_text 10 the computer system of claim 1 further comprising program instructions to generate machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 11 the computer system of claim 1 further comprising instructions to generate the plurality of machine learning pipelines from plurality of input datasets and couple each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent claim_text 12 computer program product configuring plurality of machine learning pipelines into machine learning pipeline ensemble the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media the program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 13 computerimplemented method configuring plurality of machine learning pipelines into machine learning pipeline ensemble the method comprising determining by reinforcement learning agent coupled to machine learning pipeline performance information of the machine learning pipeline receiving by the reinforcement learning agent configuration parameter values of uncoupled machine learning pipelines of the plurality of machine learning pipelines and adjusting by the reinforcement learning agent configuration parameter values of the machine learning pipeline based on the performance information of the machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipelines claim_text 14 the computerimplemented method of claim 13 further comprising receiving performance information associated with the uncoupled machine learning pipeline determining an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjusting the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 15 the computerimplemented method of claim 14 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 16 the computerimplemented method of claim 15 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 17 the computerimplemented method of claim 13 further comprising determining similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merging the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 18 the computerimplemented method of claim 14 further comprising generating new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 19 the computerimplemented method of claim 13 further comprising generating machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 20 the computerimplemented method of claim 13 further comprising generating the plurality of machine learning pipelines from plurality of input datasets and coupling each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent lang en",
    "label": 1
  },
  {
    "id1": "022-568-943-881-422",
    "id2": "051-909-779-474-634",
    "claims1": "claims claim_text wireless communication method based on machine learningartificial intelligence by user equipment ue comprising maintaining one or more machine learning models by one or more tables lists or groups machine learning based on machine learning and performing machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 1 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 2 or 3 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 4 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 5 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the ue performs the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the ue performs the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 6 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 7 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 8 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 9 wherein the machine learning model switching comprises at least one of followings machine learning new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 10 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 11 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 12 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 1 to 15 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to claim 16 or 17 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 19 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 21 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the ue according to any one of claims 16 to 22 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text wireless communication method based on machine learningartificial intelligence by base station comprising maintaining or managing one or more machine learning models by one or more tables lists or groups based on machine learning and controlling user equipment ue to perform machine learning model updating or machine learning model switching wherein the machine learning model updating or the machine learning model switching comprises replacement of first machine learning model by second machine learning model and the machine learning model updating is based on one or more machine learning model performance conditions or an indication from first base station explicitly or implicitly or from ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 24 wherein the machine learning models are maintained in groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 wherein the machine learning models grouped based on one or any combination of the following factors channel model channel parameters signal to noise ratio snr range model complexity ue capability modulation order rank bandwidth part bwp size delay spread doppler frequency shift antenna ports andor an antenna geometry claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 25 or 26 wherein grouping of the machine learning models is indicated to the ue by downlink control information dci medium access control mac control element ce radio resource control rrc signaling bitmap or multiplelevel bitmap claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 27 wherein the machine learning models in the table or the list based on machine learning models are updated based on usage frequencies of the machine learning models performances of the machine learning models andor the ue request claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 28 wherein the one or more machine learning model performance conditions comprise if machine learning model performance is higher than first value once the first base station controls the ue to perform the machine learning model updating if the machine learning model performance is higher than the first value and if number of occurrences of the first value exceeds maximum number of times configured by the first base station the first base station controls the ue to perform the machine learning model updating or if the ue enters new scenario and new machine learning model is appliedfunctioningto be deployed in the new scenario the ue performs the machine learning model updating claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 29 wherein the indication from the first base station comprises macce an rrc signaling or dci field used to indicate an identifier id aname of the second machine learning model andor one or more levels or bitmaps to retrieve one or more second machine learning models in group claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 30 wherein the replacement of the first machine learning model by the second machine learning model comprises replacement of current machine learning model by new machine learning model replacement of the current machine learning model by backup machine learning model falling back to conventional processing method comprising conventional codebook type i or conventional codebook type ii or switching to relative general model obtained from machine learning claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 31 wherein the machine learning model updating comprises the replacement of the first machine learning model by the second machine learning model in same group and the machine learning model switching comprises the replacement of the first machine learning model by the second machine learning model in different groups claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 32 wherein the machine learning model switching comprises at least one of followings new rrc entries at csi report configuration are defined the machine learning models to indicate selection of one machine learning model the csi report if the new rrc entries are configured both the new machine learning model and the current machine learning model two report settings the reporting is switched to that using the new machine learning model by the first base station using macce or dci field if the csi report configuration the new machine learning model is not preconfigured new signaling of new dci field andor macce is defined to activate the new machine learning model the new codebook and report the csi based on the new codebook if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to conventional codebook comprising codebook type i codebook type ii or relative general machine learning model with flag in an uplink control information uci indicating the type of the codebook and an rrc configuration the deployment of the new machine learning model is initiated or if the csi report configuration the new machine learning model is not preconfigured and the new signaling is not defined the ue fallbacks to predefined codebook comprising the codebook type i the codebook type ii or the relative general machine learning model and an rrc configuration the deployment of the new machine learning model is initiated claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 33 wherein if the rrc reconfiguration is initiated and the new codebook is not predownloaded at the ue the downloading process are initiated by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 34 wherein the machine learning model of the ue is downloaded via data channel from the first base station or the machine learning model is downloaded from third node claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the ue considers an environment around the ue is changed the ue requests the second base station to change the machine learning model and the request is in the uci claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set and the second base station informs the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 35 wherein when the ue is handed over from the first base station to second base station if the first base station or the second base station considers an environment around the ue is changed the first base station and the second base station are able to judge that data collected by the first base station and the second base station are not contributed to same training data set the second base station informs the first base station to inform the ue to prearrange the new machine learning model by dci macce or an rrc signaling claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 24 to 38 wherein the machine learning model switching comprises time window beginning from an indication of switching to new codebookmachine learning model and ending at time when the new codebookmachine learning model begins to function and the time window is switching time claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 wherein size of the time window is related to the ue ability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from receipt of physical downlink control channel pdcch indicating the machine learning model switching and if csi report falls into the switching time the csi report is processed by the current machine learning model which is in use before the receiving of the pdcch claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to claim 39 or 40 wherein if beginning time of the switching time is accounted from the receiving of the pdcch indicating the machine learning model switching and if reporting time of report is larger than the switching time the report is obtained using new machine learning model claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 42 wherein the switching time is configured by the first base station to the ue or portion of the switching time is configured by the first base station to the ue claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 32 wherein the switching time is reported by the ue ue capability or portion of the switching time is reported by the ue the ue capability claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 44 wherein the switching time comprises dcimacce processing time plus data processing time of an input of the current machine learning model or the switching time is value reported by the ue or configured by the first base station claim_text the wireless communication method based on machine learningartificial intelligence by the first base station according to any one of claims 39 to 45 wherein an oldlegacy machine learning model continues to take effects until the switching time ends or the oldlegacy machine learning model falls back to conventional codebook type i or conventional codebook type ii and the conventional codebook type i or the conventional codebook type ii continues to take effects until the switching time ends claim_text user equipment ue comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 1 to 23 claim_text base station comprising memory transceiver and processor coupled to the memory and the transceiver wherein the processor is configured to execute the method of any one of claims 24 to 46 claim_text nontransitory machinereadable storage medium having stored thereon instructions that when executed by computer cause the computer to perform the method of any one of claims 1 to 46 claim_text chip comprising processor configured to call and run computer program stored in memory to cause device in which the chip is installed to execute the method of any one of claims 1 to 46 claim_text computer readable storage medium in which computer program is stored wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program product comprising computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46 claim_text computer program wherein the computer program causes computer to execute the method of any one of claims 1 to 46",
    "claims2": "claims claim_text 1 method of diagnosing whether subject has depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the one or more biomarkers comprise levels of the one or more biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the one or more biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the one or more biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 010 claim_text 8 the method of claim 1 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 9 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of two or more biomarkers selected from tables 1 and 2 claim_text 10 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of three or more biomarkers selected from tables 1 and 2 claim_text 11 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of four or more biomarkers selected from tables 1 and 2 claim_text 12 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of five or more biomarkers selected from tables 1 and 2 claim_text 13 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of ten or more biomarkers selected from tables 1 and 2 claim_text 14 the method of claim 1 wherein the method comprises analyzing the biological sample to determine the level of fifteen or more biomarkers selected from tables 1 and 2 claim_text 15 the method of claim 1 wherein the biological sample is blood plasma claim_text 16 the method of claim 1 wherein the sample is analyzed using one or more techniques selected from the group consisting of mass spectrometry elisa and antibody linkage claim_text 17 method of determining whether subject is predisposed to developing depression comprising analyzing biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 and 2 and comparing the levels of the one or more biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the one or more biomarkers in order to determine whether the subject is predisposed to developing depression claim_text 18 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression in the sample wherein the one or more biomarkers are selected from tables 1 2 and 3 and the first sample is obtained from the subject at first time point analyzing second biological sample from subject to determine the levels of the one or more biomarkers wherein the second sample is obtained from the subject at second time point and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 19 the method of claim 18 wherein the method further comprises comparing the levels of one or more biomarkers in the first sample the levels of one or more biomarkers in the second sample andor the results of the comparison of the levels of the one or more biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 20 method of assessing the efficacy of composition treating depression comprising analyzing from subject having depression and currently or previously being treated with composition biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers in the sample to levels of the one or more biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the one or more biomarkers c depressionnegative reference levels of the one or more biomarkers d depressionprogressionpositive reference levels of the one or more biomarkers andor depressionregressionpositive reference levels of the one or more biomarkers claim_text 21 the method of claim 20 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 22 method assessing the efficacy of composition in treating depression comprising analyzing first biological sample from subject to determine the levels of one or more biomarkers depression the first sample obtained from the subject at first time point wherein the one or more biomarkers are selected from tables 1 2 and 3 administering the composition to the subject analyzing second biological sample from the subject to determine the levels of the one or more biomarkers the second sample obtained from the subject at second time point after administration of the composition and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 23 the method of claim 22 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 24 method of assessing the relative efficacy of two or more compositions treating depression comprising analyzing from first subject having depression and currently or previously being treated with first composition first biological sample to determine the levels of one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 analyzing from second subject having depression and currently or previously being treated with second composition second biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the first sample to the levels of the one or more biomarkers in the second sample in order to assess the relative efficacy of the first and second compositions treating depression claim_text 25 the method of claim 24 further comprising analyzing from third subject having depression and currently or previously being treated with third composition third biological sample to determine the levels of the one or more biomarkers and comparing the levels of one or more biomarkers in the third sample to the levels of the one or more biomarkers in the first and second samples in order to assess the relative efficacy of the first second and third compositions treating depression claim_text 26 the method of claim 24 wherein the one or more biomarkers are selected from those biomarkers in tables 1 and 2 having p values of less than 005 claim_text 27 method screening composition activity in modulating one or more biomarkers of depression comprising contacting one or more cells with composition analyzing at least portion of the one or more cells or biological sample associated with the cells to determine the levels of one or more biomarkers of depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and comparing the levels of the one or more biomarkers with predetermined standard levels the biomarkers to determine whether the composition modulated the levels of the one or more biomarkers claim_text 28 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in the one or more cells in the absence of the composition claim_text 29 the method of claim 27 wherein the predetermined standard levels the biomarkers are levels of the one or more biomarkers in one or more control cells not contacted with the composition claim_text 30 the method of claim 27 wherein the method is conducted in vivo claim_text 31 the method of claim 27 wherein the method is conducted in vitro claim_text 32 method identifying potential drug target depression comprising identifying one or more biochemical pathways associated with one or more biomarkers depression wherein the one or more biomarkers are selected from tables 1 2 and 3 and identifying protein affecting at least one of the one or more identified biochemical pathways the protein being potential drug target depression claim_text 33 method treating subject having depression comprising administering to the subject an effective amount of one or more biomarkers selected from table 1 that are decreased in subjects having depression compared to subjects not having depression lang en",
    "label": 0
  },
  {
    "id1": "079-134-842-572-348",
    "id2": "163-517-312-183-954",
    "claims1": "claims claim_text 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising or replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing the library of dataset from the datastore identifying based on the task and the dataset additional data from the library of dataset and augmenting the dataset with the additional data claim_text 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space claim_text 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 6 the computerimplemented method of claim 1 wherein the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test claim_text 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set claim_text 11 the computerimplemented method of claim 1 wherein detecting the data deficit comprises identifying invalid data types and outlier values wherein the additional data further comprise approximated values corresponding to the outliers values corrected data types corresponding to the invalid data types and the approximated values comprise similar data points from the other dataset claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server accessing look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identifying using the look up table the machine learning tool corresponding to the type of task based on the definition of the task forming utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task claim_text 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access the library of dataset from the datastore identify based on the task and the dataset additional data from the library of dataset and augment the dataset with the additional data claim_text 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset claim_text 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space claim_text 16 the computing apparatus of claim 12 wherein monitoring further comprises test the machine learning model receive from the application performance assessment of the machine learning model and generate performance indicator of the machine learning model based on the testing and the performance assessment determine that the performance indicator of the machine learning model transgresses machine learning model performance threshold and in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold update the machine learning model wherein updating the machine learning model further comprises update the dataset update the definition of the task based on the performance indicator of the machine learning model and form utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset claim_text 17 the computing apparatus of claim 12 the machine learning platform comprises target specific feature discovery system coupled to data ingestion system and target specification system the target specific feature discovery system comprising spectral signal embedding system userspecified signal generation system feature reduction system feature set optimization system the spectral signal embedding system comprising plurality of signal scanning systems and information embedding systems each signal scanning system and information embedding system corresponding to feature of the dataset claim_text 18 the computing apparatus of claim 12 wherein updating the machine learning model further comprises update the definition of the task access from the datastore additional data based on the updated definition of the task and form utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset claim_text 19 the computing apparatus of claim 12 wherein the instructions further configure the computing apparatus to perform operations comprising identify features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals train the machine learning model based on the identified features and generate using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time claim_text 20 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising receive selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal perform back test the selected signal using the trained machine learning model and validate the trained machine learning model based on results from the back test claim_text 21 the computing apparatus of claim 19 wherein the instructions further configure the computing apparatus to perform operations comprising access feature set of the dataset form an augmented feature set from the feature set measure dependencies of the augmented feature set each target of target set and generate single feature each target of the target set claim_text 22 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by computer cause the computer to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server access look up table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool identify using the look up table the machine learning tool corresponding to the type of task based on the definition of the task form utilizing the machine learning algorithm machine learning model based on the dataset the task and the machine learning tool deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring by detecting data deficit based on the performance of the machine learning model wherein detecting the data deficit comprises identifying missing values and frequency of the missing values by analyzing statistics of the dataset identifying source of the data deficit faulty sensor providing the data in response to detecting the data deficit generating an internal action by accessing without user intervention from the datastore additional data that remedy the data deficit the additional data comprising replacement data from another dataset of library of dataset with similar properties to the dataset adapting the additional data to match statistical properties of the dataset and updating the machine learning model based on the adapted additional data and the task lang en",
    "claims2": "claims claim_text 1 method comprising initiating active machine learning through an active machine learning system configured to train an auxiliary machine learning model to produce at least one new labeled observation refining target machine learning model based at least on the active machine learning wherein the target machine learning model includes limitedcapacity machine learning model and retraining the auxiliary machine learning model with the at least one new labeled observation subsequent to refining the capacity of the target machine learning model claim_text 2 method claim 1 recites wherein the auxiliary machine learning model includes capacity larger than the target machine learning model claim_text 3 method claim 1 recites wherein the auxiliary machine learning model includes semantic machine learning model claim_text 4 method claim 3 recites wherein the semantic machine learning model includes bagofwords machine learning model claim_text 5 method claim 1 recites wherein initiating the active machine learning comprises selecting one or more unlabeled observations from pool of unlabeled observations claim_text 6 method claim 5 recites wherein the refining the capacity includes incrementally adding or removing features from the target machine learning model based on an output of the auxiliary machine learning model responsive to processing the one or more unlabeled observations claim_text 7 method claim 1 recites wherein the refining the capacity includes incrementally adding or removing features from the target machine learning model based at least on the initiated active machine learning of the auxiliary machine learning model claim_text 8 method claim 1 recites further comprising implementing diversity in the initiated active machine learning by at least one submodular function claim_text 9 method claim 1 recites further comprising implementing diversity in the initiated active machine learning by establishing subset labelset in pool of unlabeled observations configured to provide diverse unlabeled observations from the subset labelset claim_text 10 method claim 9 recites further comprising selecting one or more unlabeled observations from the subset labelset processing by the auxiliary machine learning model claim_text 11 method claim 1 recites further comprising reducing colorblindness of the target machine learning model based at least on disagreement between the auxiliary machine learning model and the target machine learning model claim_text 12 computerreadable medium having computerexecutable instructions thereupon that when executed by computer cause the computer to perform operations comprising selecting an unlabeled observation from pool of unlabeled observations through an auxiliary machine learning model wherein it is not known to which one of plurality of classes the unlabeled observation belongs converting the unlabeled observation to new labeled observation based on an output of the auxiliary machine learning model responsive to the unlabeled observation refining capacity of target machine learning model based on the converting wherein the target machine learning model is limitedcapacity machine learning model and retraining the auxiliary machine learning model with the new labeled observation subsequent to refining the capacity of the target machine learning model claim_text 13 computerreadable medium claim 12 recites wherein the refining the capacity includes incrementally adding at least one feature to the target machine learning model based on features contained within the new labeled observation and incrementally removing at least one feature from the target machine learning model based on the features contained within the new labeled observation claim_text 14 computerreadable medium claim 12 recites wherein the selecting the unlabeled observation includes selecting the unlabeled observations based at least on optimization of at least one submodular function claim_text 15 computerreadable medium claim 12 recites wherein the selecting the unlabeled observation includes selecting the unlabeled observations from subset labelset in the pool of unlabeled observations the subset labelset configured to provide diverse unlabeled observations claim_text 16 computerreadable medium claim 12 recites wherein the refining the capacity of the target machine learning model includes reducing colorblindness of the target machine learning model based at least on disagreement between the auxiliary machine learning model and the target machine learning model claim_text 17 an active machine learning system the system comprising an auxiliary machine learning model configured to assign first score to an unlabeled observation target machine learning model configured to assign second score to the unlabeled observation wherein the target machine learning model and the auxiliary machine learning model are from different machine learning model classes and wherein the target machine learning model is limitedcapacity machine learning model comparison component configured to compare the first score and the second score to determine probability that the target machine learning model has returned false positive or false negative result and featuring component configured to receive the output of the comparison component claim_text 18 system claim 17 recites wherein the comparison component configured to compare the first score and the second score is further configured to perform comparison comprising determining magnitude of the difference between the first score and the second score determining that the target machine learning model has returned false positive when the magnitude is negative and determining that the target machine learning model has returned false negative when the magnitude is positive claim_text 19 system claim 18 recites further comprising capacityrefining component in operative communication with the featuring component the capacityrefining component configured to extend scope of the target machine learning model to include new feature previously not within the scope of the target machine learning model when the target machine learning model has returned false positive claim_text 20 system claim 18 recites further comprising capacityrefining component in operative communication with the featuring component the capacityrefining component configured to narrow scope of the target machine learning model to remove feature previously within the scope of the target machine learning model when the target machine learning model has returned false positive lang en",
    "label": 1
  },
  {
    "id1": "130-984-086-698-559",
    "id2": "034-722-477-434-593",
    "claims1": "claims claim_text 1 computerimplemented method of training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 2 the method of claim 1 wherein the first machine learning task and the second machine learning task are different supervised learning tasks claim_text 3 the method of claim 1 wherein the first machine learning task and the second machine learning tasks are different reinforcement learning tasks claim_text 4 the method of claim 1 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the objective function claim_text 6 the method of claim 4 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text 7 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text 8 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text 9 the method of claim 1 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text 13 the method of claim 4 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response claim_text 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text 15 the method of claim 1 the method further comprising providing the trained machine learning model use in processing data after training the machine learning model on the second machine learning task claim_text 16 the method of claim 1 wherein the first and second machine learning tasks each comprise reinforcement learning task and wherein the reinforcement learning task is controlling an agent to interact with an environment to achieve goal claim_text 17 the method of claim 1 wherein the first and second machine learning tasks each comprise classification task and wherein the classification task is processing data to classify the data claim_text 18 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 19 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task lang en",
    "claims2": "claims claim_text 1 computer system configuring plurality of machine learning pipelines into machine learning pipeline ensemble the system comprising one or more computer processors one or more computer readable storage media computer program instructions the computer program instructions being stored on the one or more computer readable storage media execution by the one or more computer processors and the computer program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 2 the computer system of claim 1 wherein the plurality of machine learning pipelines are heterogeneous claim_text 3 the computer system of claim 1 wherein the instructions the reinforcement learning agent coupled to the machine learning pipeline to adjust the configuration values of the coupled machine learning pipeline further include instructions to receive performance information associated with the uncoupled machine learning pipeline determine an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjust the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 4 the computer system of claim 3 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 5 the computer system of claim 4 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 6 the computer system of claim 1 further comprising instructions to determine similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merge the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 7 the computer system of claim 3 further comprising instructions to generate new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 8 the computer system of claim 1 wherein the configuration parameter values of the coupled and uncoupled machine learning pipelines include at least one value selected from the group consisting of training dataset learning environment machine learning pipeline structure an objective function and hyperparameter set claim_text 9 the computer system of claim 1 wherein performance information of the uncoupled machine learning pipeline includes at least one performance metric selected from the group consisting of prediction accuracy value prediction accuracy value drift diversity of predictions running time and an entropy value claim_text 10 the computer system of claim 1 further comprising program instructions to generate machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 11 the computer system of claim 1 further comprising instructions to generate the plurality of machine learning pipelines from plurality of input datasets and couple each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent claim_text 12 computer program product configuring plurality of machine learning pipelines into machine learning pipeline ensemble the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media the program instructions including instructions reinforcement agent coupled to machine learning pipeline of the plurality of machine learning pipelines to determine performance information associated with the coupled machine learning pipeline receive configuration parameter values from an uncoupled machine learning pipeline of the plurality of machine learning pipelines and adjust configuration parameter values of the coupled machine learning pipeline based at least in part on the performance information of the coupled machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipeline claim_text 13 computerimplemented method configuring plurality of machine learning pipelines into machine learning pipeline ensemble the method comprising determining by reinforcement learning agent coupled to machine learning pipeline performance information of the machine learning pipeline receiving by the reinforcement learning agent configuration parameter values of uncoupled machine learning pipelines of the plurality of machine learning pipelines and adjusting by the reinforcement learning agent configuration parameter values of the machine learning pipeline based on the performance information of the machine learning pipeline and the configuration parameter values of the uncoupled machine learning pipelines claim_text 14 the computerimplemented method of claim 13 further comprising receiving performance information associated with the uncoupled machine learning pipeline determining an overall system performance of the plurality of machine learning pipelines based at least in part on the performance information of the coupled machine learning pipeline and the performance information of the uncoupled machine learning pipeline and readjusting the configuration parameter values of the coupled machine learning pipeline based on the overall system performance claim_text 15 the computerimplemented method of claim 14 wherein readjusting the configuration parameter values of the coupled machine learning pipeline is further based on the overall system performance compared to desired collective system performance claim_text 16 the computerimplemented method of claim 15 wherein the desired collective system performance includes at least one performance metric selected from the group consisting of collaborative behavior competitive behavior and mixed competitivecollaborative behavior claim_text 17 the computerimplemented method of claim 13 further comprising determining similarity value between the coupled machine learning pipeline and the uncoupled machine learning pipeline based at least in part on performance information and configuration parameter values of the coupled and uncoupled machine learning pipelines and merging the coupled machine learning pipeline and the uncoupled machine learning pipeline responsive to determining that the similarity value associated with the coupled and uncoupled machine learning pipelines exceeds predetermined threshold value claim_text 18 the computerimplemented method of claim 14 further comprising generating new machine learning pipeline responsive to determining that the overall system performance indicates uncovered prediction settings claim_text 19 the computerimplemented method of claim 13 further comprising generating machine learning pipeline ensemble by combining the coupled and uncoupled machine learning pipelines claim_text 20 the computerimplemented method of claim 13 further comprising generating the plurality of machine learning pipelines from plurality of input datasets and coupling each machine learning pipeline in the plurality of machine learning pipelines with respective reinforcement learning agent lang en",
    "label": 1
  },
  {
    "id1": "190-820-803-645-579",
    "id2": "022-386-433-016-642",
    "claims1": "claims claim_text 1 machine learning computer system comprising first student machine learning system that comprises deep neural network that comprises one or more inner layers wherein the first student machine learning system using machine learning automatically learns from and make predictions on input source data and first learning coach machine learning system that is in communication with the first student machine learning system wherein input to the first learning coach machine learning system comprises values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning system and the first learning coach machine learning system using machine learning automatically learns and implements an enhancement to the first student machine learning system based on the values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning system to improve operation of the first student machine learning system claim_text 2 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes different patterns than the first student machine learning system claim_text 3 the machine learning computer system of claim 1 wherein the first learning coach machine learning system has different objective than the first student machine learning system claim_text 4 the machine learning computer system of claim 1 wherein the values related to learned parameters and activation values comprise observed andor computed values claim_text 5 the machine learning computer system of claim 1 wherein the first learning coach machine learning system comprises machine learning architecture that is not deep neural network claim_text 6 the machine learning computer system of claim 1 wherein the enhancement comprises one or more revised hyperparameters the deep neural network of the first student machine learning system that improve learning by the deep neural network of the first student machine learning system claim_text 7 10 canceled claim_text 11 the machine learning computer system of claim 1 wherein the enhancement comprises structural change to the deep neural network of the first student machine learning system claim_text 12 the machine learning computer system of claim 11 wherein the deep neural network of the first student machine learning system comprises multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional nodes to be added to selected layer of the deep neural network of the first student machine learning system wherein the selected layer is one of the one or more inner layers of the deep neural network of the first student machine learning system claim_text 13 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing set of virtual nodes and activation levels the virtual nodes associated with particular set of data input values to the first student machine learning system claim_text 14 the machine learning computer system of claim 13 wherein existing nodes in the selected layer receive inputs from the virtual nodes the existing nodes not backpropagate to virtual nodes during training the virtual nodes backpropagate to layer of the deep neural network of the first student machine learning system below the selected layer and activations of the virtual nodes are controlled by the first learning coach machine learning system claim_text 15 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing first and second sets of nodes to be added to the selected layer wherein the first and second sets consist of the same number of nodes and first learning coach machine learning system controls regularization to the second set of nodes so that activations the second set of nodes tend to agree with the first set of nodes to control dropout rate of the nodes in the first and second sets claim_text 16 canceled claim_text 17 the machine learning computer system of claim 12 wherein the learning coach machine implements the one or more additional nodes by providing connection weights the additional nodes of the deep neural network of the first student machine learning system claim_text 18 the machine learning computer system of claim 11 wherein the deep neural network of the first student machine learning system comprises network with multiple layers wherein each layer comprises one or more nodes and the structural change comprises one or more additional layers to be added to the deep neural network of the first student machine learning system claim_text 19 the machine learning computer system of claim 1 wherein the enhancement comprises selectively controlling training data input to the first student machine learning system to control the learning of the deep neural network of the first student machine learning system claim_text 20 the machine learning computer system of claim 1 wherein the machine learning system comprises computer network that comprises first computer system that comprises at least one processor and highspeed memory and remote secondary storage that is in communication with the first computer system connection weights and activations the first student machine learning system are stored in the high speed memory so that the first student machine learning system can be run when the first student machine learning system is active and the connection weights and activations the first student machine learning system are stored in the secondary storage when the first student machine learning system is not active claim_text 21 24 canceled claim_text 25 machine learning computer system comprising first set of one or more processing cores first set of one or more computer readable media first student machine learning module maintained on the first set of one or more computer readable media that when executed by the first set of one or more processing cores causes the first set of one or more processing cores to using machine learning automatically learn from and make predictions on input source data wherein the first student machine learning module comprises deep neural network that comprises one or more inner layers second set of one or more processing cores second set of one or more computer readable media and first learning coach machine learning module maintained on the second set of computer readable media that when executed by the second set of one or more processing cores causes the second set of one or more processing cores to receive input values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning module and using machine learning automatically learn and implement change to the first student machine learning module based on the values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning module to improve operation of the first student machine learning module claim_text 26 the machine learning computer system of claim 4 wherein the computed values comprise partial derivatives the nodes of the one or more inner layers of the deep neural network of the first student machine learning system claim_text 27 35 canceled claim_text 36 method of improving operation of first student machine learning system that using machine learning automatically learns from and makes predictions on input source data wherein the first student machine learning module comprises deep neural network that comprises one or more inner layers the method comprising receiving by first learning coach machine learning system from the first student machine learning system values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning system and using machine learning automatically learning and implementing by the first learning coach machine learning system an enhancement to the first student machine learning system based on the values related to learned parameters and activation values of nodes of the one or more inner layers of the deep neural network of the first student machine learning system to improve operation of the first student machine learning system claim_text 37 43 canceled claim_text 44 the machine learning system of claim 1 wherein the first learning coach machine learning system comprises pattern recognition system that recognizes patterns of learning performance of machine learning system claim_text 45 the machine learning system of claim 1 wherein the learned parameters comprise connection weights and biases the nodes of the one or more inner layers of the deep neural network of the first student machine learning system lang en",
    "claims2": "claims claim_text 1 method in data processing system comprising at least one processor and at least one memory the at least one memory comprising instructions executed by the at least one processor to cause the at least one processor to implement machine learning framework wherein the machine learning framework operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 2 the method of claim 1 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 3 the method of claim 1 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 4 the method of claim 1 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 5 the method of claim 4 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 6 the method of claim 1 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 7 the method of claim 1 wherein the machine learning framework further operates to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 8 computer program product comprising computer readable storage medium having computer readable program stored therein wherein the computer readable program when executed on computing device causes the computing device to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operates to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 9 the computer program product of claim 8 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in the trained machine learning model index data storage trained machine learning model metadata models each trained machine learning algorithm in the plurality of trained machine learning models claim_text 10 the computer program product of claim 8 wherein the machine learning framework further operates to determine each machine learning algorithm in the plurality of machine learning algorithms in the machine learning algorithm repository degree of matching of the machine learning algorithm to the one or more machine learning algorithm search criteria wherein the output of the information describing the at least one matching machine learning algorithm comprises ranked listing of the at least one matching machine learning algorithm based on the degree of matching each matching machine learning algorithm in the at least one matching machine learning algorithm claim_text 11 the computer program product of claim 8 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository claim_text 12 the computer program product of claim 11 wherein the plurality of universal apis comprises at least one of data collections api that provides computer logic forming logical collections of data samples associating features vectors with data samples and collections and associating output labels generated by trained machine learning model with data collections training api that provides computer logic training machine learning model using the selected machine learning algorithm describing the trained machine learning model generated by the training api and associating the trained machine learning model with data collection used to train the trained machine learning model prediction api that provides computer logic classifying new data instances based on previously trained machine learning model and searching previously trained machine learning models crossvalidation api that provides computer logic enabling selection of datasets testing trained machine learning model or clustering api that provides computer logic performing clustering of feature vectors based on vector similarity measure claim_text 13 the computer program product of claim 8 wherein the machine learning algorithm metadata model comprises description of aggregate data collections associated with the machine learning algorithm description of features generated by the machine learning algorithm from the data collections description of the machine learning model generated by the machine learning algorithm and provenance information about the machine learning algorithm claim_text 14 the computer program product of claim 8 wherein the machine learning framework is further configured to generate in response to at least one matching trained machine learning model being identified in the search of the trained machine learning model index data storage the analytics pipeline by integrating the at least one matching trained machine learning model into at least one stage of the analytics pipeline and train in response to the search of the machine learning algorithm index data storage identifying at least one matching machine learning algorithm machine learning model by executing the at least one matching machine learning algorithm and integrate the trained machine learning model into stage of the analytics pipeline claim_text 15 an apparatus comprising processor and memory coupled to the processor wherein the memory comprises instructions which when executed by the processor cause the processor to implement machine learning framework which operates to register in machine learning algorithm repository plurality of machine learning algorithms wherein each machine learning algorithm is an algorithm used to train machine learning model to perform related task index by the machine learning framework the plurality of machine learning algorithms to generate and store in machine learning algorithm index data storage machine learning algorithm metadata model each machine learning algorithm in the plurality of machine learning algorithms receive via user interface of the machine learning framework user specification of at least one analytics pipeline task which at least one machine learning model is to be trained convert by machine learning algorithm search criteria generation engine of the machine learning framework the user specification to one or more machine learning algorithm search criteria search by the machine learning framework trained machine learning model index data storage indexing plurality of trained machine learning models to identify zero or more matching trained machine learning models having corresponding trained machine learning model metadata model that matches the one or more machine learning algorithm search criteria output via the user interface information describing the zero or more matching trained machine learning model and in response to the search of the trained machine learning model index data storage resulting in zero matching trained machine learning models being identified search by machine learning algorithm search engine of the machine learning framework the machine learning algorithm index data storage based on the one or more machine learning algorithm search criteria to identify at least one matching machine learning algorithm having corresponding machine learning algorithm metadata model that matches the one or more machine learning algorithm search criteria and output via the user interface information describing the at least one matching machine learning algorithm wherein the machine learning framework further operate to receive user selection of machine learning algorithm from the at least one matching machine learning algorithm and train machine learning model using the selected machine learning algorithm to generate new trained machine learning model and wherein training the machine learning model using the selected machine learning algorithm comprises invoking plurality of universal application programming interfaces apis to perform the training of the machine learning model claim_text 16 the apparatus of claim 15 wherein the machine learning framework is further configured to register in trained machine learning model repository the plurality of trained machine learning models and index by the machine learning framework the plurality of trained machine learning models to generate and store in trained machine learning model index data storage trained machine learning model metadata model each trained machine learning algorithm in the plurality of trained machine learning models claim_text 17 the apparatus of claim 15 wherein the machine learning framework comprises the plurality of universal apis training the machine learning model using the selected machine learning algorithm wherein the plurality of universal apis provide standardized mechanisms all of the machine learning algorithms registered in the machine learning algorithm repository lang en",
    "label": 1
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "127-177-660-785-528",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text claims what is claimed is 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning mode requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 8 the method of claim 1 wherein generating the updated machine learning mode comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 1 1 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "label": 1
  },
  {
    "id1": "023-604-939-311-912",
    "id2": "022-995-952-592-178",
    "claims1": "claims claim_text claims 1 method providing automated machine learning visualization by one or more processors comprising receiving one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generating machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extracting metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generating an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 2 the method of claim 1 wherein receiving the one or more machine learning tasks further includes receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 3 the method of claim 1 further including defining metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defining metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 4 the method of claim 1 further including ranking the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranking structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranking the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 5 the method of claim 1 further including displaying the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 6 the method of claim 5 further including associating one or more nodes with one or more of the plurality of concentric rings based associating and displaying details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displaying each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 7 the method of claim 5 further including automatically updating the interactive visualization gui upon occurrence of one or more triggering events or automatically updating the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rings data rings and composition modules rings 8 system providing automated machine learning visualization comprising one or more computers with executable instructions that when executed cause the system to receive one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules generate machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines extract metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and generate an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 9 the system of claim 8 wherein the executable instructions receiving the one or more machine learning tasks further include receiving training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 10 the system of claim 8 wherein the executable instructions further define metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or define metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 11 the system of claim 8 wherein the executable instructions further rank the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria rank structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or rank the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 12 the system of claim 8 wherein the executable instructions further display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 13 the system of claim 8 wherein the executable instructions further associate one or more nodes with one or more of the plurality of concentric rings based associate and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or display each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 14 the system of claim 8 wherein the executable instructions further automatically update the interactive visualization gui upon occurrence of one or more triggering events or automatically update the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings 15 computer program product providing automated machine learning visualization by processor the computer program product comprising nontransitory computerreadable storage medium having computer readable program code portions stored therein the computerreadable program code portions comprising an executable portion that receives one or more machine learning tasks one or more transformers and one or more estimators into one or more machine learning composition modules an executable portion that generates machine learning model pipeline an ensemble of plurality of machine learning model pipelines or combination thereof along with corresponding metadata using the one or more machine learning composition modules wherein machine learning model pipeline is sequence of transformers and estimators and an ensemble of machine learning pipelines are an ensemble of machine learning pipelines an executable portion that extracts metadata from the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof and an executable portion that generates an interactive visualization graphical user interface gui of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the extracted metadata or combination thereof 16 the computer program product of claim 15 wherein the executable portion that executable portion that receives the one or more machine learning tasks further receives in the one or more machine learning tasks training data hold out data test data optimization metrics evaluation metrics target variable the one or more transformers and the one or more estimators into the one or more machine learning composition modules 17 the computer program product of claim 15 further including an executable portion that defines metadata the machine learning pipeline to include structure metadata performance metadata provenance metadata or combination thereof relating to the machine learning pipeline or defines metadata the ensemble of plurality of machine learning model pipelines to include structure metadata performance metadata provenance metadata or combination thereof relating to the ensemble of plurality of machine learning model pipelines 18 the computer program product of claim 15 further including an executable portion that ranks the extracted metadata and the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof according to pipeline ranking criteria and metadata ranking criteria ranks structure metadata performance metadata provenance metadata or combination thereof the machine learning pipeline according to the metadata ranking criteria or ranks the structure metadata the performance metadata the provenance metadata or combination thereof the ensemble of plurality of machine learning model pipelines according to the metadata ranking criteria 19 the computer program product of claim 15 further including an executable portion that display the interactive visualization gui radial structure having plurality of concentric rings having one or more nodes displayed therein wherein the plurality of concentric rings include at least machine learning pipeline rings data rings estimator rings transformer rings and composition module rings wherein the one or more nodes represent the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof data the one or more estimators the one or more transformers and the machine learning composition modules used to generate the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof wherein the having one or more nodes in the plurality of concentric rings are displayed in sequence based on different ranking criteria 20 the computer program product of claim 15 further including an executable portion that associates one or more nodes with one or more of the plurality of concentric rings based associates and display details relating to the machine learning model pipeline or the ensemble of plurality of machine learning model pipelines upon the user interacting with the one or more of the plurality of concentric rings or displays each of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination associated with the one or more selected data transformers the one or more data estimators the one or more machine learning composition modules one or more data partitions or combination thereof upon the user interacting with the one or more of the plurality of concentric rings 21 the computer program product of claim 15 further including an executable portion that automatically updates the interactive visualization gui upon occurrence of one or more triggering events or automatically updates the interactive visualization upon user selecting the pipeline ranking criteria and the metadata ranking criteria of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof the one or more transformers the one or more estimators the one or more machine learning composition modules or combination thereof visualization in one or more corresponding rings of the interactive visualization selecting or interacting with one or more nodes located within the interactive visualization of the machine learning model pipeline the ensemble of plurality of machine learning model pipelines or combination thereof or selecting or interacting with one or more nodes of one or more of plurality of metadata rings within the interactive visualization gui wherein the plurality of metadata rings include transformer rings estimators rinds data rings and composition modules rings lang en",
    "claims2": "claims claim_text 1 method comprising training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models claim_text 2 the method of claim 1 wherein managing the one or more machine learning features includes generating new version of the machine learning model based on the feature importance metrics claim_text 3 the method of claim 1 wherein managing the one or more machine learning features includes determining to remove one of the one or more machine learning features based on determination that its associated feature importance metric does not meet threshold value claim_text 4 the method of claim 1 wherein managing the one or more machine learning features includes retraining the machine learning model to remove at least one of the one or more machine learning features from the machine learning model claim_text 5 the method of claim 4 wherein managing the one or more machine learning features includes automatically deleting stored data of the at least one removed feature claim_text 6 the method of claim 4 wherein managing the one or more machine learning features includes automatically causing data of the at least one removed feature to be longer collected claim_text 7 the method of claim 1 wherein managing the one or more machine learning features includes modifying at least one of the one or more machine learning features and retraining the machine learning model using the at least one modified feature of the one or more machine learning features claim_text 8 the method of claim 1 wherein managing the one or more machine learning features includes generating new feature based on at least one of the one or more machine learning features and retraining the machine learning model using the new feature claim_text 9 the method of claim 1 wherein determining the feature importance metric selected one of the machine learning features includes comparing base performance of the machine learning model selected test dataset with new performance of the machine learning model modified version of the selected test dataset of the selected machine learning feature claim_text 10 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by modifying values corresponding to the selected machine learning feature using modification approach selected based on property of machine learning model claim_text 11 the method of claim 9 wherein the modified version of the selected test dataset of the selected machine learning feature is generated at least in part by randomizing values corresponding to the selected machine learning feature claim_text 12 the method of claim 1 wherein the each feature importance metric includes numerical value representing an amount of contribution of the corresponding machine learning feature to an inference result of the machine learning model claim_text 13 the method of claim 1 wherein the each feature importance metric includes numerical value representing rank order of the corresponding machine learning feature compared to others of the machine learning features claim_text 14 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes determining ranking order of the machine learning features of the plurality of different machine learning models claim_text 15 the method of claim 1 wherein managing together the machine learning features of the plurality of different machine learning models includes identifying feature sharing across plurality of the different machine learning models claim_text 16 the method of claim 15 wherein identifying the feature sharing across the plurality of the different machine learning models includes identifying plurality of different feature importance metrics same shared feature claim_text 17 the method of claim 15 wherein managing together machine learning features of the plurality of different machine learning models includes determining whether to remove selected machine learning feature of one of the different machine learning models based on identified sharing of the selected machine learning feature among plurality of the different machine learning models claim_text 18 system comprising processor configured to train machine learning model determine feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics manage one or more machine learning features of the plurality of machine learning features of the machine learning model wherein being configured to manage the one or more machine learning features of the machine learning model includes being configured to manage together machine learning features of plurality of different machine learning models and memory coupled to the processor and configured to provide the processor with instructions claim_text 19 the system of claim 18 wherein being configured to manage together the machine learning features of the plurality of different machine learning models includes being configured to identify feature sharing across plurality of the different machine learning models claim_text 20 computer program product the computer program product being embodied in nontransitory computer readable storage medium and comprising computer instructions training machine learning model determining feature importance metric each machine learning feature of plurality of machine learning features of the machine learning model and based on the feature importance metrics managing one or more machine learning features of the plurality of machine learning features of the machine learning model wherein managing the one or more machine learning features of the machine learning model includes managing together machine learning features of plurality of different machine learning models lang en",
    "label": 1
  },
  {
    "id1": "052-798-027-613-66X",
    "id2": "036-552-384-938-884",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 2 the machine learning system of claim 1 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 3 the machine learning system of claim 2 wherein the vector norm is the l2 norm claim_text 4 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 5 the machine learning system of claim 1 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 6 the machine learning system of claim 5 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 7 the machine learning system of claim 1 wherein the regularization term comprises relaxation strength hyperparameter claim_text 8 the machine learning computer system of claim 7 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 9 the machine learning system of claim 1 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 10 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes including first node and iteratively training the neural network comprises in forward propagations through the neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 11 the computerimplemented machine learning method of claim 10 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 12 the computerimplemented machine learning method of claim 11 wherein the vector norm is the l2 norm claim_text 13 the computerimplemented machine learning method of claim 10 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 14 the computerimplemented machine learning method of claim 10 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 15 the computerimplemented machine learning method of claim 10 wherein the regularization term comprises relaxation strength hyperparameter claim_text 16 the computerimplemented machine learning method of claim 15 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 17 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the neural network by in forward propagations through the neural network compute activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes wherein the one or more other softtied nodes are not nodes in the neural network claim_text 18 the machine learning computer system of claim 17 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 19 the machine learning system of claim 17 wherein the regularization term comprises relaxation strength hyperparameter claim_text 20 the machine learning computer system of claim 19 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 21 the machine learning system of claim 17 wherein the first node is feature node of the neural network such that the first node explicitly represents feature claim_text 22 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores neural network wherein the neural network comprises multiple nodes such that the neural network comprises first node and iteratively training the neural network comprises by in forward propagations through the neural network computing activation values each of the multiple nodes of the neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes and wherein the one or more other softtied nodes are not nodes in the neural network claim_text 23 the computerimplemented machine learning method of claim 22 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 24 the computerimplemented machine learning method of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 25 the computerimplemented machine learning method of claim 24 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text we claim 1 method 400 determining positioning method user equipment ue in network the method comprising loading by loading unit 302 at location server 300 plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods receiving by transceiver unit 304 at the location server 300 location request associated with service from the ue wherein the location request relates to determining location of the ue selecting by an execution unit 306 at the location server 300 first positioning method from the plurality of positioning methods based on the location request selecting by the execution unit 306 at the location server 300 fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeating by the execution unit 306 one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with nonsupported status selecting by the execution unit 306 at the location server 300 cell identifier cellid based positioning method in an event each of the selected fallback positioning methods from the set of fallback positioning methods is associated with nonsupported status 2 the method 400 claimed in claim 1 wherein each of the plurality of positioning methods is based on one of service identifier service id associated with the service and quality of service qos information associated with the location request 3 the method 400 claimed in claim 2 wherein post the receiving of the location request associated with the service from the ue the method comprises checking by processing unit 308 at the location server 300 an availability of at least one of the service id and the qos information in the received location request 4 the method 400 claimed in claim 2 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 5 the method 400 claimed in claim 1 wherein the location server 300 is location management function lmf 6 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 the first positioning method from the plurality of positioning methods the method comprises determining by the execution unit 306 one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 7 the method 400 claimed in claim 1 wherein prior to the selecting by the execution unit 306 at the location server 300 the fallback positioning method from the set of fallback positioning methods the method comprises determining one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 8 the method 400 claimed in claim 6 wherein the cellid identifies base station associated with the ue 9 system 300 determining positioning method user equipment ue in network the system comprising location server 300 wherein the location server 300 comprises loading unit 302 configured to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 connected to at least the loading unit 302 the transceiver unit 304 configured to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 connected to at least the transceiver unit 304 the execution unit 306 configured to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method 10 the system 300 claimed in claim 9 wherein each of the plurality of positioning methods is based on at least one of service identifier service id associated with the service and quality of service qos information associated with the location request 11 the system 300 claimed in claim 10 wherein post receiving the location request associated with the service from the ue the system comprises processing unit 308 configured to check an availability of at least one of the service id and the qos information in the received location request 12 the system 300 claimed in claim 10 wherein the first positioning method is the primary positioning method in an event the location request comprises the service id 13 the system 300 claimed in claim 9 wherein the location server 300 is location management function lmf 14 the system 300 claimed in claim 9 wherein prior to the selecting the first positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the first positioning method wherein the supported status refers to the first positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the first positioning method being not supported by the ue and the base station 15 the system 300 claimed in claim 9 wherein prior to the selecting the fallback positioning method the execution unit 306 is configured to determine one of supported status and the nonsupported status associated with the fallback positioning method wherein the supported status refers to the fallback positioning method being supported by at least one of the ue and base station and the nonsupported status refers to the fallback positioning method being not supported by the ue and the base station 16 the system 300 claimed in claim 9 wherein the cellid identifies base station associated with the ue 17 nontransitory computerreadable storage medium storing instructions determining positioning method of user equipment ue in network the storage medium comprising executable code which when executed by one or more units of system comprising location server 300 causes loading unit 302 to load plurality of positioning methods wherein the plurality of positioning methods comprises at least primary positioning method and set of fallback positioning methods transceiver unit 304 to receive location request associated with service from the ue wherein the location request relates to determining location of the ue an execution unit 306 to select first positioning method from the plurality of positioning methods based on the location request select fallback positioning method from the set of fallback positioning methods in an event the first positioning method is associated with nonsupported status repeat one or more remaining fallback positioning methods from the set of fallback positioning methods selection of next fallback positioning method in an event the selected fallback positioning method is associated with the nonsupported status and select cell identifier cellid in an event each of the selected fallback positioning methods from the set of fallback positioning methods is nonsupported positioning method lang en",
    "label": 0
  },
  {
    "id1": "044-438-135-761-652",
    "id2": "091-570-367-989-78X",
    "claims1": "claims claim_text 1 canceled claim_text 2 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes including first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 3 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation values of the first node each training data item associated to the cluster by adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 4 the machine learning system of claim 3 wherein the regularization term comprises vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 5 the machine learning system of claim 4 wherein the vector norm is the l2 norm claim_text 6 the machine learning system of claim 3 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to compute the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 7 the machine learning system of claim 2 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to assign training data items to the cluster according to clustering algorithm claim_text 8 the machine learning system of claim 7 wherein the clustering algorithm comprises kmeans clustering algorithm claim_text 9 the machine learning system of claim 3 wherein the regularization term comprises relaxation strength hyperparameter claim_text 10 the machine learning computer system of claim 9 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 11 the machine learning system of claim 2 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 12 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes including first node and iteratively training the first neural network comprises in forward propagations through the first neural network in first iteration of the training computing activation values each of the multiple nodes each training data item in set of training data items and soft tying the activation values of the first node each training data item in the set of training data items that is assigned to cluster claim_text 13 the computerimplemented machine learning method of claim 12 wherein soft tying the activation values of the first node each training data item associated to the cluster comprises adding regularization term to cost function the first node wherein the regularization term each training data item assigned to the cluster is based on difference between i the activation value the first node the training data item and ii an average activation value the first node across all training data items assigned to the cluster claim_text 14 the computerimplemented machine learning method of claim 13 wherein the regularization term is vector norm of the i the activation value the first node the training data item and ii the average activation value the first node across all training data items assigned to the cluster claim_text 15 the computerimplemented machine learning method of claim 14 wherein the vector norm is the l2 norm claim_text 16 the computerimplemented machine learning method of claim 13 further comprising computing by the one or more programmed processor cores the average activation value the first node across all training data items assigned to the cluster in an iteration prior to the first iteration claim_text 17 the computerimplemented machine learning method of claim 12 further comprising assigning by the one or more programmed processor cores training data items to the cluster according to clustering algorithm claim_text 18 the computerimplemented machine learning method of claim 13 wherein the regularization term comprises relaxation strength hyperparameter claim_text 19 the computerimplemented machine learning method of claim 18 wherein the relaxation strength hyperparameter multiplies each training data item in the set of training data items that is assigned to the cluster the difference between the activation value of the first node the training data item and the average activation value the first node across all training data items in the cluster claim_text 20 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and the executable instructions when executed by the at least one processor core cause the at least one processor core to train the first neural network by in forward propagations through the first neural network compute activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tie the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 21 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 22 the machine learning computer system of claim 20 wherein the executable instructions when executed by the at least one processor core cause the at least one processor core to soft tie the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item by adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 23 the machine learning system of claim 22 wherein the regularization term comprises relaxation strength hyperparameter claim_text 24 the machine learning computer system of claim 23 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 25 the machine learning system of claim 20 wherein the first node is feature node of the first neural network such that the first node explicitly represents feature claim_text 26 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores first neural network wherein the first neural network comprises multiple nodes such that the first neural network comprises first node and iteratively training the first neural network comprises by in forward propagations through the first neural network computing activation values each of the multiple nodes of the first neural network including the first node each training data item in first set of training data items and soft tying the activation value of the first node first training data item in the first set of training data items to activation values of one or more other softtied nodes the first training data item wherein the one or more other softtied nodes are not nodes in the first neural network claim_text 27 the computerimplemented machine learning method of claim 26 further comprising soft tying by the one or more programmed processor cores the activation value of the first node every training data item in the first set of training data items which the one or more other softtied nodes computed activation values claim_text 28 the computerimplemented machine learning method of claim 26 wherein soft tying the activation value of the first node the first training data item to the activation values of one or more other softtied nodes the first training data item comprises adding regularization term to cost function the first node wherein the regularization term is based on difference between i the activation value the first node the first training data item and ii an average activation value the first training data item across the first node and the one or more other softtied nodes claim_text 29 the computerimplemented machine learning method of claim 28 wherein the regularization term comprises relaxation strength hyperparameter claim_text 30 the computerimplemented machine learning method of claim 29 wherein the relaxation strength hyperparameter multiplies the difference between i the activation value the first node the first training data item and ii the average activation value the first training data item across the first node and the one or more other softtied nodes lang en",
    "claims2": "claims claim_text 1 method of aiding in diagnosing whether subject has depression comprising analyzing blood sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid alphahydroxyisobutyric acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to depressionpositive andor depressionnegative reference levels of the biomarkers in order to diagnose whether the subject has depression claim_text 2 the method of claim 1 wherein the depressionnegative reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects not having depression and the depressionpositive reference levels of the biomarkers comprise levels of the biomarkers in one or more samples from one or more subjects diagnosed with depression claim_text 3 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 4 the method of claim 2 wherein differential levels of the biomarkers between the sample and the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 5 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionpositive reference levels are indicative of diagnosis of depression in the subject claim_text 6 the method of claim 2 wherein levels of the biomarkers in the sample corresponding to the depressionnegative reference levels are indicative of diagnosis of depression in the subject claim_text 7 the method of claim 1 wherein the biological sample is blood plasma claim_text 8 the method of claim 1 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 9 the method of claim 8 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 10 method of monitoring progressionregression of depression in subject comprising analyzing first biological sample that was removed from subject to determine the levels of biomarkers depression in the sample wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and the first sample is obtained from the subject at first time point and wherein the analysis method the blood sample is mass spectrometry analyzing second biological sample that was removed from subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to monitor the progressionregression of depression in the subject claim_text 11 the method of claim 10 wherein the method further comprises comparing the levels of the biomarkers in the first sample the levels of the biomarkers in the second sample andor the results of the comparison of the levels of the biomarkers in the first and second samples to depressionpositive reference levels depressionnegative reference levels depressionprogressionpositive reference levels andor depressionregressionpositive reference levels of the biomarkers claim_text 12 the method of claim 10 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 13 the method of claim 12 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 14 method of assessing the efficacy of composition treating depression comprising analyzing blood sample removed from subject having depression and currently or previously being treated with composition to determine the levels of biomarkers depression wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry and comparing the levels of the biomarkers in the sample to levels of the biomarkers in previouslytaken biological sample from the subject wherein the previouslytaken biological sample was obtained from the subject before being treated with the composition b depressionpositive reference levels of the biomarkers c depressionnegative reference levels of the biomarkers d depressionprogressionpositive reference levels of the biomarkers andor depressionregressionpositive reference levels of the biomarkers claim_text 15 the method of claim 14 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids claim_text 16 the method of claim 15 wherein the fatty acids are selected from the group consisting of stearate myristate linoleate palmitoleate oleate palmitate and pentadecanoate claim_text 17 method assessing the efficacy of composition in treating depression comprising analyzing first blood sample obtained from subject to determine the levels of biomarkers depression the first sample obtained from the subject at first time point wherein the biomarkers comprise 3hydroxybutanoic acid and 4aminobutanoic acid and wherein the analysis method the blood sample is mass spectrometry administering the composition to the subject analyzing second biological sample obtained from the subject to determine the levels of the biomarkers wherein the second sample is obtained from the subject at second time point after administration of the composition and wherein the analysis method is mass spectrometry and comparing the levels of the biomarkers in the first sample to the levels of the biomarkers in the second sample in order to assess the efficacy of the composition treating depression claim_text 18 the method of claim 17 wherein the biomarkers further comprise one or more biomarkers selected from the group consisting of glycerol and one or more fatty acids lang en",
    "label": 0
  },
  {
    "id1": "026-671-265-300-529",
    "id2": "186-442-146-751-290",
    "claims1": "claims claim_text 1 method improving locality of machine learning models the method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 3 the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 4 the method of claim 3 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 5 the method of claim 4 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 6 the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 8 machine learning computations system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 9 the system of claim 8 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 10 the system of claim 9 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 11 the system of claim 10 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 12 the system of claim 11 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 13 the system of claim 9 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text 14 the system of claim 8 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text 15 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprising determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and performing machine learning computations using the updated machine learning model claim_text 16 the nontransitory computer storage medium of claim 15 wherein the output data comprises tensor and the one or more operations splitting the output data into the multiple portions of the output data comprises an split operation splitting the tensor into multiple subtensors claim_text 17 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations claim_text 19 the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprises modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text 20 the nontransitory computer storage medium of claim 16 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text claims what is claimed is 1 computerimplemented method comprising accessing training data computing resource limit setting and parameters of machine learning model forming at server machine learning training strategy based on the training data and the computing resource limit setting forming machine learning model configuration based on the machine learning training strategy selecting sampled data from the training data based on the machine learning training strategy and providing the machine learning model configuration and the sampled data to machine learning platform 2 the computerimplemented method of claim 1 further comprising training using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitoring computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data 3 the computerimplemented method of claim 2 further comprising generating an updated machine learning model configuration recommendation based on the resource usage data 4 the computerimplemented method of claim 2 further comprising receiving from client device the training data the computing resource limit setting and parameters of the machine learning model and providing the resource usage data and the trained machine learning model to the client device 5 the computerimplemented method of claim 2 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model 6 the computerimplemented method of claim 2 wherein forming the machine learning training strategy comprises providing summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receiving the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learning model and an accuracy of the machine learning model 7 the computerimplemented method of claim 6 further comprising providing the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy forming an updated machine learning model configuration based on the updated machine learning training strategy and providing the updated machine learning model configuration and the sampled data to the machine learning platform 8 the computerimplemented method of claim 1 further comprises testing deployed machine learning model based on the machine learning model configuration accessing performance assessment of the deployed machine learning model and generating performance indicator of the deployed machine learning model based on the testing and the performance assessment determining that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises updating the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model 9 the computerimplemented method of claim 1 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device 10 the computerimplemented method of claim 1 wherein the machine learning training strategy comprises one of random forest classifier or gaussian process regressor 11 computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the apparatus to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform 12 the computing apparatus of claim 11 wherein the instructions further configure the apparatus to train using the machine learning platform at the server the machine learning model with the sampled data and the machine learning model configuration an output of training of the machine learning model comprising trained machine learning model and monitor computing resources of the machine learning platform during training of the machine learning model an output of monitoring computing resources comprising resource usage data 13 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to generate an updated machine learning model configuration recommendation based on the resource usage data 14 the computing apparatus of claim 12 wherein the instructions further configure the apparatus to receive from client device the training data the computing resource limit setting and parameters of the machine learning model and provide the resource usage data and the trained machine learning model to the client device 15 the computing apparatus of claim 12 wherein the resource usage data indicate time length and peak memory used during the training of the machine learning model 16 the computing apparatus of claim 12 wherein forming the machine learn training strategy comprises provide summary of the training data and the computing resource limit setting to resource estimator and an efficiency accuracy tradeoff modeler and receive the machine learning training strategy from the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator is configured to estimate complexity of the machine learning model based on the summary of the training data wherein the efficiency accuracy tradeoff modeler is configured to model tradeoff between an efficiency of the machine learn model and an accuracy of the machine learning model 17 the computing apparatus of claim 16 wherein the instructions further configure the apparatus to provide the resource usage data and the machine learning training strategy to the resource estimator and the efficiency accuracy tradeoff modeler wherein the resource estimator and the efficiency accuracy tradeoff modeler are configured to generate an updated machine learning training strategy form an updated machine learning model configuration based on the updated machine learning training strategy and provide the updated machine learning model configuration and the sampled data to the machine learning platform 18 the computing apparatus of claim 11 further comprises test deployed machine learning model based on the machine learning model configuration access performance assessment of the deployed machine learning model and generate performance indicator of the deployed machine learning model based on the testing and the performance assessment determine that the performance indicator of the deployed machine learning model transgresses deployed machine learning model performance threshold in response to determining that the performance indicator of the deployed machine learning model transgresses the deployed machine learning model performance threshold updating the deployed machine learning model wherein updating the deployed machine learning model further comprises update the machine learning training strategy and the sampled data based on the performance indicator of the machine learning model 19 the computing apparatus of claim 11 wherein the machine learning platform is operated on second server the second server being configured to train using the machine learning platform the machine learning model with the sampled data and the machine learning model configuration an output of training the machine learning model comprising trained machine learning model monitor computing resources of the machine learning platform during training of the machine learning model on the second server an output of monitoring the computing resources comprising resource usage data and provide the resource usage data and the trained machine learning model to client device 20 nontransitory computerreadable storage medium the computer readable storage medium including instructions that when executed by computer cause the computer to access training data computing resource limit setting and parameters of machine learning model form at server machine learning training strategy based on the training data and the computing resource limit setting form machine learning model configuration based on the machine learning training strategy select sampled data from the training data based on the machine learning training strategy and provide the machine learning model configuration and the sampled data to machine learning platform lang en",
    "label": 1
  },
  {
    "id1": "151-548-458-500-410",
    "id2": "189-406-053-095-096",
    "claims1": "claims claim_text 1 47 canceled claim_text 48 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by broker component maintaining provider register containing information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the method comprising receiving request desired machine learning model from machine learning model consumer determining based on the information contained in the provider register machine learning model among the machine learning models provided by the machine learning model providers that matches the desired machine learning model and sending response to the machine learning model consumer providing information associated with the determined machine learning model claim_text 49 the method of claim 48 wherein the request includes information characterizing the desired machine learning model and wherein determining the machine learning model that matches the desired machine learning model includes matching the information characterizing the desired machine learning model with the information contained in the provider register claim_text 50 the method of claim 49 wherein the information characterizing the desired machine learning model includes at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 51 the method of claim 48 wherein the information contained in the provider register includes each machine learning model provided by one of the plurality of machine learning model providers at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 52 the method of claim 48 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes notification on the availability of the determined machine learning model and wherein the response including the notification is sent to the machine learning model consumer conditionally when the determined machine learning model matches the desired machine learning model better than machine learning model previously sent to the machine learning model consumer matching the desired machine learning model claim_text 53 the method of claim 52 further comprising receiving upon receiving the request the desired machine learning model registration message from machine learning model provider to register its machine learning models with the provider register wherein determining the machine learning model that matches the desired machine learning model includes checking the machine learning models registered by the registration message on match with the desired machine learning model if match with the desired machine learning model is determined sending request the determined machine learning model to the machine learning model provider providing the determined machine learning model and receiving the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the request claim_text 54 the method of claim 48 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes an output value output by the determined machine learning model in response to the one or more input values claim_text 55 the method of claim 54 further comprising sending the one or more input values to the machine learning model provider providing the determined machine learning model input to the desired machine learning model and receiving an output value output by the determined machine learning model from the machine learning model provider providing the determined machine learning model in response to the one or more input values claim_text 56 the method of claim 48 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response to the machine learning model consumer includes access information to the machine learning model provider providing the determined machine learning model claim_text 57 the method of claim 48 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 58 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model consumer and comprising sending request desired machine learning model to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers and receiving response from the broker component providing information associated with machine learning model determined by the broker component among the machine learning models provided by the machine learning model providers matching the desired machine learning model claim_text 59 the method of claim 58 wherein the request includes information characterizing the desired machine learning model the information characterizing the desired machine learning model including at least one of an expected output parameter provided by the desired machine learning model one or more expected input parameters required by the desired machine learning model an expected type of the desired machine learning model and one or more evaluation metricbased conditions indicative of output characteristics expected to be supported by the desired machine learning model claim_text 60 the method of claim 58 wherein the request is request to subscribe obtaining the desired machine learning model use at the machine learning model consumer and wherein the information associated with the determined machine learning model provided in the response from the broker component includes notification on the availability of the determined machine learning model and optionally the determined machine learning model claim_text 61 the method of claim 58 wherein the request is request to use the desired machine learning model and wherein the request includes one or more input values to be passed input to the desired machine learning model claim_text 62 the method of claim 58 wherein the request is request to obtain access information to machine learning model provider providing machine learning model that matches the desired machine learning model and wherein the information associated with the determined machine learning model provided in the response from the broker component includes access information to the machine learning model provider providing the determined machine learning model claim_text 63 the method of claim 62 further comprising sending to the machine learning model provider providing the determined machine learning model using the access information request to use the desired machine learning model wherein the request includes one or more input values to be passed input to the desired machine learning model and receiving from the machine learning model provider providing the determined machine learning model an output value output by the determined machine learning model in response to the one or more input values claim_text 64 the method of claim 58 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system claim_text 65 method facilitating use of machine learning models in system comprising plurality of machine learning model providers the method being performed by machine learning model provider of the plurality of machine learning model providers and comprising sending to broker component maintaining provider register including information about the plurality of machine learning model providers and machine learning models provided by the plurality of machine learning model providers the provider register enabling the broker component to determine machine learning model among the machine learning models provided by the plurality of machine learning model providers that matches desired machine learning model requested by machine learning model consumer registration message to register machine learning models provided by the machine learning model provider with the provider register of the broker component claim_text 66 the method of claim 65 wherein the registration message includes each machine learning model provided by the machine learning model provider at least one of an output parameter provided by the respective machine learning model one or more input parameters required by the respective machine learning model type of the respective machine learning model and one or more evaluation metric values indicative of output characteristics supported by the respective machine learning model claim_text 67 the method of claim 65 wherein the system is mobile communication system and the broker component is discoverable by at least one of the machine learning model consumer and the plurality of machine learning model providers via network repository function nrf of the mobile communication system lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time of the at least one compound in the chromatography bed such that equation i is satisfied equation i t res t diff equation i calculating concentration czt of the at least one compound in the mobile phase at predetermined location of the chromatography device and at predetermined time t based on the adsorption isotherm claim_text 2 the method according to claim 1 wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 3 the method according to claim 1 wherein the minimum residence time of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 4 the method according to claim 1 wherein in the calculation step czt is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 5 the method according to claim 4 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 6 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 7 the method according claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 8 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 9 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 10 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 11 the method according to claim 10 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 12 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 13 the chromatography method according to claim 12 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising iii carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 15 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 11 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "032-496-986-475-726",
    "id2": "013-300-859-082-341",
    "claims1": "claims claim_text computerimplemented method improving locality of machine learning models the method performed by data processing apparatus the method comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the method of claim 1 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the method of claim 3 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the method of claim 2 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the method of one of claims 1 to 5 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text machine learning computations system comprisingn data processing apparatus and n memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the system of claim 7 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the system of claim 9 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the system of claim 8 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor claim_text the system of one of claims 7 to 11 wherein generating the updated machine learning model comprises modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the one or more machine learning processors that outputs of the operation are stored in based on the characteristics of the memory hierarchy the one or more machine learning processors claim_text nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprisingn receiving data of machine learning model the data representing operations of the machine learning model n receiving data specifying characteristics of memory hierarchy one or more machine learning processors on which the machine learning model is going to be deployed the memory hierarchy including multiple memories storing machine learning data used by the one or more machine learning processors when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory n generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model the generating comprisingn determining that output data of given operation of the machine learning model should be stored in highest bandwidth memory of the multiple memories based on the machine learning model n determining that the output data of the given operation has data size that is larger than data storage capacity of the highest bandwidth memory and n in response to determining that the output data of the given operation has the data size that is larger than the data storage capacity of the highest bandwidth memory adding to the updated machine learning model one or more operations splitting the output data into multiple portions of output data such that each portion of output data has data size that is less than or equal to the data storage capacity of the highest bandwidth memory and n performing machine learning computations using the updated machine learning model claim_text the nontransitory computer storage medium of claim 13 whereinn the output data comprises tensor and n the one or more operations splitting the output data into the multiple portions of the output data comprises split operation splitting the tensor into multiple subtensors claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the machine learning model respective sequence of operations each subtensor wherein each respective sequence of operations corresponds to particular sequence of operations to be performed on the tensor in the machine learning model claim_text the nontransitory computer storage medium of claim 18 wherein generating the updated machine learning model comprisesn adding to the machine learning model respective control dependency at final operation of one or more of the respective sequences of operations that shifts control from the respective sequence of operations to first operation in next respective sequence of operations and optionally n modifying the machine learning model to store the subtensor given sequence of operations on the highest bandwidth memory while the given sequence of operations is being performed claim_text the nontransitory computer storage medium of claim 14 wherein generating the updated machine learning model comprises adding to the updated machine learning model concat operation configured to merge the multiple subtensors into an output tensor lang en",
    "claims2": "claims claim_text 1 network slice configuration method comprising sending by first network element network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 2 the network slice configuration method of claim 1 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 3 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 4 the network slice configuration method of claim 1 wherein the information of the network slice supported by the ta is sent by the first network element to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 5 the network slice configuration method of claim 1 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 6 network slice configuration method comprising receiving by second network element network slice configuration information of first network element and storing by the second network element network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 7 the network slice configuration method of claim 6 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 8 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 9 the network slice configuration method of claim 6 wherein the information of the network slice supported by the ta is received by the second network element from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 10 the network slice configuration method of claim 6 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 11 first network element comprising sending module which is configured to send network slice configuration information to second network element so that the second network element stores network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 12 the first network element of claim 11 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 13 the first network element of claim 11 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 14 the first network element of claim 11 wherein the sending module is further configured to send information of network slice supported by ta to the second network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 15 the first network element of claim 11 wherein the network slice information comprises single network slice selection assistance information snssai claim_text 16 second network element comprising receiving module which is configured to receive slice configuration information of first network element and storing module which is configured to storing network slice information corresponding to the network slice configuration information wherein the network slice configuration information comprises information of network slice supported by tracing area ta claim_text 17 the second network element of claim 16 wherein the network slice configuration information comprises information of network slice to which data radio bearer of terminal belongs claim_text 18 the second network element of claim 16 wherein the information of the network slice supported by the ta is network slice supported by the ta to which cell belongs claim_text 19 the second network element of claim 16 wherein the receiving module is further configured to receive information of network slice supported by ta from the first network element in served cell information field of f1 setup request message or gnbdu configuration update message claim_text 20 the second network element of claim 16 wherein the network slice information comprises single network slice selection assistance information snssai lang en",
    "label": 0
  },
  {
    "id1": "152-848-867-961-18X",
    "id2": "057-360-526-753-072",
    "claims1": "claims claim_text claims we claim 1 method comprising training via at least one processor of computer system using public data inputs public data machine learning model training via the at least one processor using private data inputs private data machine learning model training via the at least one processor public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing via the at least one processor the public data machine learning model using current public data input resulting in public data machine learning prediction executing via the at least one processor the private data machine learning model using current pnvate data input resulting in private data machine learning prediction and executing via the at least one processor the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 2 the method of claim 1 wherein the public data and the private data are associated with freight transport costs 3 the method of claim 2 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 4 the method of claim 2 wherein the private data comprises overhead costs and earner costs 5 the method of claim 1 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 6 the method of claim 5 further comprising adding via the at least one processor the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding via the at least one processor the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 7 the method of claim 1 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 8 system comprising at least one processor and nontransitory computerreadable storage medium having instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 9 the system of claim 8 wherein the public data and the private data are associated with freight transport costs 10 the system of claim 9 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 11 the system of claim 9 wherein the private data comprises overhead costs and earner costs 12 the system of claim 8 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 13 the system of claim 12 the nontransitory computerreadable storage medium having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 14 the system of claim 8 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 15 nontransitory computerreadable storage medium having instructions stored which when executed by at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 16 the nontransitory computerreadable storage medium of claim 15 wherein the public data and the private data are associated with freight transport costs 17 the nontransitory computerreadable storage medium of claim 16 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 18 the nontransitory computerreadable storage medium of claim 16 wherein the private data comprises overhead costs and carrier costs 19 the nontransitory computerreadable storage medium of claim 15 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 20 the nontransitory computerreadable storage medium of claim 19 having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data lang en",
    "claims2": "claims claim_text 1 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer setting detected number of active connections in counters provided each output port and controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections wherein said controlling step further includes outputting cell addressed to an output port corresponding to counter with said count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to said output port and subtracting value of said counter corresponding to an output port that has output cells at every output of cell claim_text 2 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of accumulating cell input from an input port in an input multiplex buffer connected to said input port inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell outputting cell accumulated in said output buffer and accumulating said output cell in an output separation buffer corresponding to an output port of said output cell outputting cell accumulated in said output separation buffer to be output to an output port to which said cell is addressed detecting number of active connections existing between each output port and each input multiplex buffer controlling number of cells output from each output separation buffer so that said number of cells are in proportional to number of said active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on number of said detected active connections detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of cell addressed to said output port from an input multiplex buffer claim_text 3 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of setting detected number of active connections in counters provided each output port and sequentially outputting cell addressed to an output port corresponding to counter with count value not set to 0 among said counters and subtracting value of counter corresponding to an output port that has output cell at every output of said cell when outputting cells accumulated in an output separation buffer to an output port claim_text 4 an asynchronous transfer mode hereinafter referred to atm switch method an atm switch having plurality of input multiplex buffers connected to plurality of input ports plurality of output separation buffers connected to plurality of output ports and plurality of output buffers each provided said respective output separation buffers said output buffers having capacity accumulating cells equal to number of said input multiplex buffers said atm switch method comprising the steps of step of accumulating cell input from an input port in an input multiplex buffer connected to said input port step of inhibiting output of cell addressed to certain output port from an input multiplex buffer when said cell of said certain output port equal to or more than threshold value has been accumulated in said input multiplex buffer step of outputting cell except said inhibited output cell from said input multiplex buffer and accumulating said output cell in an output buffer corresponding to an output port of said output cell step of inhibiting output of cells when an output separation buffer corresponding to said cells is fully occupied and otherwise outputting cells to be accumulated in corresponding output separation buffer step of detecting number of active connections existing between each output port and each input multiplex buffer and step of controlling number of cells output from each output separation buffer so that said number of cells are proportional to number of active connections existing between an output port to which said cell is addressed and an input multiplex buffer that has output said cell based on said detected number of active connections wherein said step controlling output of said cell further comprising the steps of detecting an output port to which cells equal to or more than number of cells proportional to number of active connections are addressed based on count value of counter provided said output port with respect to each output port connected to an output separation buffer when number of cells output from said output separation buffer is equal to or more than number of cells input to said output separation buffer and inhibiting output of the cells addressed to said output from an input multiplex buffer claim_text 5 an asynchronous transfer mode hereinafter referred to atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively input multiplex buffer control means controlling said input multiplex buffer so that output of cell addressed to certain output port indicated by first back pressure signal is inhibited and output separation buffer control means monitoring an accumulation amount of cells in said output separation buffer at every output port to which said cell is addressed and outputting said first back pressure signal to said input multiplex buffer control means to inhibit output of cell addressed to an output port having said accumulation amount equal to or more than predetermined threshold value detection means detecting number of active connections existing between each output port and each input multiplex buffer and said output separation buffer control means further comprises output control means controlling number of cells addressed to said output port output from an output separation buffer so that said number of cells are in proportional to number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said output control means further comprises counter each provided an output port means setting each number of active connections detected by said detection by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting counter value corresponding to an output port which has output cells at every output of cell claim_text 6 the atm switch of claim 5 wherein said output control means comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to said output separation buffer when said cells output from said output separation buffer are equal to or larger than cells input thereto and means outputting said first back pressure signal inhibiting output of cells addressed to said detected output port to said input multiplex buffer control means claim_text 7 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises counter provided each output port means setting each number of active connections detected by said detection means in said counter and means sequentially outputting cell addressed to an output port corresponding to counter having count value not set to 0 among said counters when outputting cells accumulated in an output separation buffer to an output port and subtracting value of said counter corresponding to an output port that has output cell at every output of cell claim_text 8 an atm switch comprising plurality of input multiplex buffers connected to plurality of input ports respectively plurality of output separation buffers connected to plurality of output ports respectively plurality of output buffers provided each of said output separation buffers accumulating cell output from said input multiplex buffer each of said output buffers allowed to input same number of cells number of said input multiplex buffers input multiplex buffer control means controlling said input multiplex buffer to inhibit output of cell addressed to an output port indicated by first back pressure received thereby output buffer control means controlling said output buffer to inhibit output of cell addressed to an output separation buffer indicated by second back pressure received thereby first output separation buffer control means monitoring an accumulation amount of cells in each of said output separation buffers and outputting said first back pressure signal inhibiting output of cells addressed to said output port to said input multiplex buffer control means when said accumulation amount becomes equal to or more than predetermined threshold value second output separation buffer control means monitoring an accumulation amount of each of said output separation buffers and outputting said second back separation buffer when any output separation buffer has vacant space accumulating cells detection means detecting number of active connections existing between each output port and each input multiplex buffer and third output separation buffer control means controlling number of cells addressed to said output port output from an output separation buffer to be proportional to the number of active connections existing between said output port and said input multiplex buffers based on number of active connections detected by said detection means wherein said third output separation buffer control means further comprises means detecting an output port to which cells equal to or more than number of active connections have been output based on count value of counter corresponding to an output port connected to an output separation buffer when cells output separation buffer when cells output from said output separation buffer is equal to or more than cells input thereto and means outputting said first back pressure signal inhibiting output of cell addressed to said detected output port to said input multiplex buffer control means lang en",
    "label": 0
  },
  {
    "id1": "151-873-840-528-894",
    "id2": "152-806-746-371-786",
    "claims1": "claims claim_text 1 method comprising receiving request to verify provenance of machine learning system generating digital fingerprint the machine learning system based on header associated with the machine learning system metadata associated with the machine learning system and content associated with the machine learning system wherein the digital fingerprint includes first value that corresponds to the header associated with the machine learning system second value that corresponds to the metadata associated with the machine learning system and third value that corresponds to the content associated with the machine learning system determining that the first value that corresponds to the header associated with the machine learning system matches stored value that corresponds to header associated with previously verified machine learning system and providing notification that the machine learning system is same machine learning system the previously verified machine learning system claim_text 2 the method of claim 1 wherein the request includes the header associated with the machine learning system the metadata associated with the machine learning system and the content associated with the machine learning system claim_text 3 the method of claim 1 wherein the first value that corresponds to the header associated with the machine learning system is generated based on corresponding values associated with pieces of information included in the header associated with the machine learning system claim_text 4 the method of claim 1 wherein the second value that corresponds to the metadata associated with the machine learning system is generated based on corresponding values associated with particular set of data fields included in the metadata associated with the machine learning system claim_text 5 the method of claim 1 wherein the third value that corresponds to the content associated with the machine learning system is generated based on the content associated with the machine learning system claim_text 6 the method of claim 1 wherein the stored value that corresponds to the header associated with the previously verified machine learning system is part of digital envelope associated with the previously verified machine learning system claim_text 7 the method of claim 6 wherein the stored value that corresponds to the header associated with the previously verified machine learning system is unencrypted claim_text 8 the method of claim 6 wherein the digital envelope associated with the previously verified machine learning system includes the stored value that corresponds to the header and combined value that corresponds to metadata associated with the previously verified machine learning system and content associated with the previously verified machine learning system claim_text 9 the method of claim 8 wherein the combined value is encrypted claim_text 10 the method of claim 1 wherein first subprocess is utilized to generate the first value that corresponds to the header associated with the machine learning system second subprocess is utilized to generate the second value that corresponds to the metadata associated with the machine learning system and third subprocess is utilized to generate the third value that corresponds to the content associated with the machine learning system claim_text 11 the method of claim 10 wherein the first subprocess and the second subprocess and the third subprocess are performed in parallel claim_text 12 the method of claim 10 wherein the first subprocess and the second subprocess and the third subprocess are sequentially performed claim_text 13 the method of claim 10 wherein the first subprocess includes normalizing pieces of information included in the header associated with the machine learning system claim_text 14 the method of claim 13 wherein the first subprocess further includes generating an ngram having particular length based on the normalized pieces of information claim_text 15 the method of claim 14 wherein the first subprocess further includes performing similarity hashing on the generated ngram to generate hashed value claim_text 16 the method of claim 15 wherein the first subprocess further includes encoding the hashed value claim_text 17 the method of claim 16 wherein the hashed value is the first value that corresponds to the header associated with the machine learning system claim_text 18 system comprising processor configured to receive request to verify provenance of machine learning system generate digital fingerprint the machine learning system based on header associated with the machine learning system metadata associated with the machine learning system and content associated with the machine learning system wherein the digital fingerprint includes first value that corresponds to the header associated with the machine learning system second value that corresponds to the metadata associated with the machine learning system and third value that corresponds to the content associated with the machine learning system determine that the first value that corresponds to the header associated with the machine learning system matches stored value that corresponds to header associated with previously verified machine learning system and provide notification that the machine learning system is same machine learning system the previously verified machine learning system and memory coupled to the processor and configured to provide the processor with instructions claim_text 19 the system of claim 18 wherein the request includes the header associated with the machine learning system the metadata associated with the machine learning system and the content associated with the machine learning system claim_text 20 computer program product embodied in nontransitory computer readable medium and comprising computer instructions receiving request to verify provenance of machine learning system generating digital fingerprint the machine learning system based on header associated with the machine learning system metadata associated with the machine learning system and content associated with the machine learning system wherein the digital fingerprint includes first value that corresponds to the header associated with the machine learning system second value that corresponds to the metadata associated with the machine learning system and third value that corresponds to the content associated with the machine learning system determining that the first value that corresponds to the header associated with the machine learning system matches stored value that corresponds to header associated with previously verified machine learning system and providing notification that the machine learning system is same machine learning system the previously verified machine learning system lang en",
    "claims2": "claims claim_text 1 pipeline type processor asynchronous transfer mode atm cells comprisingninput buffers storing n pieces of asynchronous transmission mode atm cellsnn pairs of processor and buffer pipeline processing said each cell the processors provided in first pipeline arrangement and the buffers provided in second pipeline arrangementnan address determining section determining pipeline processing condition said address determining section determining processing instruction code starting address of said each cell based upon the information of said cell stored in said input buffer andnan n1 address transferring sections respectively provided between adjacent ones of said processors in the first pipeline arrangement each of said address transferring sections transferring said processing instruction code starting address between one of the adjacent ones of said processors on an incoming side of the first pipeline arrangement to an other of the adjacent ones of said processors on an outgoing side of the pipeline arrangementnwherein said processors sequentially execute pipeline processing based upon said processing instruction code starting address andnwherein the first pipeline arrangement is separate from the second pipeline arrangement and wherein said each cell is passed along the second pipeline arrangement of said buffers execution by said respective processors at different one of plurality of states claim_text 2 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein each of said address determining sections includesnan information storing memory storing information required retrievalnan address storing memory storing an address at which processing instruction code executed by said processor is storedna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining section claim_text 3 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 4 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci claim_text 5 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said address determining section determines storing address an instruction code according to data format claim_text 6 pipeline type processor asynchronous transfer mode atm cells according to claim 1 wherein said pipeline processing condition is not determined by any of said processors but instead is provided respective processing instruction code starting address each said processor claim_text 7 pipeline type processor asynchronous transfer mode atm cells provided with plurality of processors each executing partial process dedicated to each processor an atm cell being stored into buffer corresponding to each processor said pipeline type processor comprisingnan address storing memory which stores plurality of sets of predetermined process starting addresses each of processors constituting said pipeline type processor each set of predetermined process starting addresses corresponding to conditions of the atm cell to be processed in different respective statenan address retrieving section which retrieves set of predetermined process starting addresses from said address storing memory in accordance with conditions extracted from an input atm cell and transfers each predetermined processing starting address to the corresponding processor andnan execution control section which detects atm cell arrival at each buffer corresponding to each processor and instructs the execution of process to the processor when the atm cell arrives at the corresponding buffernwherein said processors are disposed in first pipeline arrangement separate from second pipeline arrangement in which said buffers are disposed claim_text 8 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein at least one of said processors includes state transition control sections which read instruction code from memory and decode and execute pipeline processing based upon said starting address claim_text 9 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprising an address transferring section provided between said processors said address transferring section transferring said processing instruction code starting address claim_text 10 pipeline type processor asynchronous transfer mode atm cells according to claim 7 further comprisingnan input buffernan information storing memory storing information required retrievalna cell determining section generating retrieval condition based upon the contents of said information storing memory and said input buffer andna retrieving section informing said processor of processing instruction code starting address according to retrieval condition generated by said cell determining sectionnwherein the processing instruction code starting address is respectively different each of said processors based on plurality of states the retrieval condition that are obtained from the information storing memory in correspondence with particular condition that matches the retrieval condition claim_text 11 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein time in which said each processor processes said each cell stored in said buffer is equal the upper limit of said time being one cell time of said atm cell claim_text 12 pipeline type processor asynchronous transfer mode atm cells according to claim 7 wherein said information includes at least one of cell type virtual path identifier vpi and virtual channel identifier vci lang en",
    "label": 0
  },
  {
    "id1": "140-416-112-830-70X",
    "id2": "130-909-330-439-20X",
    "claims1": "claims claim_text 1 machine learning computer system comprising at least one processor core and memory in communication with the at least one processor core wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to iteratively train an ensemble with set of training data items wherein the ensemble comprises multiple machine learning ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to train the ensemble in first iteration of the training by selectively directing with preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 2 the machine learning computer system of claim 1 wherein the preliminary classifier is not trained with the ensemble claim_text 3 the machine learning computer system of claim 2 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 4 the machine learning computer system of claim 2 wherein the memory stores executable instructions that when executed by the at least one processor core cause the at least one processor core to further train the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 5 the machine learning computer system of claim 1 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 6 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to train the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 7 the machine learning computer system of claim 6 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 8 the machine learning computer system of claim 1 wherein the memory further stores instruction that when executed by the at least one processor core cause the at least one processor core to after training the ensemble and the preliminary classifier classify an actual data item with the ensemble in an operational use of the ensemble wherein the at least one processor core in the operational use is cause to determine with the preliminary classifier the selected set of ml ensemble members to direct the actual data item based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 9 computerimplemented machine learning method comprising iteratively training by one or more programmed processor cores machine learning ml system with set of training data items wherein the ml system comprises an ensemble that comprises multiple ml ensemble members wherein each of the multiple ml ensemble members comprises classifier and preliminary classifier that is trained through machine learning and iteratively training the ml system comprises in first iteration of the training selectively directing by the preliminary classifier certain training data items in the set of training data items to each of the certain training data items selected set of the multiple ml ensemble members wherein each of the certain training data items in the first iteration the selected set of the multiple ml ensemble members comprises one or more but less than all of the multiple ml ensemble members there is an unselected set of the multiple ml ensemble members at time that the certain training data item is directed to the selected set of the multiple ml ensemble members such that the unselected set comprises one or more ml ensemble members in the set of the multiple ml ensemble that are not in the selected set at the time the certain training data item is directed to the selected set of the multiple ml ensemble members the selected set of the multiple ml ensemble members uses the certain training item in the training of the selected set of the multiple ml ensemble members and the unselected set of the multiple ml ensemble members not use the certain training item in the training of the unselected set of the multiple ml ensemble members and training the preliminary classifier through machine learning to select the selected set of the multiple ml ensemble members to be directed each of the certain training data items in the first iteration based on prediction by the preliminary classifier of which of the multiple ml ensemble members are likely to correctly classify each of the certain training data items in the first iteration claim_text 10 the computerimplemented machine learning method of claim 9 wherein the preliminary classifier is not trained with the ensemble claim_text 11 the computerimplemented machine learning method of claim 10 wherein derivatives of error cost functions the multiple ml ensemble members are not backpropagated to the preliminary classifier claim_text 12 the computerimplemented machine learning method of claim 10 further comprising training by the one or more programmed processor cores the preliminary classifier to optimize combination of cost of errors by the ensemble and cost of computation the ensemble claim_text 13 the computerimplemented machine learning method of claim 9 wherein the ensemble comprises heterogeneous mixture of machine learning models claim_text 14 the computerimplemented machine learning method of claim 9 wherein training the preliminary classifier comprises training the preliminary classifier under guidance from learning coach wherein the learning coach is trained through machine learning to improve learning by the preliminary classifier claim_text 15 the computerimplemented machine learning method of claim 9 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item claim_text 16 the computerimplemented machine learning method of claim 14 further comprising after training the ml system classifying an actual data item with the ensemble in an operational use of the ensemble wherein classifying the actual data item comprises directing by the preliminary classifier the actual data item to selected set of the ml ensemble members of the ensemble based on prediction by the preliminary classifier of which of the multiple ml ensemble are likely to correctly classify the actual training data item lang en",
    "claims2": "claims claim_text 1 method of determining the concentration of at least one compound in chromatography method comprising the steps of selecting the at least one compound selecting stationary phase selecting mobile phase selecting chromatography device having chromatography bed comprising the stationary phase and the mobile phase obtaining an adsorption isotherm of the at least one compound on the stationary phase obtaining the maximum diffusion time t diff of the at least one compound in the mobile phase selecting minimum residence time t res of the at least one compound in the chromatography bed such that equation i is satisfied t res t diff equation i calculating concentration cz t of the at least one compound in the mobile phase at predetermined location z of the chromatography bed and at predetermined time t based on the adsorption isotherm wherein the calculation step is carried out without taking the diffusion of the at least one compound in the mobile phase into account claim_text 2 the method according to claim 1 wherein the minimum residence time t res of the compound in the chromatography bed is selected such that equation i1 is satisfied t res 2 t diff equation i1 claim_text 3 the method according to claim 1 wherein in the calculation step cz t is calculated based on the following equation wherein z is sum including the following terms wherein qz t represents the binding capacity of the at least one compound by the stationary phase  b is the bulk porosity of the chromatography bed and v is the velocity of the mobile phase in the chromatography bed claim_text 4 the method according to claim 3 wherein the sum z further includes the term wherein d ax is the axial dispersion coefficient of the at least one compound in the chromatography bed claim_text 5 the method according to claim 1 wherein the step of obtaining the adsorption isotherm involves one or more laboratory experiments claim_text 6 the method according to claim 1 wherein the residence time t res is 25 to 200 seconds claim_text 7 the method according to claim 1 wherein the stationary phase is chromatography membrane or monolith claim_text 8 the method according to claim 1 wherein the at least one compound comprises protein andor drug claim_text 9 method of obtaining at least one chromatography method parameter selected from the group consisting of stationary phase mobile phase and chromatography device comprising the steps of i executing the method according to claim 1 n times wherein n is an integer of 2 or more wherein the n executions differ from one another with respect to at least one of the stationary phase selection step the mobile phase selection step and the chromatography device selection step and ii selecting at least one stationary phase at least one mobile phase andor at least one chromatography device based on the result of step i claim_text 10 the method according to claim 9 wherein step i is carried out such that in each of the n executions the selection of the at least one compound comprises selecting at least two different compounds and in each of the n executions the at least two different compounds are the same claim_text 11 chromatography method comprising the method of determining the concentration of at least one compound in chromatography method according to claim 1 and further comprising carrying out the chromatography claim_text 12 the chromatography method according to claim 11 wherein the calculation step includes calculating the concentration of the compound in the mobile phase at the outlet of the chromatography device c out t at several points in time t and the carrying out the chromatography step includes collecting the mobile phase at time t where c out t00 mmoll claim_text 13 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 9 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii claim_text 14 chromatography method comprising the method of obtaining at least one chromatography method parameter according to claim 10 and further comprising carrying out the chromatography based on the at least one chromatography parameter selected from the at least one stationary phase at least one mobile phase andor at least one chromatography device in step ii lang en",
    "label": 0
  },
  {
    "id1": "020-925-613-909-867",
    "id2": "086-031-183-779-370",
    "claims1": "claims claim_text 1 canceled claim_text 2 computerimplemented method comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 3 the computerimplemented method of claim 2 further comprising receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 4 the computerimplemented method of claim 2 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 5 the computerimplemented method of claim 2 further comprising monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 6 the computerimplemented method of claim 2 further comprising in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 7 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 8 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 9 the computerimplemented method of claim 2 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 10 the computerimplemented method of claim 2 further comprising testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 11 the computerimplemented method of claim 2 wherein updating the machine learning model further comprises updating definition of task of the machine learning model accessing from datastore additional data based on the updated definition of the task and forming second machine learning model based on the additional data and the updated definition of the task claim_text 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising accessing by one or more processors of server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data claim_text 13 the computing apparatus of claim 12 wherein the operations further comprise receiving definition of task that identifies target of the machine learning model from machine learning platform that operates at the server accessing lookup table that maps type of task with machine learning tool the machine learning tool comprising one of regression tool classification tool and an unsupervised machine learning tool and identifying using the lookup table the machine learning tool corresponding to the type of task based on the definition of the task wherein forming the machine learning model is based on the first dataset the task and the machine learning tool claim_text 14 the computing apparatus of claim 12 wherein the deployment of the machine learning model comprises deploying the machine learning model by providing an application that is external to machine learning platform with access to the machine learning model claim_text 15 the computing apparatus of claim 12 wherein the operations further comprise monitoring after the deployment of the machine learning model performance of the machine learning model detecting the data deficit based on the performance of the machine learning model by analyzing statistics from the first dataset and identifying source of the data deficit faulty sensor claim_text 16 the computing apparatus of claim 12 wherein the operations further comprise in response to detecting the data deficit adapting the replacement data of the second dataset to match statistical properties of the first dataset and updating the machine learning model based on the adapted replacement data claim_text 17 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises accessing library of dataset from datastore identifying based on task of the machine learning model and the first dataset the second dataset from the library of dataset and augmenting the first dataset with the replacement data from the second dataset claim_text 18 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises preparing the second dataset processing by partitioning and filtering the second dataset based on task of the machine learning model wherein the machine learning model is based on the prepared second dataset claim_text 19 the computing apparatus of claim 12 wherein accessing the replacement data from the second dataset further comprises defining model search space based on the first dataset wherein the machine learning model is formed from the model search space claim_text 20 the computing apparatus of claim 12 wherein the operations further comprise testing the machine learning model accessing performance assessment of the machine learning model generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the first dataset updating definition of task of the machine learning model based on the performance indicator of the machine learning model and forming second machine learning model based on the updated definition of the task and the updated first dataset claim_text 21 nontransitory computerreadable storage medium the nontransitory computerreadable storage medium including instructions that when executed by server cause the server to perform operations comprising accessing by one or more processors of the server first dataset forming machine learning model based on the first dataset detecting after deployment of the machine learning model data deficit of the machine learning model by identifying missing values and frequency of the missing values from the first dataset in response to detecting the data deficit accessing replacement data from second dataset the replacement data remedying the data deficit by having similar properties to the first dataset and updating the machine learning model based on the replacement data lang en",
    "claims2": "claims claim_text 1 an information processing device optimizing information presented by an information presenter to an information recipient comprising an attribute determination unit that determines knowledge level and an understanding level of the information recipient with respect to the information based on biological activity information of the information recipient acquired by sensor presentation information feature determination unit that determines feature of the information presented by the information presenter to the information recipient and feedback optimization unit that converts presentation format of the information into presentation format according to the knowledge level and the understanding level of the information recipient based on the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the feature of the information determined by the presentation information feature determination unit claim_text 2 the information processing device according to claim 1 wherein the presentation information feature determination unit determines the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation in case where the information is auditory information claim_text 3 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the visual recognition level based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 4 the information processing device according to claim 2 wherein the presentation information feature determination unit determines the voice speed level based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 5 the information processing device according to claim 1 wherein the knowledge level and the understanding level of the information recipient with respect to the information determined by the attribute determination unit and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient by the feedback optimization unit are presented to the information presenter claim_text 6 an information processing method optimizing information presented by an information presenter to an information recipient using an information processing device comprising first step in which knowledge level and an understanding level of the information recipient with respect to the information are determined based on biological activity of the information recipient acquired by sensor and feature of the information presented by the information presenter to the information recipient is determined and second step in which presentation format of the information is converted into presentation format according to the knowledge level and the understanding level of the information recipient based on the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the feature of the information claim_text 7 the information processing method according to claim 6 wherein in the first step the feature of the information presented by the information presenter to the information recipient visual recognition level which is degree at which the information can be visually recognized at glance and cognitive conflict level which is degree of cognitive conflict of the information recipient with respect to the intonation are determined in case where the information is visual information and voice speed level which is degree of word interval in the information and voice intonation level which is degree of strength of voice intonation are determined in case where the information is auditory information claim_text 8 the information processing method according to claim 6 wherein in the first step the visual recognition level is determined based on presenceabsence of other language in the information combination of background color and character color presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the cognitive conflict level is determined based on presenceabsence of other language in the information meaning of words and character color of the words presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 9 the information processing method according to claim 6 wherein in the first step the voice speed level is determined based on an interval of the words presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient and the voice intonation level is determined based on strength of intonation presenceabsence of other language in the information presenceabsence of continuous information and presenceabsence of judgment of the information recipient claim_text 10 the information processing method according to claim 6 further comprising third step in which the determined knowledge level and the determined understanding level of the information recipient with respect to the information and the information after converting presentation format according to the knowledge level and the understanding level of the information recipient are presented to the information presenter lang en",
    "label": 0
  },
  {
    "id1": "127-177-660-785-528",
    "id2": "152-848-867-961-18X",
    "claims1": "claims claim_text claims what is claimed is 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning mode requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 8 the method of claim 1 wherein generating the updated machine learning mode comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 1 1 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and performing machine learning computations using the updated machine learning model 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "claims2": "claims claim_text claims we claim 1 method comprising training via at least one processor of computer system using public data inputs public data machine learning model training via the at least one processor using private data inputs private data machine learning model training via the at least one processor public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing via the at least one processor the public data machine learning model using current public data input resulting in public data machine learning prediction executing via the at least one processor the private data machine learning model using current pnvate data input resulting in private data machine learning prediction and executing via the at least one processor the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 2 the method of claim 1 wherein the public data and the private data are associated with freight transport costs 3 the method of claim 2 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 4 the method of claim 2 wherein the private data comprises overhead costs and earner costs 5 the method of claim 1 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 6 the method of claim 5 further comprising adding via the at least one processor the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding via the at least one processor the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 7 the method of claim 1 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 8 system comprising at least one processor and nontransitory computerreadable storage medium having instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 9 the system of claim 8 wherein the public data and the private data are associated with freight transport costs 10 the system of claim 9 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 11 the system of claim 9 wherein the private data comprises overhead costs and earner costs 12 the system of claim 8 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 13 the system of claim 12 the nontransitory computerreadable storage medium having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data 14 the system of claim 8 wherein the execution of the public data machine learning model and the execution of the private data machine learning model occur in parallel 15 nontransitory computerreadable storage medium having instructions stored which when executed by at least one processor cause the at least one processor to perform operations comprising training using public data inputs public data machine learning model training using private data inputs private data machine learning model training public and private data machine learning model wherein the public and private data machine learning model is trained using combination of 1 historical public data machine learning predictions output by the public data machine learning model and 2 historical private data machine learning predictions output by the private data machine learning model executing the public data machine learning model using current public data input resulting in public data machine learning prediction executing the private data machine learning model using current private data input resulting in private data machine learning prediction and executing the public and private data machine learning model using the public data machine learning prediction and the private data machine learning prediction inputs resulting in final prediction 16 the nontransitory computerreadable storage medium of claim 15 wherein the public data and the private data are associated with freight transport costs 17 the nontransitory computerreadable storage medium of claim 16 wherein the public data comprises origin location data destination location data fuel price data and consumer price index data 18 the nontransitory computerreadable storage medium of claim 16 wherein the private data comprises overhead costs and carrier costs 19 the nontransitory computerreadable storage medium of claim 15 wherein the training of the public data machine learning model the training of the private data machine learning model and the training of the public and private data machine learning model occurs periodically 20 the nontransitory computerreadable storage medium of claim 19 having additional instructions stored which when executed by the at least one processor cause the at least one processor to perform operations comprising adding the public data machine learning prediction to the historical public data machine learning predictions resulting in updated historical public data and adding the private data machine learning prediction to the historical private data machine learning predictions resulting in updated historical private data wherein subsequent training of the public and private data machine learning model is based on the updated historical public data and the updated historical private data lang en",
    "label": 1
  },
  {
    "id1": "130-984-086-698-559",
    "id2": "033-247-762-034-912",
    "claims1": "claims claim_text 1 computerimplemented method of training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the method comprises determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 2 the method of claim 1 wherein the first machine learning task and the second machine learning task are different supervised learning tasks claim_text 3 the method of claim 1 wherein the first machine learning task and the second machine learning tasks are different reinforcement learning tasks claim_text 4 the method of claim 1 wherein training the machine learning model on the training data comprises adjusting the first values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the second machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task claim_text 5 the method of claim 4 wherein training the machine learning model on the training data comprises each training example in the training data processing the training example using the machine learning model in accordance with current values of parameters of the machine learning model to determine model output determining gradient of the objective function using the model output target output the training example the current values of the parameters of the machine learning model and the first values of the parameters of the machine learning model and adjusting the current values of the parameters using the gradient to optimize the objective function claim_text 6 the method of claim 4 wherein the second term depends on each of the plurality of parameters product of the respective measure of importance of the parameter and difference between the current value of the parameter and the first value of the parameter claim_text 7 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining each of the plurality of parameters an approximation of probability that current value of the parameter is correct value of the parameter given first training data used to train the machine learning model on the first task claim_text 8 the method of claim 1 wherein determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task comprises determining fisher information matrix fim of the plurality of parameters of the machine learning model with respect to the first machine learning task wherein each of the plurality of parameters the respective measure of the importance of the parameter is corresponding value on diagonal of the fim claim_text 9 the method of claim 1 further comprising after training the machine learning model on the second machine learning task to determine second values of the parameters of the machine learning model obtaining third training data training the machine learning model on third different machine learning task and training the machine learning model on the third machine learning task by training the machine learning model on the third training data to adjust the second values of the parameters so that the machine learning model achieves an acceptable level of performance on the third machine learning task while maintaining an acceptable level of performance on the first machine learning task and the second machine learning task wherein during the training of the machine learning model on the third machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task are more strongly constrained to not deviate from the second values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task and the second machine learning task claim_text 10 the method of claim 9 further comprising determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and wherein training the machine learning model on the third training data includes adjusting the second values of the parameters to optimize an objective function that includes i first term that measures performance of the machine learning model on the third machine learning task and ii second term that imposes penalty parameter values deviating from the first parameter values wherein the second term penalizes deviations from the first values more parameters that were more important in achieving acceptable performance on the first machine learning task than parameters were less important in achieving acceptable performance on the first machine learning task iii third term that imposes penalty parameter values deviating from the second parameter values wherein the third term penalizes deviations from the second values more parameters that were more important in achieving acceptable performance on the second machine learning task than parameters were less important in achieving acceptable performance on the second machine learning task claim_text 11 the method of claim 10 wherein the second term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task and ii difference between the current value of the parameter and the first value of the parameter claim_text 12 the method of claim 10 wherein the third term depends on each of the plurality of parameters product of i the respective measure of importance of the parameter to the machine learning model achieving acceptable performance on the second machine learning task and ii difference between the current value of the parameter and the second value of the parameter claim_text 13 the method of claim 4 when dependent upon claim 4 further comprising identifying when switching from one machine learning task to another and updating the second term of the objective function in response claim_text 14 the method of claim 13 wherein identifying when switching from one machine learning task to another comprises inferring which task is being performed from one or more models claim_text 15 the method of claim 1 the method further comprising providing the trained machine learning model use in processing data after training the machine learning model on the second machine learning task claim_text 16 the method of claim 1 wherein the first and second machine learning tasks each comprise reinforcement learning task and wherein the reinforcement learning task is controlling an agent to interact with an environment to achieve goal claim_text 17 the method of claim 1 wherein the first and second machine learning tasks each comprise classification task and wherein the classification task is processing data to classify the data claim_text 18 system comprising one or more computers and one or more storage devices storing instructions that are operable when executed by the one or more computers to cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task claim_text 19 computer storage medium encoded with instructions that when executed by one or more computers cause the one or more computers to perform operations training machine learning model having plurality of parameters wherein the machine learning model has been trained on first machine learning task to determine first values of the parameters of the machine learning model and wherein the operations comprise determining each of the plurality of parameters respective measure of an importance of the parameter to the machine learning model achieving acceptable performance on the first machine learning task obtaining training data training the machine learning model on second different machine learning task and training the machine learning model on the second machine learning task by training the machine learning model on the training data to adjust the first values of the parameters so that the machine learning model achieves an acceptable level of performance on the second machine learning task while maintaining an acceptable level of performance on the first machine learning task wherein during the training of the machine learning model on the second machine learning task values of parameters that were more important in the machine learning model achieving acceptable performance on the first machine learning task are more strongly constrained to not deviate from the first values than values of parameters that were less important in the machine learning model achieving acceptable performance on the first machine learning task lang en",
    "claims2": "claims claim_text 1 computerbased method building learning machine to understand and explain learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine updating one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and upon receiving first query degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states retrieving and returning by reference learning machine component state importance assignment module first set of parameters in the parameter matrix claim_text 2 the computerbased method of claim 1 further comprises upon receiving second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states analyzing by reference learning machine component state importance classification module statistical properties of set of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above first classification threshold and parameter value variances below second classification threshold claim_text 3 the computerbased method of claim 1 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that have aggregated parameter values above third classification threshold and parameter value variances above fourth classification threshold claim_text 4 the computerbased method of claim 1 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs analyzing by reference learning machine component state importance classification module statistical properties of subset of parameters in the parameter matrix corresponding to each node in the reference learning machine and returning set of nodes that would not be returned second query set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states and the third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states claim_text 5 the computerbased method of claim 1 further comprises upon receiving fifth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine updating parameters in the parameter matrix feeding the recorded second outputs into an input signal component state importance assignment module which queries reference learning machine component state importance classification module first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 6 the computerbased method of claim 5 wherein one or more values of one or more components of one or more input signals of the first set of input signals are replaced by one or more alternative values to create an altered first set of input signals claim_text 7 the computerbased method of claim 6 wherein the one or more values of the component of the input signal of the first set of input signals have degree of importance between lower bound and an upper bound claim_text 8 the computerbased method of claim 7 further comprises generating at each node of the input signals of the altered first set of input signals by the reference learning machine third outputs claim_text 9 the computerbased method of claim 8 further comprises calculating and aggregating difference between the first outputs and the third outputs and returning the difference an additional metric the degree of importance of each component of the first input signal of the first set of input signals to the reference learning machines generating of the first outputs associated with possible states claim_text 10 the computerbased method of claim 1 further comprises upon receiving sixth query description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals feeding to description generator module at least one of the input signal of the first set of input signals expected outputs the input signal of the first set of input signals fourth outputs generated by the reference learning machine given the input signal of the first set of input signals and the degree of importance of each component of an input signal to the reference learning machines generating of the first outputs associated with the possible states and constructing by the description generator module description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals claim_text 11 computerbased method building learning machine to understand and explainer learning machines comprising receiving by reference learning machine first set of input signals generating at each node of the input signals of the first set of input signals by the reference learning machine first outputs recording the first outputs generated by the reference learning machine and training reference learning machine component state importance assignment network and the reference learning machine component state importance classification network based on derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals claim_text 12 the computerbased method of claim 6 further comprises upon receiving first query degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs possible states feeding to the reference learning machine component state importance assignment network derived products of the recorded first outputs along with corresponding expected output of the learning machine each given input signal of the first set of input signals and returning the reference learning machine component state importance assignment network output query result claim_text 13 the computerbased method of claim 6 further comprises upon receiving second query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs only small number of states feeding to the reference learning machine component state importance classification network degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to small number of states query result claim_text 14 the computerbased method of claim 6 further comprises upon receiving third query set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states feeding the reference learning machine component state importance classification network degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with high degree of importance to large number of states query result claim_text 15 the computerbased method of claim 6 further comprises upon receiving fourth query set of nodes in the reference learning machine that have low degrees of importance to the reference learning machine when generating of the first outputs feeding the reference learning machine component state importance classification network the degree of importance of each node of the reference learning machine to the reference learning machines generating of the first outputs receiving from the learning machine component state importance classification network second outputs of which three states each node is associated with and returning set of nodes classified being nodes with low degree of importance query result claim_text 16 the computerbased method of claim 6 further comprises upon receiving fourth query degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of the first outputs feeding the first input signal through the reference learning machine generating at each node of the first input signal by the reference learning machine second outputs recording the second outputs generated by the reference learning machine feeding the second outputs into an input signal component state importance assignment module which queries the reference learning machine component state importance classification network first set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the second outputs only small number of states along with their dominant states projecting by the input signal component state importance assignment module derived products of the recorded second outputs of the first set of nodes and aggregating the projected derived products based on dominant states of their associated nodes to determine the degree of importance of each component of the first input signal to the reference learning machines generating of the second outputs associated with possible states claim_text 17 system building learning machine to understand and explain learning machines comprising reference learning machine wherein the reference learning machine receives first set of input signals and generates at each node of the input signals of the first set of input signals first outputs an explainer learning machine wherein the explainer learning machine records the first outputs generated by the reference learning machine updates one or more parameters in parameter matrix based at least on one of the recorded first outputs derived products of the recorded first outputs and corresponding expected output each input signal of the first set of input signals and description generator module and responds to one or more queries one or more quantitative insights about the reference learning machine and description generator module claim_text 18 the system of claim 17 wherein the one or more quantitative insights comprise at least one of degree of importance of each node in the reference learning machine to the reference learning machines generating of the first outputs given possible states set of nodes in the reference learning machine that has high degrees of importance to the reference learning machines generating of the first outputs small number of states set of nodes in the reference learning machine that have high degrees of importance to the reference learning machines generating of the first outputs large number of states set of nodes in the reference learning machine that have low degrees of importance to the reference learning machines generating of the first outputs degree of importance of each component of first input signal of the first set of input signals to the reference learning machines generating of outputs associated with the possible states and description of why the reference learning machine generated the first outputs given an input signal of the first set of input signals lang en",
    "label": 1
  },
  {
    "id1": "024-958-324-997-756",
    "id2": "038-794-758-325-674",
    "claims1": "claims claim_text machine learning model management method executed by federated learning server wherein the federated learning server is in first management domain and is connected to machine learning model management center and the method comprisesn obtaining first machine learning model from the machine learning model management center n performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model and n sending the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the method according to claim 1 wherein the obtaining first machine learning model from the machine learning model management center comprisesn sending machine learning model requirement information to the machine learning model management center and n receiving the first machine learning model determined by the machine learning model management center based on the machine learning model requirement information claim_text the method according to claim 2 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model andor machine learning model training requirement claim_text the method according to claim 3 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the method according to any one of claims 1 to 4 wherein the method further comprises nsending access permission information of the second machine learning model to the machine learning model management center claim_text the method according to any one of claims 1 to 5 wherein the method further comprises nsending the second machine learning model to the plurality of federated learning clients claim_text the method according to any one of claims 1 to 6 wherein the sending the second machine learning model to the machine learning model management center comprises nsending the second machine learning model to the machine learning model management center if an application effect of the second machine learning model meets preset condition claim_text the method according to any one of claims 1 to 7 wherein the performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model comprisesn sending the first machine learning model to the plurality of federated learning clients in the first management domain to enable each of the plurality of federated learning clients to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n obtaining plurality of intermediate machine learning models obtained by the plurality of federated learning clients and aggregating the plurality of intermediate machine learning models to obtain the second machine learning model claim_text machine learning model management method executed by machine learning model management center wherein the machine learning model management center is connected to first federated learning server and the first federated learning server is in first management domain and the method comprisesn sending first machine learning model to the first federated learning server n receiving second machine learning model from the first federated learning server wherein the second machine learning model is obtained by the first federated learning server by performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain and n replacing the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text the method according to claim 9 wherein before the sending first machine learning model to the first federated learning server the method further comprisesn receiving machine learning model requirement information sent by the first federated learning server and n determining the first machine learning model based on the machine learning model requirement information claim_text the method according to claim 10 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the method according to claim 11 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the method according to any one of claims 9 to 12 wherein the second machine learning model is machine learning model based on first training framework and the method further comprises nconverting the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the method according to any one of claims 9 to 13 wherein the method further comprises nreceiving access permission information that is of the second machine learning model and that is sent by the first federated learning server claim_text the method according to any one of claims 9 to 14 wherein the method further comprisesn sending the second machine learning model to second federated learning server wherein the second federated learning server is in the second management domain n receiving fourth machine learning model from the second federated learning server wherein the fourth machine learning model is obtained by the second federated learning server by performing federated learning with plurality of federated learning clients in the second management domain based on the second machine learning model and local network service data in the second management domain and n replacing the second machine learning model with the fourth machine learning model claim_text federated learning system comprising federated learning server and plurality of federated learning clients wherein the federated learning server and the plurality of federated learning clients are in first management domain and the federated learning server is connected to machine learning model management center whereinn the federated learning server is configured to obtain first machine learning model from the machine learning model management center and send the first machine learning model to the plurality of federated learning clients n each of the plurality of federated learning clients is configured to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n the federated learning server is further configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients aggregate the plurality of intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the federated learning system according to claim 16 whereinn the federated learning server is further configured to send the second machine learning model to the plurality of federated learning clients and n each of the plurality of federated learning clients is further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text network system comprising machine learning model management center federated learning server and plurality of federated learning clients wherein the federated learning server and the plurality of federated learning clients are in first management domain and the federated learning server is connected to the machine learning model management center whereinn the machine learning model management center is configured to send first machine learning model to the federated learning server n the federated learning server is configured to send the first machine learning model to the plurality of federated learning clients n each of the plurality of federated learning clients is configured to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client n the federated learning server is further configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients aggregate the plurality of intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain and n the machine learning model management center is further configured to replace the first machine learning model with the second machine learning model claim_text the network system according to claim 18 whereinn the federated learning server is further configured to send machine learning model requirement information to the machine learning model management center and n the machine learning model management center is further configured to send the first machine learning model to the federated learning server based on the machine learning model requirement information claim_text the network system according to claim 19 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the network system according to claim 20 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the network system according to any one of claims 18 to 21 wherein the second machine learning model is machine learning model based on first training framework and nthe machine learning model management center is further configured to convert the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the network system according to any one of claims 18 to 22 whereinn the federated learning server is further configured to send the second machine learning model to the plurality of federated learning clients and n each of the plurality of federated learning clients is further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text federated learning server wherein the federated learning server is in first management domain and is connected to machine learning model management center and the federated learning server comprisesn transceiver unit configured to obtain first machine learning model from the machine learning model management center and n processing unit configured to perform federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model wherein n the transceiver unit is further configured to send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text the federated learning server according to claim 24 wherein the transceiver unit is specifically configured ton send machine learning model requirement information to the machine learning model management center and n receive the first machine learning model determined by the machine learning model management center based on the machine learning model requirement information claim_text the federated learning server according to claim 25 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the federated learning server according to claim 26 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the federated learning server according to any one of claims 24 to 27 wherein nthe transceiver unit is further configured to send access permission information of the second machine learning model to the machine learning model management center claim_text the federated learning server according to any one of claims 24 to 28 wherein nthe transceiver unit is further configured to send the second machine learning model to the plurality of federated learning clients claim_text the federated learning server according to any one of claims 24 to 29 wherein nthe transceiver unit is specifically configured to send the second machine learning model to the machine learning model management center if an application effect of the second machine learning model meets preset condition claim_text the federated learning server according to any one of claims 24 to 30 whereinn the transceiver unit is further configured to send the first machine learning model to the plurality of federated learning clients to enable each of the plurality of federated learning clients to perform federated learning based on the first machine learning model and network service data obtained by the federated learning client to obtain an intermediate machine learning model of the federated learning client and n the processing unit is specifically configured to obtain plurality of intermediate machine learning models obtained by the plurality of federated learning clients and aggregate the plurality of intermediate machine learning models to obtain the second machine learning model claim_text machine learning model management center wherein the machine learning model management center is connected to first federated learning server and the first federated learning server is in first management domain and the machine learning model management center comprisesn sending unit configured to send first machine learning model to the first federated learning server n receiving unit configured to receive second machine learning model from the first federated learning server wherein the second machine learning model is obtained by the first federated learning server by performing federated learning with plurality of federated learning clients in the first management domain based on the first machine learning model and local network service data in the first management domain and n processing unit configured to replace the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text the machine learning model management center according to claim 32 whereinn the receiving unit is further configured to receive machine learning model requirement information sent by the first federated learning server and n the processing unit is further configured to determine the first machine learning model based on the machine learning model requirement information claim_text the machine learning model management center according to claim 33 wherein the machine learning model requirement information comprises model service information corresponding to the machine learning model andor machine learning model training requirement claim_text the machine learning model management center according to claim 34 wherein the training requirement comprises at least one of the following training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text the machine learning model management center according to any one of claims 32 to 35 wherein the second machine learning model is machine learning model based on first training framework and the processing unit is further configured to nconvert the second machine learning model into third machine learning model wherein the third machine learning model is machine learning model based on second training framework and the third machine learning model and the second machine learning model correspond to same model service information claim_text the machine learning model management center according to any one of claims 32 to 36 wherein nthe receiving unit is further configured to receive access permission information that is of the second machine learning model and that is sent by the first federated learning server claim_text the machine learning model management center according to any one of claims 32 to 37 whereinn the sending unit is further configured to send the second machine learning model to second federated learning server wherein the second federated learning server is in the second management domain n the receiving unit is further configured to receive fourth machine learning model from the second federated learning server wherein the fourth machine learning model is obtained by the second federated learning server by performing federated learning with plurality of federated learning clients in the second management domain based on the second machine learning model and local network service data in the second management domain and n the processing unit is further configured to replace the second machine learning model with the fourth machine learning model claim_text machine learning model management apparatus comprising memory and processor wherein the memory is configured to store computer instructions and the processor is configured to invoke the computer instructions to perform the method according to any one of claims 1 to 15 claim_text computerreadable storage medium wherein the computerreadable storage medium stores computer program and when the computer program is run on computer the computer is enabled to perform the method according to any one of claims 1 to 15 lang en",
    "claims2": "claims claim_text 1 system training probabilistic predictive model recommending experiment designs synthetic biology comprising nontransitory memory configured to store executable instructions and hardware processor in communication with the nontransitory memory the hardware processor programmed by the executable instructions to receive synthetic biology experimental data generate training data from the synthetic biology experimental data wherein the training data comprise plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective train using the training data plurality of level0 learners of probabilistic predictive model recommending experiment designs synthetic biology wherein an input of each of the plurality of level0 learners comprises input values of the input variables and wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable and train using i predicted values of the at least one response variable determined using the plurality of level0 learners the training inputs of the plurality of training inputs and ii the reference outputs of the plurality of reference outputs correspondence to the training inputs of the plurality of training inputs level1 learner of the probabilistic predictive model recommending experiment designs synthetic biology comprising probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable claim_text 2 10 canceled claim_text 11 the system of claim 1 wherein the synthetic biology experimental data is sparse claim_text 12 the system of claim 1 wherein number of the plurality of training inputs in the synthetic biology experiment data is number of experimental conditions number of strains number of replicates of strain of the strains or combination thereof claim_text 13 15 canceled claim_text 16 the system of claim 1 wherein one or each of the plurality of input variables andor the at least one response variable comprises promoter sequence an induction time an induction strength ribosome binding sequence copy number of gene transcription level of gene an epigenetics state of gene level of protein post translation modification state of protein level of molecule an identity of molecule level of microbe state of microbe state of microbiome titer rate yield or combination thereof optionally wherein the molecule comprises an inorganic molecule an organic molecule protein polypeptide carbohydrate sugar fatty acid lipid an alcohol fuel metabolite drug an anticancer drug biofuel flavoring molecule fertilizer molecule or combination thereof claim_text 17 21 canceled claim_text 22 the system of claim 1 wherein the predetermined response variable objective comprises maximization objective minimization objective or specification objective andor wherein the predetermined response variable objective comprises maximizing the at least one response variable minimizing the at least one response variable or adjusting the at least one response variable to predetermined value of the at least one response variable claim_text 23 the system of claim 1 wherein to train the plurality of level1 learner the hardware processor is programmed by the executable instructions to determine using the plurality of level0 learners the predicted values of the at least one response variable training inputs of the plurality of training inputs claim_text 24 the system of claim 1 wherein the level1 learner comprises bayesian ensemble of the plurality of level0 learners claim_text 25 the system of claim 1 wherein parameters of the ensemble of the plurality of level0 learners comprises i plurality of ensemble weights and ii an error variable distribution of the ensemble or standard deviation of the error variable distribution of the ensemble claim_text 26 32 canceled claim_text 33 the system of claim 1 wherein to train the level1 learner the hardware processor is programmed by the executable instructions to determine posterior distribution of the ensemble parameters given the training data or the second subset of the training data wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to determine i probability distribution of the training data or the second subset of the training data given the ensemble parameters or likelihood function of the ensemble parameters given the training data of the second subset of the training data and ii prior distribution of the ensemble parameters and wherein to determine the posterior distribution of the ensemble parameters given the training data or the second subset of the training data the hardware processor is programmed by the executable instructions to sample space of the ensemble parameters with frequency proportional to desired posterior distribution claim_text 34 canceled claim_text 35 canceled claim_text 36 the system of claim 1 wherein to train the plurality of level0 learners the hardware processor is programmed by the executable instructions to generate first subset of the training data and train using the first subset of the training data the plurality of level0 learners claim_text 37 50 canceled claim_text 51 the system of claim 1 wherein the hardware processor is programmed by the executable instructions to determine surrogate function with an input experiment design an input the surrogate function comprising an expected value of the at least one response variable determined using the input experiment design variance of the value of the at least one response variable determined using the input experiment design and an exploitationexploration tradeoff parameter and determine using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of synthetic biology experiment obtaining predetermined response variable objective associated with the at least one response variable claim_text 52 57 canceled claim_text 58 the system of claim 51 wherein to determine the plurality of recommended experiment designs the hardware processor is programmed by the executable instructions to determine plurality of possible recommended experiment designs each comprising possible recommended values of the input variables with surrogate function values determined using the surrogate function with predetermined characteristic and select the plurality of recommended experiment designs from the plurality of possible recommended experiment designs using an input variable difference factor based on the surrogate function values of the plurality of possible recommended experiment designs claim_text 59 64 canceled claim_text 65 the system of claim 51 wherein number of the plurality of recommended experiment designs is number of experimental conditions or number of strains the next cycle of the synthetic biology experiment claim_text 66 canceled claim_text 67 canceled claim_text 68 the system of claim 51 wherein to determine the plurality of possible recommended experiment designs the hardware processor is programmed by the executable instructions to sample space of the input variables with frequency proportional to the surrogate function or an exponential function of the surrogate function and prior distribution of the input variables claim_text 69 canceled claim_text 70 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine an upper bound andor lower bound one or each of the plurality of input variables based on training values of the corresponding input variable wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 71 canceled claim_text 72 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to receive an upper bound andor lower bound one or each of the plurality of input variables wherein each of the possible recommended values of the input variables is within the upper bound andor the lower bound of the corresponding input variable claim_text 73 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability distribution of the at least one response variable one or each of the plurality of recommended experiment designs claim_text 74 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of one or each of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of one or each of the plurality of recommended experiment designs being predetermined percentage closer to achieving the objective relative to the training data claim_text 75 the system of claim 51 wherein the hardware processor is programmed by the executable instructions to determine using the posterior distribution of the ensemble parameters given the training data or the second subset of the training data probability of at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable optionally wherein the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective associated with the at least one response variable comprises the probability of the at least one of the plurality of recommended experiment designs achieving the predetermined response variable objective being predetermined percentage closer to achieving the objective relative to the training data claim_text 76 canceled claim_text 77 canceled claim_text 78 method recommending experiment designs synthetic biology comprising under control of hardware processor receiving probabilistic predictive model recommending experiment designs synthetic biology comprising plurality of level0 learners and level1 learner wherein an input of each of the plurality of level0 learners comprises input values of the input variables wherein an output of each of the plurality of level0 learners comprises predicted value of at least one response variable wherein the level1 learner comprises probabilistic ensemble of the plurality of level0 learners wherein an output of the level1 learner comprises predicted probabilistic distribution of the at least one response variable wherein the plurality of level0 learners and the level1 learner are trained using training data obtained from one or more cycles of synthetic biology experiment comprising plurality of training inputs and corresponding reference outputs wherein each of the plurality of training inputs comprises training values of input variables and wherein each of the plurality of reference outputs comprises reference value of at least one response variable associated with predetermined response variable objective determining surrogate function comprising an expected value of the level1 learner variance of the level1 learner and an exploitationexploration tradeoff parameter and determining using the surrogate function plurality of recommended experiment designs each comprising recommended values of the input variables next cycle of the synthetic biology experiment achieving predetermined response variable objective associated with the at least one response variable claim_text 79 85 canceled lang en",
    "label": 0
  },
  {
    "id1": "114-059-893-173-320",
    "id2": "199-776-926-658-216",
    "claims1": "claims claim_text 1 method implemented by federated learning server in first management domain the method comprising obtaining first machine learning model from machine learning model management center performing federated learning in the first management domain based on the first machine learning model and local network service data in the first management domain to obtain second machine learning model and sending the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain claim_text 2 the method of claim 1 further comprising sending machine learning model requirement information to the machine learning model management center wherein obtaining the first machine learning model comprises receiving the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 3 the method of claim 2 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 4 the method of claim 3 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 5 the method of claim 1 further comprising sending access permission information of the second machine learning model to the machine learning model management center claim_text 6 the method of claim 1 further comprising sending the second machine learning model to federated learning clients claim_text 7 the method of claim 1 further comprising determining that an application effect of the second machine learning model meets preset condition claim_text 8 the method of claim 1 wherein performing the federated learning comprises sending the first machine learning model to federated learning clients to enable the federated learning clients to perform the federated learning based on the first machine learning model and network service data and to obtain intermediate machine learning models of the federated learning client obtaining the intermediate machine learning models from the federated learning clients and aggregating the intermediate machine learning models to obtain the second machine learning model claim_text 9 method implemented by machine learning model management center and comprising sending first machine learning model to first federated learning server in first management domain receiving second machine learning model from the first federated learning server wherein the second machine learning model is based on first federated learning in the first management domain using the first machine learning model and first local network service data in the first management domain and replacing the first machine learning model with the second machine learning model to enable the second machine learning model to be used by device in second management domain claim_text 10 the method of claim 9 wherein before sending the first machine learning model the method further comprises receiving machine learning model requirement information from the first federated learning server and determining the first machine learning model based on the machine learning model requirement information claim_text 11 the method of claim 10 wherein the machine learning model requirement information comprises model service information corresponding to the first machine learning model or machine learning model training requirement claim_text 12 the method of claim 11 wherein the machine learning model training requirement comprises at least one of training environment an algorithm type network structure training framework an aggregation algorithm or security mode claim_text 13 the method of claim 9 wherein the second machine learning model is based on first training framework wherein the method further comprises converting the second machine learning model into third machine learning model based on second training framework and wherein the third machine learning model and the second machine learning model correspond to same model service information claim_text 14 the method of claim 9 further comprising receiving access permission information of the second machine learning model from the first federated learning server claim_text 15 the method of claim 9 further comprising sending the second machine learning model to second federated learning server in the second management domain receiving fourth machine learning model from the second federated learning server wherein the fourth machine learning model is based on second federated learning in the second management domain using the second machine learning model and second local network service data in the second management domain and replacing the second machine learning model with the fourth machine learning model claim_text 16 federated learning system comprising federated learning server in first management domain and configured to obtain first machine learning model from machine learning model management center send the first machine learning model obtain intermediate machine learning models aggregate the intermediate machine learning models to obtain second machine learning model and send the second machine learning model to the machine learning model management center to enable the second machine learning model to be used by device in second management domain and federated learning clients in the first management domain and configured to receive the first machine learning model from the federated learning server and perform first federated learning based on the first machine learning model and local network service data in the first management domain to obtain the intermediate machine learning models claim_text 17 the federated learning system of claim 16 wherein the federated learning server is further configured to send the second machine learning model to the federated learning clients and wherein the federated learning clients are further configured to execute based on the second machine learning model model service corresponding to the second machine learning model claim_text 18 the federated learning system of claim 16 wherein the federated learning server is further configured to send machine learning model requirement information to the machine learning model management center and receive the first machine learning model from the machine learning model management center based on the machine learning model requirement information claim_text 19 the federated learning system of claim 16 wherein the federated learning server is further configured to send access permission information of the second machine learning model to the machine learning model management center claim_text 20 the federated learning system of claim 16 wherein the federated learning server is further configured to determine that an application effect of the second machine learning model meets preset condition lang en",
    "claims2": "claims claim_text claims what is claimed is 1 computerimplemented method comprising accessing by one or more processors of server dataset from datastore receiving definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server forming utilizing the machine learning algorithm machine learning model based on the dataset and the task deploying the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitoring performance of the machine learning model after being deployed and updating the machine learning model based on the monitoring 2 the computerimplemented method of claim 1 wherein accessing the dataset further comprises accessing library of dataset from the datastore identifying based on the task and the dataset additional data from library of dataset and augmenting the dataset with the additional data 3 the computerimplemented method of claim 1 wherein accessing the dataset further comprises preparing the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset 4 the computerimplemented method of claim 1 wherein accessing the dataset further comprises defining model search space based on the dataset wherein the machine learning model is formed from the model search space 5 the computerimplemented method of claim 1 wherein monitoring further comprises testing the machine learning model receiving from the application performance assessment of the machine learning model and generating performance indicator of the machine learning model based on the testing and the performance assessment determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold in response to determining that the performance indicator of the machine learning model transgresses the machine learning model performance threshold updating the machine learning model wherein updating the machine learning model further comprises updating the dataset updating the definition of the task based on the performance indicator of the machine learning model and forming utilizing the machine learning algorithm another machine learning model based on the updated definition of the task and the updated dataset 6 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises detecting data deficit based on the performance of the machine learning model accessing from the datastore additional data that remedy the data deficit and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the task 7 the computerimplemented method of claim 1 wherein updating the machine learning model further comprises updating the definition of the task accessing from the datastore additional data based on the updated definition of the task and forming utilizing the machine learning algorithm another machine learning model based on the additional data and the updated definition of the task wherein the target indicates at least one of an attribute of the dataset to be solved or an exploration of features of the dataset 8 the computerimplemented method of claim 1 further comprising identifying features of the dataset based on the target the target indicating future point in time the dataset comprising plurality of timevarying signals training the machine learning model based on the identified features and generating using the trained machine learning model prediction of data of an attribute of the dataset at the future point in time 9 the computerimplemented method of claim 8 further comprising receiving selection of signal from the plurality of timevarying signals in the dataset wherein the target indicates the future point in time of the signal performing back test the selected signal using the trained machine learning model and validating the trained machine learning model based on results from the back test 10 the computerimplemented method of claim 8 further comprising accessing feature set of the dataset forming an augmented feature set from the feature set measuring dependencies of the augmented feature set each target of target set and generating single feature each target of the target set 11 the computerimplemented method of claim 10 wherein forming the augmented feature set further comprises each signal of the dataset scanning corresponding signal historical points each signal of the dataset shifting the historical points to present time each signal of the dataset generating new signal based on the shifted historical points and forming an augmented feature set based on the new signals from all signals in the feature set 12 computing apparatus the computing apparatus comprising processor and memory storing instructions that when executed by the processor configure the computing apparatus to perform operations comprising access by one or more processors of server dataset from datastore receive definition of task that identifies target of machine learning algorithm from machine learning platform that operates at the server form utilizing the machine learning algorithm machine learning model based on the dataset and the task deploy the machine learning model by providing an application that is external to the machine learning platform with access to the machine learning model monitor performance of the machine learning model after being deployed and update the machine learning model based on the monitoring 13 the computing apparatus of claim 12 wherein accessing the dataset further comprises access library of dataset from the datastore identify based on the task and the dataset additional data from library of dataset and augment the dataset with the additional data 14 the computing apparatus of claim 12 wherein accessing the dataset further comprises prepare the dataset processing by partitioning and filtering the dataset based on the task wherein the machine learning model is based on the prepared dataset 15 the computing apparatus of claim 12 wherein accessing the dataset further comprises define model search space based on the prepared dataset wherein the machine learning model is formed from the model search space lang en",
    "label": 1
  },
  {
    "id1": "159-445-014-625-895",
    "id2": "040-970-435-595-428",
    "claims1": "claims claim_text 1 method performed by data processing apparatus the method comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 2 the method of claim 1 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 3 the method of claim 1 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 4 the method of claim 1 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 5 the method of claim 4 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 6 the method of claim 4 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 7 the method of claim 1 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 8 the method of claim 1 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 9 system comprising data processing apparatus and memory storage apparatus in data communication with the data processing apparatus the memory storage apparatus storing instructions executable by the data processing apparatus and that upon such execution cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 10 the system of claim 9 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 11 the system of claim 9 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 12 the system of claim 9 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation claim_text 13 the system of claim 12 wherein the second control data causes the machine learning processor to transfer the output data from the first memory to the second memory in response to third operation being executed the third operation being different from the first and second operations claim_text 14 the system of claim 12 wherein determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation comprises determining that the output data the first operation is to be stored in the first memory based on at least one of i number of operations that will be executed between the first operation and the second operation or ii an estimated duration of time between when the first operation will be executed and the second operation will be executed claim_text 15 the system of claim 9 wherein generating the updated machine learning model comprises determining that input data particular sequence of operations of the machine learning model requires more data storage capacity than particular memory of the multiple memories and in response including in the updated machine learning model multiple sequences of operations that include same sequence of operations the particular sequence of operations first control data that causes the machine learning processor to split the input data into multiple portions of data second control data that causes the machine learning processor to assign each portion of data to respective sequence of operations of the multiple sequence of operations and third control data that causes the machine learning processor to perform the multiple sequences of operations in series claim_text 16 the system of claim 9 wherein generating the updated machine learning model comprises using second machine learning model to generate the updated machine learning model based on the machine learning model and the characteristics of the memory hierarchy claim_text 17 nontransitory computer storage medium encoded with computer program the program comprising instructions that when executed by one or more data processing apparatus cause the data processing apparatus to perform operations comprising receiving data of machine learning model the data representing operations of the machine learning model and data dependencies between the operations receiving data specifying characteristics of memory hierarchy machine learning processor on which the machine learning model is going to be deployed the memory hierarchy including multiple memories at multiple memory levels storing machine learning data used by the machine learning processor when performing machine learning computations using the machine learning model the characteristics including data storage capacity of each memory and memory bandwidth of each memory wherein at least one of the memories has different memory bandwidth than at least one other memory generating based on the data of the machine learning model and the characteristics of the memory hierarchy an updated machine learning model by modifying the operations and control dependencies of the machine learning model to account the characteristics of the memory hierarchy and modifying each of one or more of the operations of the machine learning model which memory of the multiple memories of the machine learning processor that outputs of the operation are stored in based on the characteristics of the memory hierarchy including the data storage capacity and the memory bandwidth of each memory and performing machine learning computations using the updated machine learning model claim_text 18 the nontransitory computer storage medium of claim 17 wherein the data of the machine learning model comprises graph that represents the operations of the machine learning model the control dependencies between the operations and data dependencies between the operations claim_text 19 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises selecting at least portion of the operations one of the memories to store outputs of the operation based on when the outputs will be used inputs to another operation claim_text 20 the nontransitory computer storage medium of claim 17 wherein generating the updated machine learning model comprises determining that output data first operation is to be stored in first memory of the multiple memories based on when the output data the first operation will be used input by second operation the first memory having lower memory bandwidth than second memory of the multiple memories and in response including in the updated machine learning model first control data that causes the machine learning processor to store the output data the first operation in the first memory after the output data is generated by the first operation and second control data that causes the machine learning processor to transfer the output data from the first memory to the second memory prior to the output data being used input to the second operation lang en",
    "claims2": "claims claim_text what is claimed is 1 method of processing data comprising receiving at an processing device set of global parameters each machine learning model of plurality of machine learning models each respective machine learning model of the plurality of machine learning models processing at the processing device data stored locally on the processing device with respective machine learning model according to the set of global parameters to generate machine learning model output receiving at the processing device user feedback regarding the machine learning model output performing at the processing device an optimization of the respective machine learning model based on the machine learning model output and the user feedback associated with machine learning model output to generate locally updated machine learning model parameters and sending the locally updated machine learning model parameters to remote processing device and receiving from the remote processing device set of globally updated machine learning model parameters each machine learning model of the plurality of machine learning models wherein the set of globally updated machine learning model parameters each respective machine learning model are based at least in part on the locally updated machine learning model parameters 2 the method of claim 1 further comprising performing at the processing device number of optimizations before sending the locally updated machine learning model parameters to the remote processing device 3 the method of claim 1 wherein the set of globally updated machine learning model parameters each respective machine learning model of the plurality of machine learning models are based at least in part on locally updated machine learning model parameters of second processing device 4 the method of claim 1 wherein the user feedback comprises an indication of correctness of the machine learning model output 5 the method of claim 1 wherein the data stored locally on the processing device is one of image data audio data or video data 6 the method of claim 1 wherein the processing device is one of smartphone or an internet of things device 7 the method of claim 1 wherein processing at the processing device the data stored locally on the processing device with the machine learning model is performed at least in part by one or more neural processing units 8 the method of claim 1 wherein performing at the processing device the optimization of the machine learning model is performed at least in part by one or more neural processing units 9 processing device comprising memory comprising computerexecutable instructions one or more processors configured to execute the computerexecutable instructions and cause the processing device to receive set of global parameters each machine learning model of plurality of machine learning models each respective machine learning model of the plurality of machine learning models process data stored locally on processing device with respective machine learning model according to the set of global parameters to generate machine learning model output receive user feedback regarding machine learning model output perform an optimization of the respective machine learning model based on the machine learning model output and the user feedback associated with machine learning model output to generate locally updated machine learning model parameters and send the locally updated machine learning model parameters to remote processing device and receive from the remote processing device set of globally updated machine learning model parameters each machine learning model of the plurality of machine learning models wherein the set of globally updated machine learning model parameters each respective machine learning model are based at least in part on the locally updated machine learning model parameters 10 the processing device of claim 9 wherein the one or more processors are further configured to cause the processing device to perform number of optimizations before sending the locally updated machine learning model parameters to the remote processing device 11 the processing device of claim 9 wherein the set of globally updated machine learning model parameters each respective machine learning model of the plurality of machine learning models are based at least in part on locally updated machine learning model parameters of second processing device 12 the processing device of claim 9 wherein the user feedback comprises an indication of correctness of the machine learning model output 13 the processing device of claim 9 wherein the processing device is one of smartphone or an internet of things device 14 the processing device of claim 9 wherein one of the one or more processors is neural processing unit configured to process the data stored locally on the processing device with the machine learning model 15 the processing device of claim 9 wherein one of the one or more processors is neural processing unit configured to perform the optimization of the machine learning model 16 method of processing data comprising each respective machine learning model of plurality of machine learning models each respective remote processing device of plurality of remote processing devices sending from server to the respective remote processing device an initial set of global model parameters the respective machine learning model and receiving at the server from the respective remote processing device an updated set of model parameters the respective machine learning model and performing at the server an optimization of the respective machine learning model based on the updated set of model parameters received from each remote processing device of the plurality of remote processing devices to generate an updated set of global model parameters and sending from the server to each remote processing device of the plurality of remote processing devices the updated set of global model parameters each machine learning model of the plurality of machine learning models 17 the method of claim 16 wherein performing at the server an optimization of the respective machine learning model comprises computing an effective gradient each model parameter of the initial set of global model parameters the respective machine learning model 18 the method of claim 16 further comprising each respective machine learning model of the plurality of machine learning models determining corresponding density estimator parameterized by weighting parameters the respective machine learning model 19 the method of claim 18 further comprising determining prior mixture weights the respective machine learning model 20 the method of claim 16 wherein the plurality of remote processing devices comprises smartphone 21 the method of claim 16 wherein the plurality of remote processing devices comprise an internet of things device 22 the method of claim 16 wherein each respective machine learning model of the plurality of machine learning models is neural network model 23 the method of claim 22 wherein each respective machine learning model of the plurality of machine learning models comprises same network structure 24 processing device comprising memory comprising computerexecutable instructions one or more processors configured to execute the computerexecutable instructions and cause the processing device to each respective machine learning model of plurality of machine learning models each respective remote processing device of plurality of remote processing devices send to the respective remote processing device an initial set of global model parameters the respective machine learning model and receive from the respective remote processing device an updated set of model parameters the respective machine learning model and perform an optimization of the respective machine learning model based on the updated set of model parameters received from each remote processing device of the plurality of remote processing devices to generate an updated set of global model parameters and send to each remote processing device of the plurality of remote processing devices the updated set of global model parameters each machine learning model of the plurality of machine learning models 25 the processing device of claim 24 wherein in order to perform the optimization of the respective machine learning model the one or more processors are further configured to cause the processing device to compute an effective gradient each model parameter of the initial set of global model parameters the respective machine learning model 26 the processing device of claim 24 wherein the one or more processors are further configured to cause the processing device to each respective machine learning model of the plurality of machine learning models determine corresponding density estimator parameterized by weighting parameters the respective machine learning model 27 the processing device of claim 26 wherein the one or more processors are further configured to cause the processing device to each respective machine learning model of the plurality of machine learning models determine prior mixture weights the respective machine learning model 28 the processing device of claim 24 wherein the plurality of remote processing devices comprises smartphone 29 the processing device of claim 24 wherein each respective machine learning model of the plurality of machine learning models is neural network model 30 the processing device of claim 29 wherein each respective machine learning model of the plurality of machine learning models comprises same network structure lang en",
    "label": 1
  }
]